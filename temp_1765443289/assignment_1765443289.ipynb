{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies**"
      ],
      "metadata": {
        "id": "NPMqcJxNtlyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q flask pyngrok transformers accelerate bitsandbytes sentence-transformers faiss-cpu PyPDF2"
      ],
      "metadata": {
        "id": "8rGyTpzyoe8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create app.py**"
      ],
      "metadata": {
        "id": "Gw_5ofeHtp0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# ==============================================\n",
        "# \ud83d\udcda Intelligent Academic Research Assistant\n",
        "# Flask + RAG (PDFs + FAISS + Mistral-7B)\n",
        "# ==============================================\n",
        "\n",
        "import os, gc, torch, faiss, numpy as np, re, logging, warnings, io as py_io\n",
        "from flask import Flask, render_template, request\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from functools import lru_cache\n",
        "from transformers.utils import logging as hf_logging\n",
        "\n",
        "# ------------------------------------------------\n",
        "# \u2699\ufe0f Suppress noisy logs\n",
        "# ------------------------------------------------\n",
        "hf_logging.set_verbosity_error()\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# ==============================================\n",
        "# \ud83e\udde0 STEP 3 \u2014 LAZY-LOADED MODELS (CACHED)\n",
        "# (same models as your original code)\n",
        "# ==============================================\n",
        "@lru_cache(maxsize=1)\n",
        "def get_embed_model():\n",
        "    print(\"\ud83d\udd04 Loading embedding model (all-MiniLM-L12-v2)...\")\n",
        "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
        "    print(\"\u2705 Embedding model ready and cached.\")\n",
        "    return model\n",
        "\n",
        "@lru_cache(maxsize=1)\n",
        "def get_llm_pipeline():\n",
        "    print(\"\ud83d\udd04 Loading Mistral 7B Instruct (4-bit)...\")\n",
        "    model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        load_in_4bit=True\n",
        "    )\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    print(\"\u2705 Mistral 7B Instruct model loaded and cached.\")\n",
        "    return pipe\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# \ud83d\udcc2 GLOBAL STATE FOR RAG (PDFs, Chunks, Index)\n",
        "# ==============================================\n",
        "chunk_texts = []        # text chunks\n",
        "chunk_sources = []      # source PDF for each chunk\n",
        "index = None            # FAISS index\n",
        "pdf_files = []          # uploaded PDF filenames\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# \ud83d\udcc2 STEP 4 \u2014 UPLOAD & EXTRACT PDF TEXT (Helper)\n",
        "# Same logic as your original, but wrapped for Flask\n",
        "# ==============================================\n",
        "def process_uploaded_pdfs(file_storages):\n",
        "    \"\"\"\n",
        "    Reads uploaded PDF files, extracts text, creates chunks,\n",
        "    and builds global chunk_texts, chunk_sources and FAISS index.\n",
        "    \"\"\"\n",
        "    global chunk_texts, chunk_sources, index, pdf_files\n",
        "\n",
        "    pdf_files = [f.filename for f in file_storages if f and f.filename.lower().endswith(\".pdf\")]\n",
        "    if not pdf_files:\n",
        "        raise ValueError(\"\u274c No valid PDF files uploaded.\")\n",
        "\n",
        "    print(f\"\u2705 Uploaded {len(pdf_files)} file(s): {pdf_files}\")\n",
        "\n",
        "    # For size info (not required for RAG)\n",
        "    pdf_texts = \"\"\n",
        "    for file in file_storages:\n",
        "        if not file or not file.filename.lower().endswith(\".pdf\"):\n",
        "            continue\n",
        "        reader = PdfReader(py_io.BytesIO(file.read()))\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \" \"\n",
        "        pdf_texts += f\"\\n\\n--- From {file.filename} ---\\n\\n{text}\"\n",
        "\n",
        "    print(f\"\u2705 Extracted {len(pdf_texts)//1000}K characters from all PDFs.\")\n",
        "\n",
        "    # Rebuild chunks and sources\n",
        "    chunk_texts = []\n",
        "    chunk_sources = []\n",
        "\n",
        "    for file in file_storages:\n",
        "        if not file or not file.filename.lower().endswith(\".pdf\"):\n",
        "            continue\n",
        "        file.stream.seek(0)\n",
        "        reader = PdfReader(py_io.BytesIO(file.read()))\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \" \"\n",
        "        words = text.split()\n",
        "        # Same chunking as your original: chunk_size=500, overlap=50\n",
        "        for i in range(0, len(words), 500 - 50):\n",
        "            chunk = \" \".join(words[i:i + 500])\n",
        "            chunk_texts.append(chunk)\n",
        "            chunk_sources.append(file.filename)\n",
        "\n",
        "    print(f\"\u2705 Created {len(chunk_texts)} chunks with source mapping.\")\n",
        "\n",
        "    # Build FAISS index\n",
        "    build_faiss_index()\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# \ud83e\udde9 STEP 6 \u2014 BUILD FAISS INDEX\n",
        "# (same idea as your original get_faiss_index)\n",
        "# ==============================================\n",
        "def get_faiss_index(embeddings):\n",
        "    print(\"\ud83d\udd04 Building FAISS index...\")\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings.astype(\"float32\"))\n",
        "    print(\"\u2705 FAISS index built successfully.\")\n",
        "    return index\n",
        "\n",
        "def build_faiss_index():\n",
        "    global index\n",
        "    if not chunk_texts:\n",
        "        raise ValueError(\"\u274c No chunks available to build FAISS index.\")\n",
        "\n",
        "    embed_model = get_embed_model()\n",
        "    embeddings = embed_model.encode(chunk_texts, show_progress_bar=True)\n",
        "    embeddings = np.array(embeddings).astype(\"float32\")\n",
        "    index = get_faiss_index(embeddings)\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# \ud83e\udde0 DYNAMIC RELEVANCE FILTER (same logic)\n",
        "# ==============================================\n",
        "def filter_relevant_chunks(user_query, retrieved_chunks):\n",
        "    \"\"\"\n",
        "    Dynamically filters retrieved chunks using query-derived keywords.\n",
        "    Ensures only topically relevant text is used for generation.\n",
        "    \"\"\"\n",
        "    q = user_query.lower()\n",
        "    stopwords = {\n",
        "        \"what\", \"is\", \"are\", \"in\", \"the\", \"of\", \"and\", \"for\", \"to\",\n",
        "        \"explain\", \"describe\", \"how\", \"does\", \"do\", \"difference\", \"between\"\n",
        "    }\n",
        "    query_keywords = [w for w in re.findall(r\"\\w+\", q) if w not in stopwords and len(w) > 2]\n",
        "\n",
        "    scored_chunks = []\n",
        "    for c in retrieved_chunks:\n",
        "        score = sum(k in c.lower() for k in query_keywords)\n",
        "        scored_chunks.append((score, c))\n",
        "\n",
        "    scored_chunks.sort(reverse=True, key=lambda x: x[0])\n",
        "    relevant_chunks = [c for s, c in scored_chunks if s > 0]\n",
        "\n",
        "    # fallback if nothing matches\n",
        "    if len(relevant_chunks) < 1:\n",
        "        relevant_chunks = [c for _, c in scored_chunks[:3]]\n",
        "\n",
        "    return \"\\n\\n\".join(relevant_chunks)\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# \ud83e\udde0 RAG ANSWER MODE \u2014 Full Academic Response\n",
        "# Based on your original rag_query()\n",
        "# ==============================================\n",
        "def rag_answer(user_query, top_k=3):\n",
        "    \"\"\"\n",
        "    Returns full academic RAG answer + sources for a given query.\n",
        "    \"\"\"\n",
        "    if not user_query:\n",
        "        return \"\u26a0\ufe0f No question entered.\", \"\"\n",
        "\n",
        "    if index is None or not chunk_texts:\n",
        "        return \"\u26a0\ufe0f No documents indexed yet. Please upload PDFs first.\", \"\"\n",
        "\n",
        "    embed_model = get_embed_model()\n",
        "    generator = get_llm_pipeline()\n",
        "\n",
        "    query_emb = embed_model.encode([user_query])\n",
        "    query_emb = np.array(query_emb).astype(\"float32\")\n",
        "    D, I = index.search(query_emb, k=top_k)\n",
        "\n",
        "    retrieved_chunks = [chunk_texts[i] for i in I[0]]\n",
        "    retrieved_sources = [chunk_sources[i] for i in I[0]]\n",
        "\n",
        "    # \u2705 Apply dynamic filter\n",
        "    context = filter_relevant_chunks(user_query, retrieved_chunks)\n",
        "    sources_used = \", \".join(sorted(set(retrieved_sources)))\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an academic research assistant.\n",
        "Use ONLY the context provided below to answer factually and in an academic tone.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{user_query}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "    import sys\n",
        "    _stderr = sys.stderr\n",
        "    sys.stderr = py_io.StringIO()\n",
        "\n",
        "    response_raw = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=450,\n",
        "        temperature=0.3,\n",
        "        top_p=0.9,\n",
        "        do_sample=False\n",
        "    )[0][\"generated_text\"]\n",
        "\n",
        "    sys.stderr = _stderr\n",
        "    response_clean = response_raw.split(\"Answer:\")[-1].strip() if \"Answer:\" in response_raw else response_raw.strip()\n",
        "\n",
        "    # Optional log (same idea as original)\n",
        "    try:\n",
        "        with open(\"rag_log.txt\", \"a\", encoding=\"utf-8\") as log:\n",
        "            log.write(f\"Question: {user_query}\\nAnswer: {response_clean}\\nSources: {sources_used}\\n{'='*100}\\n\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f Logging error: {e}\")\n",
        "\n",
        "    gc.collect()\n",
        "    try:\n",
        "        torch.cuda.empty_cache()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return response_clean, sources_used\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# \ud83e\ude75 RAG SUMMARIZATION MODE \u2014 Concise Summary\n",
        "# Based on your original rag_summary()\n",
        "# ==============================================\n",
        "def rag_summary_answer(user_query, top_k=3):\n",
        "    \"\"\"\n",
        "    Returns concise academic summary + sources for a given query.\n",
        "    \"\"\"\n",
        "    if not user_query:\n",
        "        return \"\u26a0\ufe0f No question entered.\", \"\"\n",
        "\n",
        "    if index is None or not chunk_texts:\n",
        "        return \"\u26a0\ufe0f No documents indexed yet. Please upload PDFs first.\", \"\"\n",
        "\n",
        "    embed_model = get_embed_model()\n",
        "    generator = get_llm_pipeline()\n",
        "\n",
        "    query_emb = embed_model.encode([user_query])\n",
        "    query_emb = np.array(query_emb).astype(\"float32\")\n",
        "    D, I = index.search(query_emb, k=top_k)\n",
        "\n",
        "    retrieved_chunks = [chunk_texts[i] for i in I[0]]\n",
        "    retrieved_sources = [chunk_sources[i] for i in I[0]]\n",
        "\n",
        "    # \u2705 Apply dynamic filter\n",
        "    context = filter_relevant_chunks(user_query, retrieved_chunks)\n",
        "    sources_used = \", \".join(sorted(set(retrieved_sources)))\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an academic summarizer.\n",
        "Use ONLY the context below to generate a concise and factual summary that directly answers the user's question.\n",
        "If the text lists multiple items, challenges, or methods, combine or enumerate them clearly.\n",
        "Provide an academically concise summary (4\u20136 sentences).\n",
        "\n",
        "Context:\n",
        "{context[:4000]}\n",
        "\n",
        "Question:\n",
        "{user_query}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "    response = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=220,\n",
        "        temperature=0.25,\n",
        "        top_p=0.9,\n",
        "        do_sample=False\n",
        "    )[0][\"generated_text\"]\n",
        "\n",
        "    if \"Summary:\" in response:\n",
        "        summary_clean = response.split(\"Summary:\")[-1].strip()\n",
        "    else:\n",
        "        summary_clean = response.strip()\n",
        "\n",
        "    summary_clean = summary_clean.replace(\"in clear academic paragraphs:\", \"\").strip()\n",
        "    if not summary_clean.endswith(('.', '!', '?')):\n",
        "        summary_clean = summary_clean.rstrip(\",;:\") + \".\"\n",
        "\n",
        "    # Optional log\n",
        "    try:\n",
        "        with open(\"rag_summary_log.txt\", \"a\", encoding=\"utf-8\") as log:\n",
        "            log.write(f\"Question: {user_query}\\nSummary: {summary_clean}\\nSources: {sources_used}\\n{'='*100}\\n\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f Logging error: {e}\")\n",
        "\n",
        "    gc.collect()\n",
        "    try:\n",
        "        torch.cuda.empty_cache()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return summary_clean, sources_used\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# \ud83c\udf10 MAIN FLASK ROUTE\n",
        "# Upload PDFs once, then ask multiple questions\n",
        "# ==============================================\n",
        "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
        "def home():\n",
        "    global index, chunk_texts, chunk_sources, pdf_files\n",
        "\n",
        "    answer = \"\"\n",
        "    summary = \"\"\n",
        "    sources_answer = \"\"\n",
        "    sources_summary = \"\"\n",
        "    status_msg = \"\"\n",
        "    error = \"\"\n",
        "    mode = \"answer\"  # default radio selection\n",
        "\n",
        "    has_index = index is not None and len(chunk_texts) > 0\n",
        "\n",
        "    if request.method == \"POST\":\n",
        "        action = request.form.get(\"action\")\n",
        "\n",
        "        # ---------------------------------\n",
        "        # \ud83d\udcc2 ACTION: Upload and Index PDFs\n",
        "        # ---------------------------------\n",
        "        if action == \"upload\":\n",
        "            uploaded_files = request.files.getlist(\"pdfs\")\n",
        "            if not uploaded_files or all(f.filename == \"\" for f in uploaded_files):\n",
        "                error = \"\u26a0\ufe0f Please upload at least one academic PDF.\"\n",
        "            else:\n",
        "                try:\n",
        "                    process_uploaded_pdfs(uploaded_files)\n",
        "                    has_index = True\n",
        "                    status_msg = f\"\u2705 Indexed {len(chunk_texts)} chunks from {len(pdf_files)} PDF(s).\"\n",
        "                except Exception as e:\n",
        "                    error = f\"\u26a0\ufe0f Error while processing PDFs: {e}\"\n",
        "\n",
        "        # ---------------------------------\n",
        "        # \ud83d\udd0d ACTION: Ask Question (Answer/Summary)\n",
        "        # ---------------------------------\n",
        "        elif action == \"query\":\n",
        "            user_query = request.form.get(\"query\", \"\").strip()\n",
        "            mode = request.form.get(\"mode\", \"answer\")\n",
        "\n",
        "            if not user_query:\n",
        "                error = \"\u26a0\ufe0f Please enter an academic question.\"\n",
        "            elif index is None or not chunk_texts:\n",
        "                error = \"\u26a0\ufe0f Please upload and index PDFs before asking questions.\"\n",
        "            else:\n",
        "                if mode == \"answer\":\n",
        "                    answer, sources_answer = rag_answer(user_query)\n",
        "                elif mode == \"summary\":\n",
        "                    summary, sources_summary = rag_summary_answer(user_query)\n",
        "                else:\n",
        "                    error = \"\u26a0\ufe0f Unknown mode selected.\"\n",
        "\n",
        "    return render_template(\n",
        "        \"index.html\",\n",
        "        has_index=has_index,\n",
        "        pdf_files=pdf_files,\n",
        "        status_msg=status_msg,\n",
        "        error=error,\n",
        "        answer=answer,\n",
        "        summary=summary,\n",
        "        sources_answer=sources_answer,\n",
        "        sources_summary=sources_summary,\n",
        "        mode=mode,\n",
        "    )\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# \ud83d\ude80 RUN FLASK APP\n",
        "# ==============================================\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host=\"0.0.0.0\", port=8000, debug=False)"
      ],
      "metadata": {
        "id": "WOGaF8tnoe-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Folders**"
      ],
      "metadata": {
        "id": "j0fBySRytvu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p templates\n",
        "!mkdir -p static\n"
      ],
      "metadata": {
        "id": "t0yN_4LlofAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**templates/index.html**"
      ],
      "metadata": {
        "id": "GnWCA_U2tzSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile templates/index.html\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\" />\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
        "    <title>\ud83d\udcda Intelligent Academic Research Assistant</title>\n",
        "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\" />\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"hero-section\">\n",
        "        <div class=\"overlay\"></div>\n",
        "\n",
        "        <div class=\"hero-content\">\n",
        "            <h1>\ud83d\udcda Intelligent Academic Research Assistant</h1>\n",
        "            <p>Upload academic PDFs once, then ask research questions with Mistral + FAISS powered RAG.</p>\n",
        "\n",
        "            <!-- PDF Upload Form -->\n",
        "            <form method=\"post\" enctype=\"multipart/form-data\" class=\"card\">\n",
        "                <input type=\"hidden\" name=\"action\" value=\"upload\" />\n",
        "                <label class=\"file-label\">\n",
        "                    \ud83d\udcc2 Upload academic PDF(s)\n",
        "                    <input type=\"file\" name=\"pdfs\" accept=\".pdf\" multiple required />\n",
        "                </label>\n",
        "                <button type=\"submit\" class=\"btn-primary\">Index PDFs \ud83d\udcd1</button>\n",
        "            </form>\n",
        "\n",
        "            {% if pdf_files %}\n",
        "            <div class=\"status-box\">\n",
        "                <p><strong>\ud83d\udcc1 Uploaded PDFs:</strong></p>\n",
        "                <ul>\n",
        "                    {% for f in pdf_files %}\n",
        "                    <li>{{ f }}</li>\n",
        "                    {% endfor %}\n",
        "                </ul>\n",
        "            </div>\n",
        "            {% endif %}\n",
        "\n",
        "            {% if status_msg %}\n",
        "            <p class=\"status-msg\">{{ status_msg }}</p>\n",
        "            {% endif %}\n",
        "\n",
        "            {% if error %}\n",
        "            <p class=\"error-msg\">{{ error }}</p>\n",
        "            {% endif %}\n",
        "\n",
        "            <!-- Query Form -->\n",
        "            <form method=\"post\" class=\"card\">\n",
        "                <input type=\"hidden\" name=\"action\" value=\"query\" />\n",
        "                <textarea name=\"query\" rows=\"4\" placeholder=\"Enter your academic research question...\" required></textarea>\n",
        "\n",
        "                <div class=\"mode-selector\">\n",
        "                    <label>\n",
        "                        <input type=\"radio\" name=\"mode\" value=\"answer\" {% if mode == \"answer\" %}checked{% endif %} />\n",
        "                        \ud83e\udde0 Full Academic Answer\n",
        "                    </label>\n",
        "                    <label>\n",
        "                        <input type=\"radio\" name=\"mode\" value=\"summary\" {% if mode == \"summary\" %}checked{% endif %} />\n",
        "                        \ud83e\ude75 Concise Academic Summary\n",
        "                    </label>\n",
        "                </div>\n",
        "\n",
        "                <button type=\"submit\" class=\"btn-primary\">Generate Response \ud83d\ude80</button>\n",
        "            </form>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <!-- Results Section -->\n",
        "    {% if answer or summary %}\n",
        "    <div class=\"result-section fade-in\">\n",
        "        <div class=\"result-card\">\n",
        "            {% if answer %}\n",
        "            <h2>\ud83e\udde0 Full Academic Answer</h2>\n",
        "            <p class=\"result-text\">{{ answer }}</p>\n",
        "            {% if sources_answer %}\n",
        "            <p class=\"sources\">\ud83d\udcda Sources used: {{ sources_answer }}</p>\n",
        "            {% endif %}\n",
        "            {% endif %}\n",
        "\n",
        "            {% if summary %}\n",
        "            <h2>\ud83e\ude75 Concise Academic Summary</h2>\n",
        "            <p class=\"result-text\">{{ summary }}</p>\n",
        "            {% if sources_summary %}\n",
        "            <p class=\"sources\">\ud83d\udcda Sources used: {{ sources_summary }}</p>\n",
        "            {% endif %}\n",
        "            {% endif %}\n",
        "        </div>\n",
        "    </div>\n",
        "    {% endif %}\n",
        "</body>\n",
        "</html>\n"
      ],
      "metadata": {
        "id": "jChbppgRofCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**static/style.css**"
      ],
      "metadata": {
        "id": "HyHboU7ft3Rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile static/style.css\n",
        "@import url('https://fonts.googleapis.com/css2?family=Poppins:wght@400;600&display=swap');\n",
        "\n",
        "body {\n",
        "    margin: 0;\n",
        "    font-family: 'Poppins', sans-serif;\n",
        "    color: #fff;\n",
        "}\n",
        "\n",
        "/* Background */\n",
        ".hero-section {\n",
        "    position: relative;\n",
        "    min-height: 100vh;\n",
        "    display: flex;\n",
        "    align-items: center;\n",
        "    padding: 40px 8%;\n",
        "    background: linear-gradient(135deg, #141e30, #243b55);\n",
        "}\n",
        "\n",
        ".overlay {\n",
        "    position: absolute;\n",
        "    inset: 0;\n",
        "    background: rgba(0,0,0,0.35);\n",
        "}\n",
        "\n",
        "/* Main card */\n",
        ".hero-content {\n",
        "    position: relative;\n",
        "    z-index: 2;\n",
        "    max-width: 720px;\n",
        "    background: rgba(255,255,255,0.08);\n",
        "    border-radius: 24px;\n",
        "    padding: 28px 32px;\n",
        "    backdrop-filter: blur(8px);\n",
        "    box-shadow: 0 10px 30px rgba(0,0,0,0.45);\n",
        "}\n",
        "\n",
        "h1 {\n",
        "    margin: 0 0 10px;\n",
        "    font-size: 2rem;\n",
        "    color: #ffdf9e;\n",
        "}\n",
        "\n",
        "p {\n",
        "    margin: 4px 0;\n",
        "}\n",
        "\n",
        "/* Cards */\n",
        ".card {\n",
        "    margin-top: 18px;\n",
        "    padding: 14px 16px;\n",
        "    background: rgba(0,0,0,0.35);\n",
        "    border-radius: 16px;\n",
        "    display: flex;\n",
        "    flex-direction: column;\n",
        "    gap: 10px;\n",
        "}\n",
        "\n",
        "/* File input */\n",
        ".file-label {\n",
        "    font-size: 0.9rem;\n",
        "    background: rgba(255,255,255,0.15);\n",
        "    padding: 10px 12px;\n",
        "    border-radius: 10px;\n",
        "}\n",
        "\n",
        ".file-label input {\n",
        "    display: block;\n",
        "    margin-top: 8px;\n",
        "}\n",
        "\n",
        "/* Textarea */\n",
        "textarea {\n",
        "    width: 100%;\n",
        "    padding: 10px;\n",
        "    border-radius: 10px;\n",
        "    border: none;\n",
        "    resize: vertical;\n",
        "    min-height: 100px;\n",
        "    background: rgba(255,255,255,0.95);\n",
        "    color: #111;\n",
        "    font-size: 0.95rem;\n",
        "}\n",
        "\n",
        "/* Buttons */\n",
        ".btn-primary {\n",
        "    padding: 10px 12px;\n",
        "    border: none;\n",
        "    border-radius: 10px;\n",
        "    background: linear-gradient(135deg, #00c6ff, #0072ff);\n",
        "    color: #fff;\n",
        "    font-weight: 600;\n",
        "    cursor: pointer;\n",
        "    font-size: 0.95rem;\n",
        "    align-self: flex-start;\n",
        "    transition: 0.25s ease;\n",
        "}\n",
        "\n",
        ".btn-primary:hover {\n",
        "    transform: translateY(-2px);\n",
        "    box-shadow: 0 6px 16px rgba(0,0,0,0.35);\n",
        "}\n",
        "\n",
        "/* Status / error */\n",
        ".status-box {\n",
        "    margin-top: 12px;\n",
        "    padding: 10px 12px;\n",
        "    background: rgba(0,0,0,0.35);\n",
        "    border-radius: 10px;\n",
        "    font-size: 0.9rem;\n",
        "}\n",
        "\n",
        ".status-msg {\n",
        "    margin-top: 8px;\n",
        "    color: #b3ffd1;\n",
        "    font-size: 0.9rem;\n",
        "}\n",
        "\n",
        ".error-msg {\n",
        "    margin-top: 8px;\n",
        "    color: #ffb3b3;\n",
        "    font-size: 0.9rem;\n",
        "}\n",
        "\n",
        "/* Mode selector */\n",
        ".mode-selector {\n",
        "    display: flex;\n",
        "    gap: 16px;\n",
        "    font-size: 0.9rem;\n",
        "}\n",
        "\n",
        ".mode-selector label {\n",
        "    display: flex;\n",
        "    align-items: center;\n",
        "    gap: 6px;\n",
        "}\n",
        "\n",
        "/* Results */\n",
        ".result-section {\n",
        "    background: #0d111f;\n",
        "    padding: 40px 20px;\n",
        "    display: flex;\n",
        "    justify-content: center;\n",
        "}\n",
        "\n",
        ".result-card {\n",
        "    max-width: 900px;\n",
        "    width: 100%;\n",
        "    background: rgba(255,255,255,0.05);\n",
        "    padding: 24px 28px;\n",
        "    border-radius: 16px;\n",
        "    color: #e2e5ff;\n",
        "}\n",
        "\n",
        ".result-card h2 {\n",
        "    margin-top: 0;\n",
        "}\n",
        "\n",
        ".result-text {\n",
        "    white-space: pre-wrap;\n",
        "    line-height: 1.5;\n",
        "    margin-bottom: 10px;\n",
        "}\n",
        "\n",
        ".sources {\n",
        "    font-size: 0.85rem;\n",
        "    color: #a9b8ff;\n",
        "}\n",
        "\n",
        "/* Animation */\n",
        ".fade-in {\n",
        "    animation: fadeInUp 0.6s ease forwards;\n",
        "}\n",
        "\n",
        "@keyframes fadeInUp {\n",
        "    from { opacity: 0; transform: translateY(18px); }\n",
        "    to   { opacity: 1; transform: translateY(0); }\n",
        "}\n"
      ],
      "metadata": {
        "id": "DRzRmoNHofEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Flask + ngrok**"
      ],
      "metadata": {
        "id": "1GxQNsyWt7sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kill existing processes\n",
        "!pkill -f flask || echo \"No flask running\"\n",
        "!pkill -f ngrok || echo \"No ngrok running\""
      ],
      "metadata": {
        "id": "8TW1CbExo43g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i :8000"
      ],
      "metadata": {
        "id": "9QwlkEADo4xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) If a specific PID is blocking:\n",
        "!kill -9 617"
      ],
      "metadata": {
        "id": "e7TKvx5no4ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Flask in background\n",
        "!nohup python app.py > flask.log 2>&1 &"
      ],
      "metadata": {
        "id": "CtPZ8aVEo4qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Start ngrok\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# Enter your NGROK auth token here\n",
        "conf.get_default().auth_token = \"INPUT_YOUR_NGROK_TOKEN_HERE\"\n",
        "\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"\ud83c\udf0d Public URL:\", public_url)\n"
      ],
      "metadata": {
        "id": "UT0F3-jRofGn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}