{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1yeRZOt7O3P"
      },
      "source": [
        "## \ud83d\udcd8 How to Use Kaggle (Upload Dataset & Notebook)\n",
        "\n",
        "### \u2705 Step 1: Create Kaggle Account\n",
        "- Go to \ud83d\udc49 https://www.kaggle.com  \n",
        "- Sign in using Google / Email\n",
        "\n",
        "---\n",
        "\n",
        "### \u2705 Step 2: Upload Your Dataset\n",
        "1. Click **Datasets** \u2192 **Create New Dataset**\n",
        "2. Upload your **dataset folder or ZIP file**\n",
        "3. Add:\n",
        "   - Dataset name\n",
        "   - Short description\n",
        "4. Set visibility \u2192 **Public / Private**\n",
        "5. Click **Create**\n",
        "\n",
        "\u2705 After upload, Kaggle gives a dataset path like:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptNeyKl37O3Q"
      },
      "source": [
        "### \ud83e\ude7a Anemia Detection from Eye & Nail Images using EfficientNetV2-S + MixStyle + Focal Loss\n",
        "\n",
        "## \ud83d\udd0d Project Overview\n",
        "\n",
        "This notebook trains a **Keras 3\u2013compatible deep learning model** to detect **anemia** from medical images of:\n",
        "\n",
        "- \ud83d\udc41 Conjunctiva (eye)\n",
        "- \ud83d\udc85 Fingernails\n",
        "\n",
        "The pipeline includes:\n",
        "\n",
        "- **EfficientNetV2-S** backbone for feature extraction  \n",
        "- **Custom MixStyle layer** for domain generalization  \n",
        "- **BalancedSequence sampler** to handle class imbalance  \n",
        "- **Focal loss** to focus on hard examples  \n",
        "- Stratified Train/Validation/Test split  \n",
        "- Final evaluation with classification report & confusion matrix  \n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udcda Key Libraries (Official Docs)\n",
        "\n",
        "- TensorFlow / Keras \u2192 https://www.tensorflow.org  \n",
        "- OpenCV \u2192 https://opencv.org  \n",
        "- NumPy \u2192 https://numpy.org  \n",
        "- Pandas \u2192 https://pandas.pydata.org  \n",
        "- Matplotlib \u2192 https://matplotlib.org  \n",
        "- Seaborn \u2192 https://seaborn.pydata.org  \n",
        "- scikit-learn \u2192 https://scikit-learn.org  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE_kRCy97O3R"
      },
      "source": [
        "### \ud83d\udfe2 Section 1: Imports, Reproducibility & TensorFlow Version\n",
        "\n",
        "In this section we:\n",
        "\n",
        "- Import core Python & ML libraries:\n",
        "  - `os`, `random`, `math`, `gc` \u2192 utilities & memory handling\n",
        "  - `numpy`, `pandas` \u2192 numerical computation & tabular data  \n",
        "    - NumPy docs: https://numpy.org  \n",
        "    - Pandas docs: https://pandas.pydata.org  \n",
        "  - `cv2` \u2192 image loading & processing  \n",
        "    - OpenCV docs: https://docs.opencv.org  \n",
        "  - `matplotlib`, `seaborn` \u2192 plotting & visualization  \n",
        "    - Matplotlib: https://matplotlib.org  \n",
        "    - Seaborn: https://seaborn.pydata.org  \n",
        "  - `tensorflow.keras` \u2192 model definition & training  \n",
        "    - TensorFlow/Keras: https://www.tensorflow.org  \n",
        "  - `sklearn` \u2192 splitting and evaluation  \n",
        "    - scikit-learn: https://scikit-learn.org  \n",
        "\n",
        "- Set a fixed random seed (`SEED = 42`) for:\n",
        "  - `random`\n",
        "  - `numpy`\n",
        "  - `tf.random`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T09:03:32.372052Z",
          "iopub.status.busy": "2025-11-29T09:03:32.371573Z",
          "iopub.status.idle": "2025-11-29T09:03:48.450193Z",
          "shell.execute_reply": "2025-11-29T09:03:48.449501Z",
          "shell.execute_reply.started": "2025-11-29T09:03:32.372018Z"
        },
        "trusted": true,
        "id": "I7zIWXEc7O3R"
      },
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "#   FINAL KERAS-3 COMPATIBLE ANEMIA DETECTION TRAINING NOTEBOOK\n",
        "#   EfficientNetV2-S + MixStyle + BalancedSampler + Focal Loss\n",
        "# ==============================================================\n",
        "\n",
        "import os, random, math, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"TF VERSION:\", tf.__version__)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cFKpJd37O3S"
      },
      "source": [
        "### \ud83d\udfe2 Section 2: Dataset Paths & File Collection\n",
        "\n",
        "Here we define the directory structure for our anemia dataset:\n",
        "\n",
        "- **Conjunctiva (eye) images**  \n",
        "  - Anemic: `\"Anemia Detection/Anemic\"`  \n",
        "  - Non-Anemic: `\"Anemia Detection/Non Anemia\"`  \n",
        "\n",
        "- **Nail images**  \n",
        "  - Anemic: `\"Fingernails/Anemic\"`  \n",
        "  - Non-Anemic: `\"Fingernails/NonAnemic\"`  \n",
        "\n",
        "Using the helper function `collect_paths()` we:\n",
        "\n",
        "- Scan each folder for image files (`.png`, `.jpg`, `.jpeg`)  \n",
        "- Build a list of dictionaries with:\n",
        "  - `image_path`\n",
        "  - `label` (1 = anemic, 0 = non-anemic)\n",
        "  - `source` (`\"conj\"` or `\"nail\"`)\n",
        "\n",
        "Then we construct a **Pandas DataFrame** `df` from this list and print:\n",
        "\n",
        "- Total number of images\n",
        "- Number of images per source type (eye vs nail)\n",
        "\n",
        "This gives us a **single unified metadata table** for both modalities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-rtQ5yf7O3S"
      },
      "source": [
        "**Dataset Path**: https://www.kaggle.com/datasets/huebitsvizg/anemia-detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T09:03:48.451974Z",
          "iopub.status.busy": "2025-11-29T09:03:48.451489Z",
          "iopub.status.idle": "2025-11-29T09:03:48.456415Z",
          "shell.execute_reply": "2025-11-29T09:03:48.455723Z",
          "shell.execute_reply.started": "2025-11-29T09:03:48.451955Z"
        },
        "trusted": true,
        "id": "MZ63Z3Lc7O3S"
      },
      "outputs": [],
      "source": [
        "# 1. Dataset Paths (Modify if needed)\n",
        "paths = {\n",
        "    \"conj_anemic\":     \"/kaggle/input/anemia-using-eyes/Anemia Detection/Anemic\",\n",
        "    \"conj_nonanemic\":  \"/kaggle/input/anemia-using-eyes/Anemia Detection/Non Anemia\",\n",
        "    \"nail_anemic\":     \"/kaggle/input/anemia-detection-nails/Fingernails/Anemic\",\n",
        "    \"nail_nonanemic\":  \"/kaggle/input/anemia-detection-nails/Fingernails/NonAnemic\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T09:03:48.457655Z",
          "iopub.status.busy": "2025-11-29T09:03:48.457277Z",
          "iopub.status.idle": "2025-11-29T09:03:48.649275Z",
          "shell.execute_reply": "2025-11-29T09:03:48.648487Z",
          "shell.execute_reply.started": "2025-11-29T09:03:48.457631Z"
        },
        "trusted": true,
        "id": "uzUAm15H7O3S"
      },
      "outputs": [],
      "source": [
        "# 2. Load Filepaths Into a DataFrame\n",
        "def collect_paths(folder, label, source):\n",
        "    rows=[]\n",
        "    if not os.path.exists(folder):\n",
        "        print(\"Missing:\", folder)\n",
        "        return rows\n",
        "    for f in os.listdir(folder):\n",
        "        if f.lower().endswith(('.png','.jpg','.jpeg')):\n",
        "            rows.append({\"image_path\": os.path.join(folder,f),\n",
        "                         \"label\": label,\n",
        "                         \"source\": source})\n",
        "    return rows\n",
        "\n",
        "data = []\n",
        "data += collect_paths(paths[\"conj_anemic\"], 1, \"conj\")\n",
        "data += collect_paths(paths[\"conj_nonanemic\"], 0, \"conj\")\n",
        "data += collect_paths(paths[\"nail_anemic\"], 1, \"nail\")\n",
        "data += collect_paths(paths[\"nail_nonanemic\"], 0, \"nail\")\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Total images:\", len(df))\n",
        "print(df.source.value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVLvV9vM7O3S"
      },
      "source": [
        "## \ud83d\udfe2 Section 3: Select Image Modality (Conjunctiva or Nails)\n",
        "\n",
        "Our pipeline supports **two different input types**:\n",
        "\n",
        "- `\"conj\"` \u2192 conjunctiva (eye) images  \n",
        "- `\"nail\"` \u2192 fingernail images  \n",
        "\n",
        "We select a modality using:\n",
        "\n",
        "```python\n",
        "modality = \"conj\"   # or \"nail\"\n",
        "df_mod = df[df.source == modality].reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T09:03:48.650205Z",
          "iopub.status.busy": "2025-11-29T09:03:48.649973Z",
          "iopub.status.idle": "2025-11-29T09:03:48.658614Z",
          "shell.execute_reply": "2025-11-29T09:03:48.657900Z",
          "shell.execute_reply.started": "2025-11-29T09:03:48.650188Z"
        },
        "trusted": true,
        "id": "5OaSTF3J7O3T"
      },
      "outputs": [],
      "source": [
        "# 3. Choose ONE Modality (conj OR nail)\n",
        "modality = \"conj\"   # or \"nail\"\n",
        "df_mod = df[df.source==modality].reset_index(drop=True)\n",
        "print(\"Using modality:\", modality, \"| Images:\", len(df_mod))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOfeTljt7O3T"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udfe6 Markdown Cell  \u2014 Image Preprocessing (WB + CLAHE)\n",
        "\n",
        "```markdown\n",
        "## \ud83d\udfe2 Section 4: Image Preprocessing \u2013 White Balance & CLAHE\n",
        "\n",
        "To reduce the impact of lighting, skin tone, and acquisition device differences, we define a robust preprocessing pipeline:\n",
        "\n",
        "1. **Gray-World White Balance (`white_balance_grayworld`)**\n",
        "   - Estimates average color of the image\n",
        "   - Adjusts each channel so overall color balance is neutral\n",
        "   - Helps correct global color cast (e.g., yellowish/blueish tint)\n",
        "\n",
        "2. **CLAHE in LAB Color Space (`apply_clahe_rgb`)**\n",
        "   - Converts BGR \u2192 LAB\n",
        "   - Applies **Contrast Limited Adaptive Histogram Equalization** (CLAHE) on the L (lightness) channel  \n",
        "     - OpenCV CLAHE docs: https://docs.opencv.org/ \u2192 search *CLAHE*\n",
        "   - Enhances local contrast while controlling noise\n",
        "\n",
        "3. **Final Preprocessing (`preprocess_image`)**\n",
        "   - Read image from path with `cv2.imread`\n",
        "   - Apply white balance + CLAHE\n",
        "   - Resize to `(380, 380)`\n",
        "   - Convert BGR \u2192 RGB\n",
        "   - Normalize to `[0, 1]` and cast to `float32`\n",
        "\n",
        "This preprocessing pipeline is crucial in **medical imaging**, where subtle color variations may indicate anemia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T09:03:48.660784Z",
          "iopub.status.busy": "2025-11-29T09:03:48.660587Z",
          "iopub.status.idle": "2025-11-29T09:03:48.674514Z",
          "shell.execute_reply": "2025-11-29T09:03:48.673860Z",
          "shell.execute_reply.started": "2025-11-29T09:03:48.660769Z"
        },
        "trusted": true,
        "id": "V75w3RQn7O3T"
      },
      "outputs": [],
      "source": [
        "# 4. Preprocessing Functions (White balance + CLAHE)\n",
        "def white_balance_grayworld(img):\n",
        "    img = img.astype(np.float32)\n",
        "    avg = np.mean(img)\n",
        "    for c in range(3):\n",
        "        img[:,:,c] *= avg/(np.mean(img[:,:,c])+1e-6)\n",
        "    return np.clip(img,0,255).astype(np.uint8)\n",
        "\n",
        "def apply_clahe_rgb(img):\n",
        "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "    l,a,b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0)\n",
        "    l = clahe.apply(l)\n",
        "    lab = cv2.merge((l,a,b))\n",
        "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "def preprocess_image(path, size=(380,380)):\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        return None\n",
        "    img = white_balance_grayworld(img)\n",
        "    img = apply_clahe_rgb(img)\n",
        "    img = cv2.resize(img, size)\n",
        "    img = img[:,:,::-1]/255.0  # RGB\n",
        "    return img.astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR_kageH7O3T"
      },
      "source": [
        "### \ud83d\udfe2 Section 5: MixStyle Layer for Domain Generalization\n",
        "\n",
        "This section implements a custom **MixStyle** Keras layer, inspired by domain generalization techniques:\n",
        "\n",
        "- Works **only during training** (checked via `training` flag)\n",
        "- Randomly perturbs **feature statistics** (mean and std) across samples in a batch\n",
        "- Encourages the model to focus on **shape & structure**, not just specific color distributions\n",
        "\n",
        "Key Steps:\n",
        "\n",
        "- Compute per-sample mean and variance over spatial dimensions\n",
        "- Normalize features\n",
        "- Randomly permute batch indices and mix statistics (`mu`, `sigma`)\n",
        "- Reconstruct features with mixed style statistics\n",
        "\n",
        "Keras custom layers reference:  \n",
        "https://www.tensorflow.org/guide/keras/custom_layers_and_models  \n",
        "\n",
        "MixStyle helps the model generalize better to unseen lighting conditions, cameras, or skin tones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T09:03:48.675496Z",
          "iopub.status.busy": "2025-11-29T09:03:48.675182Z",
          "iopub.status.idle": "2025-11-29T09:03:48.690344Z",
          "shell.execute_reply": "2025-11-29T09:03:48.689732Z",
          "shell.execute_reply.started": "2025-11-29T09:03:48.675478Z"
        },
        "trusted": true,
        "id": "38M4AME97O3U"
      },
      "outputs": [],
      "source": [
        "# 5. MixStyle (Training-only)\n",
        "class MixStyle(layers.Layer):\n",
        "    def __init__(self, p=0.5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.p = p\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        if not training:\n",
        "            return x\n",
        "\n",
        "        # sample once per batch\n",
        "        apply_mix = tf.random.uniform([])\n",
        "\n",
        "        # Use TF conditional instead of Python if/else\n",
        "        return tf.cond(\n",
        "            apply_mix < self.p,\n",
        "            lambda: self._mix(x),\n",
        "            lambda: x\n",
        "        )\n",
        "\n",
        "    def _mix(self, x):\n",
        "        mu = tf.reduce_mean(x, axis=[1,2], keepdims=True)\n",
        "        var = tf.math.reduce_variance(x, axis=[1,2], keepdims=True)\n",
        "        sigma = tf.sqrt(var + 1e-6)\n",
        "\n",
        "        x_norm = (x - mu) / sigma\n",
        "\n",
        "        bs = tf.shape(x)[0]\n",
        "        perm = tf.random.shuffle(tf.range(bs))\n",
        "\n",
        "        mu2    = tf.gather(mu, perm)\n",
        "        sigma2 = tf.gather(sigma, perm)\n",
        "\n",
        "        lam = tf.random.uniform((bs,1,1,1))\n",
        "\n",
        "        mu_mix  = lam * mu + (1-lam) * mu2\n",
        "        sigma_mix = lam * sigma + (1-lam) * sigma2\n",
        "\n",
        "        return x_norm * sigma_mix + mu_mix\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"p\": self.p}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7T5cZUx7O3U"
      },
      "source": [
        "### \ud83d\udfe2 Section 6: BalancedSequence \u2013 Class-Balanced Data Generator\n",
        "\n",
        "This section defines `BalancedSequence`, a custom **Keras `Sequence`** that:\n",
        "\n",
        "- Ensures **balanced batches**:\n",
        "  - Half of each batch are label `0` (non-anemic)\n",
        "  - Half are label `1` (anemic)\n",
        "- Loads images using `preprocess_image()`\n",
        "- Applies simple augmentation (random horizontal flip) when `aug=True`\n",
        "- Applies **EfficientNetV2 preprocessing** (`eff_v2_pre`) **outside** the model graph  \n",
        "  - EfficientNetV2 Keras docs:  \n",
        "    https://keras.io/api/applications/efficientnet_v2/\n",
        "\n",
        "Why use `Sequence`?\n",
        "\n",
        "- Keras `Sequence` is:\n",
        "  - Thread-safe\n",
        "  - Works well with `model.fit`\n",
        "  - Recommended for large datasets in TF/Keras 3\n",
        "\n",
        "Balanced sampling is particularly important in **medical applications** where disease-positive cases may be relatively rare.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T09:03:48.691381Z",
          "iopub.status.busy": "2025-11-29T09:03:48.691143Z",
          "iopub.status.idle": "2025-11-29T09:03:48.717523Z",
          "shell.execute_reply": "2025-11-29T09:03:48.716933Z",
          "shell.execute_reply.started": "2025-11-29T09:03:48.691359Z"
        },
        "trusted": true,
        "id": "hc_APGpA7O3U"
      },
      "outputs": [],
      "source": [
        "# 6. BalancedSequence \u2014 SAFE VERSION\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input as eff_v2_pre\n",
        "\n",
        "class BalancedSequence(Sequence):\n",
        "    def __init__(self, df, batch=32, size=(380,380), aug=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.batch = batch\n",
        "        self.size = size\n",
        "        self.aug = aug\n",
        "\n",
        "        self.cls_idx = {\n",
        "            0: self.df.index[self.df.label==0].tolist(),\n",
        "            1: self.df.index[self.df.label==1].tolist()\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.df)/self.batch)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        imgs=[]\n",
        "        labels=[]\n",
        "\n",
        "        # equal samples per class\n",
        "        per_class = self.batch//2\n",
        "\n",
        "        for c in [0,1]:\n",
        "            for _ in range(per_class):\n",
        "                i = random.choice(self.cls_idx[c])\n",
        "                row = self.df.loc[i]\n",
        "                img = preprocess_image(row.image_path, self.size)\n",
        "                if img is None: continue\n",
        "                if self.aug and random.random()<0.5:\n",
        "                    img = np.fliplr(img)\n",
        "                imgs.append(img)\n",
        "                labels.append([float(c)])\n",
        "\n",
        "        imgs    = np.array(imgs)\n",
        "        labels  = np.array(labels)\n",
        "\n",
        "        # preprocess_input here (NOT inside model)\n",
        "        imgs = eff_v2_pre(imgs*255.0)\n",
        "\n",
        "        return imgs, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ0U7qK67O3U"
      },
      "source": [
        "### \ud83d\udfe2 Section 7: Model Architecture \u2013 EfficientNetV2-S + MixStyle Head\n",
        "\n",
        "We now define the Keras model:\n",
        "\n",
        "- **Backbone**: `EfficientNetV2S` (pretrained on ImageNet)\n",
        "  - `include_top=False` \u2192 remove default classifier\n",
        "  - Input shape = `(380, 380, 3)`\n",
        "  - Official Keras docs:  \n",
        "    https://keras.io/api/applications/efficientnet_v2/\n",
        "\n",
        "- **Custom Classification Head**:\n",
        "  1. `MixStyle` layer applied to backbone feature maps\n",
        "  2. `GlobalAveragePooling2D` to convert feature maps to a vector\n",
        "  3. Dense(256, ReLU) + Dropout(0.5)\n",
        "  4. Dense(1, sigmoid) for **binary classification** (anemic vs non-anemic)\n",
        "\n",
        "This design leverages strong pretrained features while still allowing the model to learn anemia-specific patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T09:03:48.718428Z",
          "iopub.status.busy": "2025-11-29T09:03:48.718200Z",
          "iopub.status.idle": "2025-11-29T09:03:54.333748Z",
          "shell.execute_reply": "2025-11-29T09:03:54.332984Z",
          "shell.execute_reply.started": "2025-11-29T09:03:48.718402Z"
        },
        "trusted": true,
        "id": "QKhZd6NJ7O3U"
      },
      "outputs": [],
      "source": [
        "# 7. Build Keras-3 Safe Model (NO preprocess inside graph)\n",
        "IMG_SIZE = (380,380,3)\n",
        "\n",
        "def build_model():\n",
        "    inp = layers.Input(shape=IMG_SIZE)\n",
        "    base = tf.keras.applications.EfficientNetV2S(\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\",\n",
        "        input_tensor=inp\n",
        "    )\n",
        "    x = MixStyle(p=0.5)(base.output)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    return models.Model(inp, out)\n",
        "\n",
        "model = build_model()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2i3w94L7O3U"
      },
      "source": [
        "### \ud83d\udfe2 Section 8: Focal Loss for Imbalanced Binary Classification\n",
        "\n",
        "Here we define **Focal Loss**, which improves learning in class-imbalanced settings:\n",
        "\n",
        "- Extends Binary Cross-Entropy by down-weighting **easy** examples\n",
        "- Focuses training on **hard, misclassified** examples\n",
        "- Parameters:\n",
        "  - `gamma` (typically 2.0) \u2192 focusing parameter\n",
        "  - `alpha` (typically 0.25) \u2192 weight for positive/negative balance\n",
        "\n",
        "Original focal loss paper (for reference):  \n",
        "https://arxiv.org/abs/1708.02002  \n",
        "\n",
        "This loss is widely used in medical imaging, object detection, and rare event classification tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T09:03:54.334801Z",
          "iopub.status.busy": "2025-11-29T09:03:54.334524Z",
          "iopub.status.idle": "2025-11-29T09:03:54.339987Z",
          "shell.execute_reply": "2025-11-29T09:03:54.339310Z",
          "shell.execute_reply.started": "2025-11-29T09:03:54.334778Z"
        },
        "trusted": true,
        "id": "X6gTrje67O3U"
      },
      "outputs": [],
      "source": [
        "# 8. Focal Loss\n",
        "def focal_loss(y_true,y_pred,gamma=2.0,alpha=0.25):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    eps=1e-7\n",
        "    y_pred=tf.clip_by_value(y_pred,eps,1-eps)\n",
        "    bce=-(y_true*tf.math.log(y_pred)+(1-y_true)*tf.math.log(1-y_pred))\n",
        "    w=alpha*tf.pow(1-y_pred,gamma)*y_true + (1-alpha)*tf.pow(y_pred,gamma)*(1-y_true)\n",
        "    return tf.reduce_mean(w*bce)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcHrzN5r7O3V"
      },
      "source": [
        "### \ud83d\udfe2 Section 9: Train / Validation / Test Split\n",
        "\n",
        "In this section we:\n",
        "\n",
        "- Split `df_mod` (chosen modality) into:\n",
        "  - 80% \u2192 train+val\n",
        "  - 20% \u2192 test (held out)\n",
        "- Further split the training part:\n",
        "  - 90% \u2192 actual training set\n",
        "  - 10% \u2192 validation set\n",
        "\n",
        "We use `train_test_split` from scikit-learn with:\n",
        "\n",
        "- `stratify=df_mod.label` to preserve class balance\n",
        "- `random_state=SEED` for reproducibility\n",
        "\n",
        "scikit-learn train_test_split docs:  \n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html  \n",
        "\n",
        "Indices are reset to avoid indexing issues later when building generators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T09:03:54.340896Z",
          "iopub.status.busy": "2025-11-29T09:03:54.340704Z",
          "iopub.status.idle": "2025-11-29T09:03:54.365626Z",
          "shell.execute_reply": "2025-11-29T09:03:54.365039Z",
          "shell.execute_reply.started": "2025-11-29T09:03:54.340872Z"
        },
        "trusted": true,
        "id": "WIwp_l-H7O3V"
      },
      "outputs": [],
      "source": [
        "# 9. Train / Val / Test Split\n",
        "train_df, test_df = train_test_split(df_mod, test_size=0.20,\n",
        "                                     stratify=df_mod.label, random_state=SEED)\n",
        "\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.10,\n",
        "                                    stratify=train_df.label, random_state=SEED)\n",
        "\n",
        "# FIX\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df   = val_df.reset_index(drop=True)\n",
        "test_df  = test_df.reset_index(drop=True)\n",
        "\n",
        "print(\"Train:\",len(train_df),\"Val:\",len(val_df),\"Test:\",len(test_df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCP7S8Fp7O3V"
      },
      "source": [
        "### \ud83d\udfe2 Section 10: Create Balanced Train & Validation Sequences\n",
        "\n",
        "We now build the actual data generators:\n",
        "\n",
        "```python\n",
        "train_seq = BalancedSequence(train_df, batch=32, aug=True)\n",
        "val_seq   = BalancedSequence(val_df,   batch=32, aug=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T09:03:54.366560Z",
          "iopub.status.busy": "2025-11-29T09:03:54.366312Z",
          "iopub.status.idle": "2025-11-29T09:03:54.372146Z",
          "shell.execute_reply": "2025-11-29T09:03:54.371357Z",
          "shell.execute_reply.started": "2025-11-29T09:03:54.366543Z"
        },
        "trusted": true,
        "id": "_71JN7tg7O3V"
      },
      "outputs": [],
      "source": [
        "# 10. Create Sequences\n",
        "train_seq = BalancedSequence(train_df, batch=32, aug=True)\n",
        "val_seq   = BalancedSequence(val_df,   batch=32, aug=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zewphQXL7O3V"
      },
      "source": [
        "\n",
        "### \ud83d\udfe2 Section 11: Initial Training (Frozen Backbone)\n",
        "\n",
        "Here we:\n",
        "\n",
        "- Compile the model with:\n",
        "  - Optimizer: `Adam(1e-4)`\n",
        "  - Loss: `focal_loss`\n",
        "  - Metric: `\"accuracy\"`\n",
        "- Define callbacks:\n",
        "  - `ReduceLROnPlateau` \u2192 reduce learning rate when `val_loss` plateaus  \n",
        "    Docs: https://keras.io/api/callbacks/reduce_lr_on_plateau/  \n",
        "  - `EarlyStopping` \u2192 stop early if `val_loss` stops improving  \n",
        "    Docs: https://keras.io/api/callbacks/early_stopping/\n",
        "\n",
        "This stage usually trains primarily the **top layers** and stabilizes the classifier before full fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T09:03:54.373039Z",
          "iopub.status.busy": "2025-11-29T09:03:54.372787Z",
          "iopub.status.idle": "2025-11-29T09:10:00.446623Z",
          "shell.execute_reply": "2025-11-29T09:10:00.445939Z",
          "shell.execute_reply.started": "2025-11-29T09:03:54.372998Z"
        },
        "trusted": true,
        "id": "KVrDAdny7O3V"
      },
      "outputs": [],
      "source": [
        "# 11. Compile & Train (HEAD ONLY)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    loss=focal_loss,\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "cb = [\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",factor=0.5,patience=2),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=4,restore_best_weights=True),\n",
        "]\n",
        "\n",
        "history = model.fit(train_seq, validation_data=val_seq, epochs=4, callbacks=cb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4dS_aFN7O3V"
      },
      "source": [
        "### \ud83d\udfe2 Section 12: Fine-Tuning the Last 40 Layers\n",
        "\n",
        "After initial training, we:\n",
        "\n",
        "- Unfreeze the **last 40 layers** of the model:\n",
        "  ```python\n",
        "  for layer in model.layers[-40:]:\n",
        "      layer.trainable = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T09:10:00.447947Z",
          "iopub.status.busy": "2025-11-29T09:10:00.447690Z",
          "iopub.status.idle": "2025-11-29T09:17:37.931909Z",
          "shell.execute_reply": "2025-11-29T09:17:37.931035Z",
          "shell.execute_reply.started": "2025-11-29T09:10:00.447920Z"
        },
        "trusted": true,
        "id": "d7LiTbSg7O3V"
      },
      "outputs": [],
      "source": [
        "# 12. FINE-TUNE LAST 40 LAYERS\n",
        "for layer in model.layers[-40:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    loss=focal_loss,\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history2 = model.fit(train_seq, validation_data=val_seq, epochs=6, callbacks=cb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-N5ewm27O3V"
      },
      "source": [
        "### \ud83d\udfe2 Section 13: Test Set Evaluation & Model Export\n",
        "\n",
        "In the final section we:\n",
        "\n",
        "1. Loop over all images in `test_df`\n",
        "2. Apply `preprocess_image()` + EfficientNetV2 preprocessing (`eff_v2_pre`)\n",
        "3. Use `model.predict()` to obtain probabilities\n",
        "4. Convert probabilities to binary predictions (`pred > 0.5`)\n",
        "5. Compute:\n",
        "   - `classification_report` (precision, recall, F1-score)  \n",
        "     Docs: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html  \n",
        "   - `confusion_matrix` to inspect true/false positives/negatives  \n",
        "     Docs: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html  \n",
        "\n",
        "6. Save the final model as:\n",
        "```python\n",
        "/kaggle/working/effnetv2s_balance_final.keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T09:17:37.935077Z",
          "iopub.status.busy": "2025-11-29T09:17:37.934453Z",
          "iopub.status.idle": "2025-11-29T09:18:18.467214Z",
          "shell.execute_reply": "2025-11-29T09:18:18.466442Z",
          "shell.execute_reply.started": "2025-11-29T09:17:37.935055Z"
        },
        "trusted": true,
        "id": "f3l_TVcq7O3V"
      },
      "outputs": [],
      "source": [
        "# 13. EVALUATE ON TEST SET\n",
        "X=[]; Y=[]\n",
        "for _,row in test_df.iterrows():\n",
        "    img = preprocess_image(row.image_path,(380,380))\n",
        "    X.append(img); Y.append(row.label)\n",
        "\n",
        "X = np.array(X)\n",
        "X = eff_v2_pre(X*255.0)\n",
        "Y = np.array(Y)\n",
        "\n",
        "pred = model.predict(X)\n",
        "pred_bin = (pred>0.5).astype(int).ravel()\n",
        "\n",
        "print(classification_report(Y,pred_bin,digits=4))\n",
        "print(confusion_matrix(Y,pred_bin))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T09:20:01.540829Z",
          "iopub.status.busy": "2025-11-29T09:20:01.540049Z",
          "iopub.status.idle": "2025-11-29T09:20:03.838184Z",
          "shell.execute_reply": "2025-11-29T09:20:03.837422Z",
          "shell.execute_reply.started": "2025-11-29T09:20:01.540798Z"
        },
        "trusted": true,
        "id": "pLVtlp9k7O3V"
      },
      "outputs": [],
      "source": [
        "model.save(\"/kaggle/working/effnetv2s_balance_final.keras\")\n",
        "print(\"Model saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "4pJmjCEj7O3V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgMcRFfe7O3V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVNw2Wdq7O3W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqG_JUlH7O3W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lmmByiX7O3W"
      },
      "source": [
        "# Run this in colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx1q-ctw7GcN"
      },
      "source": [
        "## \ud83e\ude7a MedVision: AI-Based Anemia Detection Web App (Flask + EfficientNetV2)\n",
        "\n",
        "### \ud83d\udccc Project Overview\n",
        "\n",
        "This project deploys a **deep learning\u2013based anemia detection system** using:\n",
        "- A trained **EfficientNetV2-S model**\n",
        "- Medical image preprocessing (White Balance + CLAHE)\n",
        "- **Flask** for web deployment\n",
        "- **ngrok** for public access from Google Colab\n",
        "\n",
        "Users upload an eye or nail image \u2192 the system predicts:\n",
        "- \u2705 Non-Anemic  \n",
        "- \ud83d\udea8 Anemic  \n",
        "along with a **confidence score**.\n",
        "\n",
        "---\n",
        "\n",
        "### \u2705 Technologies Used (Official Links)\n",
        "\n",
        "- TensorFlow / Keras \u2192 https://www.tensorflow.org  \n",
        "- Flask \u2192 https://flask.palletsprojects.com  \n",
        "- OpenCV \u2192 https://opencv.org  \n",
        "- NumPy \u2192 https://numpy.org  \n",
        "- Pillow \u2192 https://python-pillow.org  \n",
        "- ngrok \u2192 https://ngrok.com  \n",
        "- Google Colab \u2192 https://colab.research.google.com  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJV7JQRh7GcO"
      },
      "source": [
        "####  Cell 1: Install Dependencies & Mount Google Drive\n",
        "\n",
        "This cell prepares the **deployment environment** by:\n",
        "- Installing required libraries for deep learning and web deployment\n",
        "- Mounting Google Drive to load the trained model\n",
        "- Creating folders for HTML templates and static files\n",
        "\n",
        "---\n",
        "\n",
        "####  Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ebq4uuyyw0hB"
      },
      "outputs": [],
      "source": [
        "!pip install flask pyngrok opencv-python pillow numpy tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1V2IuKbxIGy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGqP57DpxVq4"
      },
      "outputs": [],
      "source": [
        "!mkdir -p templates static"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkOLKzWv7GcQ"
      },
      "source": [
        "\n",
        "---\n",
        "##  Cell 2: Flask Backend & Model Loading\n",
        "\n",
        "This cell creates the **Flask backend application** and:\n",
        "- Loads the trained **EfficientNetV2-S anemia model**\n",
        "- Registers the custom `MixStyle` layer for compatibility\n",
        "- Prepares upload directory and image size configuration\n",
        "\n",
        "---\n",
        "\n",
        "###  Model Path Configuration\n",
        "\n",
        "```python\n",
        "MODEL_PATH = \"/content/drive/My Drive/Anemia Detection/effnetv2s_balance_final.keras\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsL4VHf21UAh"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from flask import Flask, render_template, request\n",
        "from werkzeug.utils import secure_filename\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input as effv2_pre\n",
        "\n",
        "# ---------------------------\n",
        "# CONFIG\n",
        "# ---------------------------\n",
        "MODEL_PATH = \"/content/drive/My Drive/Anemia Detection/effnetv2s_balance_final.keras\"\n",
        "UPLOAD_FOLDER = \"static/uploads\"\n",
        "IMG_HW = 380\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "\n",
        "# ---------------------------\n",
        "# Minimal MixStyle (NO-OP for inference)\n",
        "# ---------------------------\n",
        "class MixStyle(layers.Layer):\n",
        "    def __init__(self, p=0.5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.p = p\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        return x\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"p\": self.p}\n",
        "\n",
        "# ---------------------------\n",
        "# LOAD MODEL\n",
        "# ---------------------------\n",
        "print(\"Loading model:\", MODEL_PATH)\n",
        "\n",
        "model = tf.keras.models.load_model(\n",
        "    MODEL_PATH,\n",
        "    custom_objects={\"MixStyle\": MixStyle},\n",
        "    compile=False\n",
        ")\n",
        "\n",
        "print(\"Model Loaded Successfully.\")\n",
        "\n",
        "# ---------------------------\n",
        "# PREPROCESSING\n",
        "# ---------------------------\n",
        "def white_balance_grayworld(img):\n",
        "    img = img.astype(np.float32)\n",
        "    avg = (img[:,:,0].mean() + img[:,:,1].mean() + img[:,:,2].mean())/3.0 + 1e-6\n",
        "    for c in range(3):\n",
        "        img[:,:,c] *= avg/(img[:,:,c].mean()+1e-6)\n",
        "    return np.clip(img,0,255).astype(np.uint8)\n",
        "\n",
        "def apply_clahe_rgb(img):\n",
        "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "    l,a,b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0)\n",
        "    l2 = clahe.apply(l)\n",
        "    return cv2.cvtColor(cv2.merge((l2,a,b)), cv2.COLOR_LAB2BGR)\n",
        "\n",
        "def preprocess_for_model(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        raise ValueError(\"Error reading image.\")\n",
        "\n",
        "    img = white_balance_grayworld(img)\n",
        "    img = apply_clahe_rgb(img)\n",
        "    img = cv2.resize(img, (IMG_HW, IMG_HW))\n",
        "\n",
        "    img_rgb = img[:, :, ::-1]  # BGR \u2192 RGB\n",
        "    img_norm = img_rgb.astype(np.float32) / 255.0\n",
        "\n",
        "    x = effv2_pre(np.expand_dims(img_norm * 255.0, axis=0))\n",
        "    return x, img_rgb\n",
        "\n",
        "# ---------------------------\n",
        "# FLASK\n",
        "# ---------------------------\n",
        "app = Flask(__name__)\n",
        "app.config[\"UPLOAD_FOLDER\"] = UPLOAD_FOLDER\n",
        "\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "def predict():\n",
        "    if \"image\" not in request.files:\n",
        "        return \"No file uploaded\", 400\n",
        "\n",
        "    file = request.files[\"image\"]\n",
        "    if file.filename == \"\":\n",
        "        return \"Empty filename\", 400\n",
        "\n",
        "    filename = secure_filename(file.filename)\n",
        "    save_path = os.path.join(UPLOAD_FOLDER, filename)\n",
        "    file.save(save_path)\n",
        "\n",
        "    # preprocess\n",
        "    x, _ = preprocess_for_model(save_path)\n",
        "\n",
        "    # prediction\n",
        "    pred = float(model.predict(x)[0][0])\n",
        "    label = \"Anemic\" if pred >= 0.5 else \"Non-Anemic\"\n",
        "    confidence = round(pred if pred >= 0.5 else 1 - pred, 4)\n",
        "\n",
        "\n",
        "    return render_template(\n",
        "        \"result.html\",\n",
        "        image_path=\"/\" + save_path,\n",
        "        result=label,\n",
        "        confidence=confidence\n",
        "    )\n",
        "\n",
        "# ---------------------------\n",
        "# RUN\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host=\"0.0.0.0\", port=5000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y87Nw4NS7GcQ"
      },
      "source": [
        "\n",
        "### \ud83d\udfe2 Cell : Frontend Upload Interface (index.html)\n",
        "\n",
        "This page provides the **user interface** for:\n",
        "\n",
        "- Uploading a medical image\n",
        "- Submitting it for AI analysis\n",
        "\n",
        "---\n",
        "\n",
        "#### \u2705 UI Features\n",
        "\n",
        "- Glass-morphism medical card layout\n",
        "- Image upload box\n",
        "- \u201cAnalyze Image\u201d button\n",
        "- Responsive mobile-friendly design\n",
        "\n",
        "The page uses:\n",
        "- HTML for structure  \n",
        "- CSS for styling  \n",
        "\n",
        "\u2705 This is the **entry point of the AI diagnostic system**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSLLVifYyWLF"
      },
      "outputs": [],
      "source": [
        "%%writefile templates/index.html\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>MedVision \u2022 Anemia Detection</title>\n",
        "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\">\n",
        "</head>\n",
        "\n",
        "<body class=\"bg-med\">\n",
        "\n",
        "<div class=\"center-container\">\n",
        "    <div class=\"glass-card\">\n",
        "\n",
        "        <h1 class=\"title\">MedVision Diagnostics</h1>\n",
        "        <p class=\"subtitle\">AI-Powered Non-Invasive Anemia Screening</p>\n",
        "\n",
        "        <form action=\"/predict\" method=\"POST\" enctype=\"multipart/form-data\">\n",
        "\n",
        "            <label class=\"upload-box\">\n",
        "                <span class=\"upload-text\">Tap or Click to Upload an Image</span>\n",
        "                <input type=\"file\" name=\"image\" accept=\"image/*\" required>\n",
        "            </label>\n",
        "\n",
        "            <button type=\"submit\" class=\"primary-btn\">Analyze Image</button>\n",
        "        </form>\n",
        "\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "</body>\n",
        "</html>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6JhNiqc7GcR"
      },
      "source": [
        "### \ud83d\udfe2 Cell : Diagnosis Result Page (result.html)\n",
        "\n",
        "This page displays:\n",
        "\n",
        "- Uploaded medical image\n",
        "- Final diagnosis:\n",
        "  - \u2705 Non-Anemic\n",
        "  - \ud83d\udea8 Anemic\n",
        "- Confidence percentage\n",
        "- Visual confidence progress bar\n",
        "- Button to return to the home page\n",
        "\n",
        "\u2705 This provides a clean, understandable **medical-style report view** for users.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St0VGHn1yfv1"
      },
      "outputs": [],
      "source": [
        "%%writefile templates/result.html\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>MedVision \u2022 Diagnosis Result</title>\n",
        "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\">\n",
        "</head>\n",
        "\n",
        "<body class=\"bg-med\">\n",
        "\n",
        "<div class=\"center-container\">\n",
        "    <div class=\"glass-card\">\n",
        "\n",
        "        <h1 class=\"title\">Diagnosis Result</h1>\n",
        "\n",
        "        <img src=\"{{ image_path }}\" class=\"result-image\">\n",
        "\n",
        "        <div class=\"result-section\">\n",
        "            <p class=\"result-label\">Diagnosis</p>\n",
        "            <p class=\"result-value\">{{ result }}</p>\n",
        "\n",
        "            <p class=\"confidence-label\">Confidence</p>\n",
        "\n",
        "            <div class=\"confidence-bar\">\n",
        "                <div class=\"confidence-fill\" style=\"width: {{ confidence }}%;\"></div>\n",
        "            </div>\n",
        "\n",
        "            <p class=\"confidence-value\">{{ confidence }}%</p>\n",
        "        </div>\n",
        "\n",
        "        <a href=\"/\" class=\"secondary-btn back-home\">\u2190 Back to Home</a>\n",
        "\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "</body>\n",
        "</html>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpnpComm7GcR"
      },
      "source": [
        "### \ud83d\udfe2 Cell : Web Application Styling (CSS)\n",
        "\n",
        "This file defines the full **visual theme** of the MedVision web app:\n",
        "\n",
        "- Gradient medical background\n",
        "- Glass-effect UI cards\n",
        "- Upload box hover effects\n",
        "- Diagnosis result highlights\n",
        "- Confidence progress bar\n",
        "- Mobile-responsive layout\n",
        "\n",
        "\u2705 The design focuses on:\n",
        "- Clean medical aesthetics\n",
        "- Accessibility\n",
        "- Professional look\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UGTsgSqywgR"
      },
      "outputs": [],
      "source": [
        "%%writefile static/style.css\n",
        "\n",
        "\n",
        "/* ---------- GLOBAL ---------- */\n",
        "body {\n",
        "    margin: 0;\n",
        "    padding: 0;\n",
        "    background: linear-gradient(145deg, #e6efff, #f8fbff);\n",
        "    font-family: 'Segoe UI', sans-serif;\n",
        "}\n",
        "\n",
        "/* ---------- CENTER EVERYTHING ---------- */\n",
        ".center-container {\n",
        "    display: flex;\n",
        "    justify-content: center;\n",
        "    align-items: center;\n",
        "    min-height: 100vh;\n",
        "    padding: 20px;\n",
        "}\n",
        "\n",
        "/* ---------- CARD ---------- */\n",
        ".glass-card {\n",
        "    width: 100%;\n",
        "    max-width: 420px;\n",
        "    padding: 30px 28px;\n",
        "    background: rgba(255, 255, 255, 0.75);\n",
        "    backdrop-filter: blur(14px);\n",
        "    border-radius: 20px;\n",
        "    box-shadow: 0px 8px 30px rgba(0, 0, 0, 0.12);\n",
        "    text-align: center;\n",
        "}\n",
        "\n",
        "/* ---------- TEXT ---------- */\n",
        ".title {\n",
        "    font-size: 27px;\n",
        "    font-weight: 800;\n",
        "    color: #0a3d7e;\n",
        "    margin-bottom: 6px;\n",
        "}\n",
        "\n",
        ".subtitle {\n",
        "    color: #4d77aa;\n",
        "    font-size: 15px;\n",
        "    margin-bottom: 22px;\n",
        "}\n",
        "\n",
        "/* ---------- UPLOAD BOX ---------- */\n",
        ".upload-box {\n",
        "    width: 90%;\n",
        "    max-width: 500px;         /* limits width */\n",
        "    margin: 0 auto 25px auto; /* centers element */\n",
        "    padding: 30px 20px;\n",
        "    border: 2px dashed #7aa6ff;\n",
        "    background: #f2f6ff;\n",
        "    border-radius: 18px;\n",
        "    cursor: pointer;\n",
        "    transition: 0.25s;\n",
        "    display: flex;\n",
        "    justify-content: center;\n",
        "    align-items: center;\n",
        "    text-align: center;\n",
        "}\n",
        "\n",
        ".upload-box:hover {\n",
        "    border-color: #3f7bfd;\n",
        "    background: #e8efff;\n",
        "}\n",
        "\n",
        ".upload-text {\n",
        "    font-weight: 600;\n",
        "    color: #315a9d;\n",
        "    font-size: 18px;\n",
        "}\n",
        "\n",
        ".upload-box input {\n",
        "    display: none;\n",
        "}\n",
        "\n",
        "/* ---------- BUTTON ---------- */\n",
        ".primary-btn {\n",
        "    width: 100%;\n",
        "    padding: 14px;\n",
        "    border: none;\n",
        "    border-radius: 12px;\n",
        "    font-size: 17px;\n",
        "    font-weight: 600;\n",
        "    background: #1e5af7;\n",
        "    color: white;\n",
        "    cursor: pointer;\n",
        "    transition: 0.25s;\n",
        "    margin-top: 8px;\n",
        "}\n",
        "\n",
        ".primary-btn:hover {\n",
        "    background: #1444c5;\n",
        "}\n",
        "\n",
        "/* ---------- RESULT PAGE ---------- */\n",
        ".result-image {\n",
        "    width: 55%;\n",
        "    margin-top: 20px;\n",
        "    border-radius: 14px;\n",
        "    box-shadow: 0px 6px 18px rgba(0, 0, 0, 0.15);\n",
        "}\n",
        "\n",
        ".result-label {\n",
        "    text-align: left;\n",
        "    margin-top: 20px;\n",
        "    font-size: 17px;\n",
        "    font-weight: 600;\n",
        "    color: #0a3d7e;\n",
        "}\n",
        "\n",
        ".result-value {\n",
        "    font-size: 22px;\n",
        "    color: #1e5af7;\n",
        "    font-weight: 800;\n",
        "    margin-bottom: 14px;\n",
        "}\n",
        "\n",
        "/* ---------- CONFIDENCE BAR ---------- */\n",
        ".confidence-bar {\n",
        "    width: 100%;\n",
        "    height: 12px;\n",
        "    background: #d6e3ff;\n",
        "    border-radius: 20px;\n",
        "    margin-top: 5px;\n",
        "    overflow: hidden;\n",
        "}\n",
        "\n",
        ".confidence-fill {\n",
        "    height: 100%;\n",
        "    background: linear-gradient(90deg, #1e5af7, #4b87ff);\n",
        "    border-radius: 20px;\n",
        "    transition: width 0.3s;\n",
        "}\n",
        "\n",
        ".confidence-value {\n",
        "    text-align: right;\n",
        "    margin-top: 5px;\n",
        "    font-size: 18px;\n",
        "    font-weight: bold;\n",
        "    color: #0c4dba;\n",
        "}\n",
        "\n",
        "/* ---------- BACK BUTTON ---------- */\n",
        ".secondary-btn {\n",
        "    display: inline-block;\n",
        "    margin-top: 20px;\n",
        "    padding: 12px 18px;\n",
        "    background: #e4eaff;\n",
        "    border-radius: 12px;\n",
        "    color: #0a3d7e;\n",
        "    text-decoration: none;\n",
        "    font-weight: 600;\n",
        "}\n",
        "\n",
        ".secondary-btn:hover {\n",
        "    background: #d4ddff;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kn-qMIT7GcS"
      },
      "source": [
        "###  Cell : Stop Any Previously Running Flask & ngrok Processes\n",
        "\n",
        "This cell safely **terminates any existing Flask servers and ngrok tunnels** before starting a new deployment instance.\n",
        "\n",
        "---\n",
        "\n",
        "####  Why This Step Is Important\n",
        "\n",
        "If old processes are still running:\n",
        "-  Flask may fail to start due to **port already in use**\n",
        "-  ngrok may create **multiple conflicting tunnels**\n",
        "-  You may see incorrect or cached outputs\n",
        "\n",
        "Stopping them ensures:\n",
        " Clean server restart  \n",
        " Reliable ngrok tunnel creation  \n",
        " No port conflicts  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZklGyEOUvKAu"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 6\ufe0f\u20e3 Kill any previous processes\n",
        "# ===============================\n",
        "!pkill -f flask || echo \"No flask running\"\n",
        "!pkill -f ngrok || echo \"No ngrok running\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DYiYUhC7GcS"
      },
      "source": [
        "### \ud83d\udfe2 Cell : Check If Port 5000 Is Already in Use\n",
        "\n",
        "This cell checks whether **any process is currently using port 5000**, which is the default port for your Flask web application.\n",
        "\n",
        "---\n",
        "\n",
        "#### \u2705 Why This Step Is Important\n",
        "\n",
        "If port `5000` is already in use:\n",
        "-  Flask will fail to start  \n",
        "-  You may see an error like: *\u201cAddress already in use\u201d*  \n",
        "-  Your app will not be accessible  \n",
        "\n",
        "This command helps you **identify which process is blocking the port** before launching Flask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vzib0VwOuP6I"
      },
      "outputs": [],
      "source": [
        "!lsof -i :5000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwGIodUE7GcT"
      },
      "source": [
        "### \ud83d\udfe2 Cell : Forcefully Terminate a Specific Process by PID\n",
        "\n",
        "This cell forcefully **kills a specific running process** using its **Process ID (PID)**.  \n",
        "It is typically used when a process is **locking the Flask port (5000)** and cannot be stopped normally.\n",
        "\n",
        "---\n",
        "\n",
        "#### \u2705 Why This Step Is Important\n",
        "\n",
        "If a Flask or background process:\n",
        "-  Does not stop using `pkill`\n",
        "-  Continues to occupy port `5000`\n",
        "-  Prevents your web app from launching  \n",
        "\n",
        "Then force-killing the process by PID ensures:\n",
        " Immediate termination  \n",
        " Port is released instantly  \n",
        " No further port conflicts  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdrvlSXGvTTB"
      },
      "outputs": [],
      "source": [
        "!kill -9 17471"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObhpAXqj7GcT"
      },
      "source": [
        "###  Cell : Run Flask Server in the Background (Non-Blocking Mode)\n",
        "\n",
        "This cell starts the **Flask web application in the background**, allowing the notebook to remain interactive while the server keeps running continuously.\n",
        "\n",
        "---\n",
        "\n",
        "####  Why This Step Is Important\n",
        "\n",
        "Running Flask in the background allows you to:\n",
        "\n",
        "-  Start **ngrok** in the next step  \n",
        "-  Avoid blocking the notebook execution  \n",
        "-  Keep the web app running without interruption  \n",
        "-  Monitor logs separately  \n",
        "\n",
        "If Flask runs in the foreground:\n",
        "-  The notebook will freeze  \n",
        "-  You won\u2019t be able to launch ngrok  \n",
        "-  You must manually interrupt execution  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lwB5xyLvZzA"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 7\ufe0f\u20e3 Run Flask in the background\n",
        "# ===============================\n",
        "!nohup python app.py > flask.log 2>&1 &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMEF9gfZ7GcT"
      },
      "source": [
        "### \ud83d\udfe2 Cell 9: Run Flask Server in the Background (Non-Blocking Mode)\n",
        "\n",
        "This cell starts the **Flask web application in the background** so that:\n",
        "-  The server keeps running continuously  \n",
        "-  The notebook remains free for other commands  \n",
        "-  You can start ngrok after this without interruption  \n",
        "\n",
        "---\n",
        "\n",
        "#### \u2705 Why This Step Is Important\n",
        "\n",
        "If Flask runs in the foreground:\n",
        "-  The notebook will be blocked  \n",
        "-  You won\u2019t be able to run ngrok  \n",
        "-  You\u2019ll need to manually stop the server  \n",
        "\n",
        "Running Flask in the background ensures:\n",
        " Smooth deployment workflow  \n",
        " Continuous server execution  \n",
        " Ability to monitor logs separately  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFQuFRM-7GcU"
      },
      "source": [
        "### \ud83d\udd11 How to Create & Use an ngrok Authentication Token (One-Time Setup)\n",
        "\n",
        "This step is **mandatory** to generate a public URL for your Flask app running in Google Colab.\n",
        "\n",
        "---\n",
        "\n",
        "####  Step 1: Create a Free ngrok Account\n",
        "\n",
        "1. Open the official ngrok website:  \n",
        "   https://ngrok.com  \n",
        "2. Click **Sign Up**  \n",
        "3. Sign up using:\n",
        "   - Google account OR\n",
        "   - Email + password  \n",
        "4. Log in to your ngrok dashboard after signup.\n",
        "\n",
        "---\n",
        "   \n",
        "####  Step 2: Get Your ngrok Auth Token\n",
        "\n",
        "1. After logging in, go to:  \n",
        "   **Dashboard \u2192 Your Authtoken**\n",
        "2. You will see a command like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9_46HwpugDy"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 8\ufe0f\u20e3 Start ngrok tunnel\n",
        "# ===============================\n",
        "from pyngrok import ngrok, conf\n",
        "conf.get_default().auth_token = \"\"  # \ud83d\udd11 replace with your token\n",
        "\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"\ud83c\udf0d Public URL:\", public_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBeQpnXe7GcU"
      },
      "source": [
        "#### Used to check logs of the model while running the app . Mostly used while we face any errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjwTQbR6vfia"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 9\ufe0f\u20e3 Check logs (optional)\n",
        "# ===============================\n",
        "!sleep 3 && tail -n 20 flask.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJlSkAbl39gD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 8468339,
          "sourceId": 13352166,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 8821124,
          "sourceId": 13848619,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 8821150,
          "sourceId": 13848656,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}