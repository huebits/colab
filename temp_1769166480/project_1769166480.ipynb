{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# \ud83d\udcd8 **1 \u2014 Project Introduction**\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "This project implements **Brain Tumor Segmentation** using a **U-Net deep learning model** trained on MRI images.\n",
        "\n",
        "### \ud83d\udd0d What this system can do:\n",
        "\n",
        "* Upload MRI Image (.png, .jpg, .jpeg)\n",
        "* Upload MRI H5 file (.h5)\n",
        "* Auto-convert H5 \u2192 PNG\n",
        "* Run Deep Learning segmentation\n",
        "* Generate:\n",
        "\n",
        "  * \u2714 Tumor Overlay Mask\n",
        "  * \u2714 Binary Mask (green tumor region)\n",
        "  * \u2714 Segmented Visualization\n",
        "\n",
        "### \ud83c\udf10 Web Deployment\n",
        "\n",
        "The project also includes a **Flask Web App** with:\n",
        "\n",
        "* Upload page\n",
        "* Real-time segmentation results\n",
        "* ngrok for Public Sharing\n",
        "\n",
        "---\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# \ud83d\udcd8 **2 \u2014 Install All Dependencies**\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "This step installs all required libraries for:\n",
        "\n",
        "* Deep Learning (PyTorch)\n",
        "* Image Processing\n",
        "* Flask Web Backend\n",
        "* ngrok Deployment\n",
        "* Albumentations Transforms\n",
        "* MRI (.h5) file support\n",
        "\n",
        "---\n",
        "\n",
        "## \u2705 **CELL 1 \u2014 Install Dependencies**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FOrwDOjiumFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Kaggle Code"
      ],
      "metadata": {
        "id": "8ff4Cm2EtW9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 \u2014 clean imports (NO ALBUMENTATIONS, NO SKLEARN)\n",
        "\n",
        "import os, random, glob\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# manual split (no sklearn needed)\n",
        "def simple_split(data_list, val_ratio=0.15, seed=42):\n",
        "    random.Random(seed).shuffle(data_list)\n",
        "    n_val = int(len(data_list) * val_ratio)\n",
        "    val = data_list[:n_val]\n",
        "    train = data_list[n_val:]\n",
        "    return train, val\n",
        "\n",
        "# PyTorch transforms instead of albumentations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n"
      ],
      "metadata": {
        "id": "bESgulaOtBVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# \ud83d\udcd8 **3 \u2014 Import Libraries & Setup Environment**\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "This step:\n",
        "\n",
        "* Imports all required Python libraries\n",
        "* Sets random seeds for reproducibility\n",
        "* Confirms GPU availability\n",
        "\n",
        "---\n",
        "\n",
        "## \u2705 **CELL 2 \u2014 Imports & Reproducibility**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HtTn0Ks5uzBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 \u2014 FIXED Dataset class (handles corrupted PNGs)\n",
        "\n",
        "class BrainTumorDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.png\")))\n",
        "        self.mask_paths  = sorted(glob.glob(os.path.join(mask_dir, \"*.png\")))\n",
        "\n",
        "        # pair images and masks by filename\n",
        "        mask_map = {os.path.basename(m): m for m in self.mask_paths}\n",
        "\n",
        "        self.pairs = []\n",
        "        for img in self.image_paths:\n",
        "            name = os.path.basename(img)\n",
        "            if name in mask_map:\n",
        "                self.pairs.append((img, mask_map[name]))\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.pairs[idx]\n",
        "\n",
        "        # ---- read image safely ----\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # corrupted file handling\n",
        "        if img is None or mask is None:\n",
        "            print(\"\u274c Corrupted file skipped:\", img_path)\n",
        "            return self.__getitem__((idx + 1) % len(self.pairs))\n",
        "\n",
        "        # convert to float32\n",
        "        img = img.astype(\"float32\")\n",
        "        mask = mask.astype(\"float32\")\n",
        "\n",
        "        # normalize\n",
        "        img /= 255.0\n",
        "        mask = (mask > 127).astype(\"float32\")\n",
        "\n",
        "        # add channel dim\n",
        "        img = np.expand_dims(img, axis=-1)\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "\n",
        "        # apply transforms\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "            mask = self.transform(mask)\n",
        "        else:\n",
        "            img = torch.from_numpy(img).permute(2,0,1)\n",
        "            mask = torch.from_numpy(mask).permute(2,0,1)\n",
        "\n",
        "        return img, mask\n"
      ],
      "metadata": {
        "id": "oCKiH_P1tBTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# \ud83d\udcd8 **4 \u2014 Dataset Paths / Kaggle Download Setup**\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "This step configures:\n",
        "\n",
        "* Image directory\n",
        "* Mask directory\n",
        "* Optional Kaggle dataset download\n",
        "* Validation that directories exist\n",
        "\n",
        "---\n",
        "\n",
        "## \u2705 **CELL 3 \u2014 Set Dataset Paths**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iQR2BOMivNx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image and mask directories\n",
        "\n",
        "IMG_DIR = \"/kaggle/input/brain-tumor-2021-dataset/kaggle/working/brats_images\"\n",
        "MASK_DIR = \"/kaggle/input/brain-tumor-2021-dataset/kaggle/working/brats_masks\"\n",
        "\n",
        "print(\"IMG_DIR =\", IMG_DIR)\n",
        "print(\"MASK_DIR =\", MASK_DIR)\n"
      ],
      "metadata": {
        "id": "Xb8rgMSQtBOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# \ud83d\udcd8 **5 \u2014 Create MRI Dataset Loader**\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "This step builds a **custom PyTorch Dataset class** that:\n",
        "\n",
        "* Reads PNG/JPG images\n",
        "* Pairs images and masks by filename\n",
        "* Handles corrupted images\n",
        "* Converts grayscale \u2192 1-channel tensors\n",
        "* Normalizes pixel values\n",
        "\n",
        "---\n",
        "\n",
        "## \u2705 **CELL 4 \u2014 Dataset Class**\n",
        "\n"
      ],
      "metadata": {
        "id": "NdHipMhBvXak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "\n",
        "# Directories\n",
        "SAVE_DIR = \"/kaggle/working/\"\n",
        "TRAIN_IMG_DIR = os.path.join(SAVE_DIR, \"train_images\")\n",
        "TRAIN_MASK_DIR = os.path.join(SAVE_DIR, \"train_masks\")\n",
        "VAL_IMG_DIR = os.path.join(SAVE_DIR, \"val_images\")\n",
        "VAL_MASK_DIR = os.path.join(SAVE_DIR, \"val_masks\")\n",
        "\n",
        "# Create split directories\n",
        "os.makedirs(TRAIN_IMG_DIR, exist_ok=True)\n",
        "os.makedirs(TRAIN_MASK_DIR, exist_ok=True)\n",
        "os.makedirs(VAL_IMG_DIR, exist_ok=True)\n",
        "os.makedirs(VAL_MASK_DIR, exist_ok=True)\n",
        "\n",
        "# Load dataset to get image-mask pairs\n",
        "dataset = BrainTumorDataset(IMG_DIR, MASK_DIR, transform_train)\n",
        "\n",
        "# Split\n",
        "train_list, val_list = simple_split(dataset.pairs, val_ratio=0.15)\n",
        "\n",
        "print(\"Total images:\", len(dataset.pairs))\n",
        "print(\"Train:\", len(train_list), \" Val:\", len(val_list))\n",
        "\n",
        "# Copy training images/masks\n",
        "print(\"\\nSaving Train Images...\")\n",
        "for img_path, mask_path in train_list:\n",
        "    shutil.copy(img_path, TRAIN_IMG_DIR)\n",
        "    shutil.copy(mask_path, TRAIN_MASK_DIR)\n",
        "\n",
        "# Copy validation images/masks\n",
        "print(\"\\nSaving Val Images...\")\n",
        "for img_path, mask_path in val_list:\n",
        "    shutil.copy(img_path, VAL_IMG_DIR)\n",
        "    shutil.copy(mask_path, VAL_MASK_DIR)\n",
        "\n",
        "print(\"\\n\u2705 Split completed and saved in:\")\n",
        "print(TRAIN_IMG_DIR)\n",
        "print(TRAIN_MASK_DIR)\n",
        "print(VAL_IMG_DIR)\n",
        "print(VAL_MASK_DIR)\n"
      ],
      "metadata": {
        "id": "AYmnnb8MtBL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# \ud83d\udcd8 **6 \u2014 Data Augmentation & Loader Setup**\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "This step:\n",
        "\n",
        "* Creates training augmentations\n",
        "* Performs 85/15 train-validation split\n",
        "* Prepares efficient PyTorch DataLoaders\n",
        "\n",
        "---\n",
        "\n",
        "## \u2705 **CELL 5 \u2014 Transforms, Split & DataLoaders**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UZOLAHavvk22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Directories\n",
        "base = \"/kaggle/working/\"\n",
        "\n",
        "folders = [\n",
        "    \"train_images\",\n",
        "    \"train_masks\",\n",
        "    \"val_images\",\n",
        "    \"val_masks\"\n",
        "]\n",
        "\n",
        "# Create one ZIP per folder\n",
        "for folder in folders:\n",
        "    folder_path = base + folder\n",
        "    zip_path = base + folder  # output same name\n",
        "    print(f\"Zipping \u2192 {zip_path}.zip ...\")\n",
        "    shutil.make_archive(zip_path, 'zip', folder_path)\n",
        "\n",
        "print(\"\\n\u2705 All zip files created successfully!\")\n"
      ],
      "metadata": {
        "id": "PDWizTvztBJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# \ud83d\udcd8 **7 \u2014 Define U-Net Model Architecture**\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "This section implements:\n",
        "\n",
        "* Encoder\n",
        "* Bottleneck\n",
        "* Decoder\n",
        "* Skip Connections\n",
        "* Final Sigmoid Output\n",
        "\n",
        "This U-Net is ideal for biomedical segmentation tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## \u2705 **CELL 6 \u2014 U-Net Model Definition**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dmsnmZAZvqQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 \u2014 Training Loop\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def validate(model, loader, device):\n",
        "    model.eval()\n",
        "    dices = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            preds = model(imgs)\n",
        "            preds_bin = (preds > 0.5).float()\n",
        "\n",
        "            for p, t in zip(preds_bin, masks):\n",
        "                dices.append(dice_coef(p, t).item())\n",
        "\n",
        "    return np.mean(dices) if len(dices) > 0 else 0.0\n",
        "\n",
        "\n",
        "EPOCHS = 10        # you can increase to 30 later\n",
        "best_dice = 0.0\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    loop = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
        "\n",
        "    for imgs, masks in loop:\n",
        "        imgs = imgs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(imgs)\n",
        "        loss = criterion(preds, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    # ---- Validation ----\n",
        "    val_dice = validate(model, val_loader, device)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch}: TrainLoss={np.mean(train_losses):.4f}  ValDice={val_dice:.4f}\")\n",
        "\n",
        "    # ---- Save best model ----\n",
        "    if val_dice > best_dice:\n",
        "        best_dice = val_dice\n",
        "        torch.save(model.state_dict(), \"/kaggle/working/unet_brats21_best.pth\")\n",
        "        print(f\"\u2705 Saved new best model (Dice={best_dice:.4f})\")\n",
        "\n",
        "print(\"\\n\ud83c\udfaf Training Finished!\")\n",
        "print(\"Best Validation Dice:\", best_dice)\n"
      ],
      "metadata": {
        "id": "_zWzZKjltBG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UOx7RtHktAqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dFMtYrNEtAn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5AqPdHbItAkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Ym0lxx_tAiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Colab Code"
      ],
      "metadata": {
        "id": "xQmiX-37tbJJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HnwcyAmwIJV"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# 1\ufe0f\u20e3 Install Required Libraries\n",
        "# =========================================================\n",
        "!pip install flask pyngrok torch torchvision pillow opencv-python numpy albumentations h5py tqdm scikit-learn matplotlib\n",
        "!mkdir -p templates static uploads results models\n",
        "\n",
        "print(\"\u2705 Installation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# \ud83d\udcd8 **11 \u2014 Build Flask Backend (app.py)**\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "This step:\n",
        "\n",
        "* Loads trained U-Net\n",
        "* Accepts image/H5 uploads\n",
        "* Converts H5 \u2192 PNG\n",
        "* Runs segmentation\n",
        "* Saves results in `results/` directory\n",
        "* Exposes web routes for frontend\n",
        "\n",
        "---\n",
        "\n",
        "## \u2705 **CELL 10 \u2014 Create app.py**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2FDglftBv8NX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from flask import Flask, render_template, request, send_from_directory\n",
        "import os, torch, cv2, numpy as np, h5py\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch import nn\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config['UPLOAD_FOLDER'] = 'uploads'\n",
        "app.config['RESULT_FOLDER'] = 'results'\n",
        "os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n",
        "os.makedirs(app.config['RESULT_FOLDER'], exist_ok=True)\n",
        "\n",
        "# ===============================\n",
        "# U-Net Definition\n",
        "# ===============================\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self,x): return self.seq(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self,in_ch=1,out_ch=1,base=32):\n",
        "        super().__init__()\n",
        "        self.d1=DoubleConv(in_ch,base)\n",
        "        self.p1=nn.MaxPool2d(2)\n",
        "        self.d2=DoubleConv(base,base*2)\n",
        "        self.p2=nn.MaxPool2d(2)\n",
        "        self.d3=DoubleConv(base*2,base*4)\n",
        "        self.p3=nn.MaxPool2d(2)\n",
        "        self.d4=DoubleConv(base*4,base*8)\n",
        "        self.u1=nn.ConvTranspose2d(base*8,base*4,2,stride=2)\n",
        "        self.c1=DoubleConv(base*8,base*4)\n",
        "        self.u2=nn.ConvTranspose2d(base*4,base*2,2,stride=2)\n",
        "        self.c2=DoubleConv(base*4,base*2)\n",
        "        self.u3=nn.ConvTranspose2d(base*2,base,2,stride=2)\n",
        "        self.c3=DoubleConv(base*2,base)\n",
        "        self.outc=nn.Conv2d(base,out_ch,1)\n",
        "    def forward(self,x):\n",
        "        c1=self.d1(x); c2=self.d2(self.p1(c1)); c3=self.d3(self.p2(c2)); c4=self.d4(self.p3(c3))\n",
        "        x=self.c1(torch.cat([self.u1(c4),c3],1))\n",
        "        x=self.c2(torch.cat([self.u2(x),c2],1))\n",
        "        x=self.c3(torch.cat([self.u3(x),c1],1))\n",
        "        return torch.sigmoid(self.outc(x))\n",
        "\n",
        "# ===============================\n",
        "# Load model\n",
        "# ===============================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = UNet().to(device)\n",
        "model_path = 'models/unet_brats_best.pth'\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    print(\"\u2705 Loaded trained UNet model\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f No trained model found. Please train and save to 'models/'\")\n",
        "\n",
        "# ===============================\n",
        "# Preprocessing\n",
        "# ===============================\n",
        "transform = A.Compose([\n",
        "    A.Resize(256,256),\n",
        "    A.Normalize(mean=(0.5,), std=(0.5,)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "def process_input(filepath):\n",
        "    \"\"\"Handle both .png and .h5 files\"\"\"\n",
        "    ext = os.path.splitext(filepath)[-1].lower()\n",
        "    if ext == '.h5':\n",
        "        with h5py.File(filepath, 'r') as f:\n",
        "            img = f['image'][()][...,0]  # take first MRI channel\n",
        "            mask = np.max(f['mask'][()], axis=-1)\n",
        "        img_n = ((img - img.min())/(img.max()-img.min()+1e-6)*255).astype(np.uint8)\n",
        "        converted = filepath.replace('.h5','.png')\n",
        "        cv2.imwrite(converted, img_n)\n",
        "        return converted\n",
        "    else:\n",
        "        return filepath\n",
        "\n",
        "def segment_brain_tumor(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.resize(img, (256,256))\n",
        "    img_norm = img.astype(np.float32)/255.0\n",
        "    img_norm = np.expand_dims(img_norm, axis=2)\n",
        "    t = transform(image=img_norm)\n",
        "    x = t['image'].unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        pred = model(x).cpu().numpy()[0,0]\n",
        "    mask = (pred>0.5).astype(np.uint8)*255\n",
        "    color_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "    overlay = color_img.copy()\n",
        "    overlay[mask>0] = (0,255,0)\n",
        "    blended = cv2.addWeighted(color_img, 0.7, overlay, 0.3, 0)\n",
        "    return blended, mask\n",
        "\n",
        "# ===============================\n",
        "# Flask routes\n",
        "# ===============================\n",
        "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
        "def home():\n",
        "    original, segmented, mask_file = None, None, None\n",
        "    if request.method == \"POST\":\n",
        "        f = request.files['image']\n",
        "        if not f:\n",
        "            return render_template(\"index.html\", error=\"Please upload a file.\")\n",
        "        filename = f.filename\n",
        "        upload_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
        "        f.save(upload_path)\n",
        "        print(f\"\ud83d\udcc1 Uploaded: {filename}\")\n",
        "\n",
        "        # handle .h5 conversion if needed\n",
        "        processed = process_input(upload_path)\n",
        "\n",
        "        seg_out = os.path.join(app.config['RESULT_FOLDER'], f\"seg_{os.path.basename(processed)}\")\n",
        "        mask_out = os.path.join(app.config['RESULT_FOLDER'], f\"mask_{os.path.basename(processed)}\")\n",
        "\n",
        "        result, mask = segment_brain_tumor(processed)\n",
        "        cv2.imwrite(seg_out, result)\n",
        "        cv2.imwrite(mask_out, mask)\n",
        "\n",
        "        original = os.path.basename(processed)\n",
        "        segmented = os.path.basename(seg_out)\n",
        "        mask_file = os.path.basename(mask_out)\n",
        "\n",
        "    return render_template(\"index.html\", original=original, segmented=segmented, mask_file=mask_file)\n",
        "\n",
        "@app.route(\"/uploads/<filename>\")\n",
        "def uploaded(filename):\n",
        "    return send_from_directory(app.config['UPLOAD_FOLDER'], filename)\n",
        "\n",
        "@app.route(\"/results/<filename>\")\n",
        "def results(filename):\n",
        "    return send_from_directory(app.config['RESULT_FOLDER'], filename)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host=\"0.0.0.0\", port=8000, debug=False)\n"
      ],
      "metadata": {
        "id": "Ia2SOMNhwUma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# \ud83d\udcd8 **12 \u2014 Create Frontend UI (HTML Template)**\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "This step builds:\n",
        "\n",
        "* Clean upload interface\n",
        "* Result display layout\n",
        "* Error handling messages\n",
        "\n",
        "This is placed in **templates/index.html**\n",
        "\n",
        "---\n",
        "\n",
        "## \u2705 **CELL 11 \u2014 Create index.html**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ECYxxO7PwFWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile templates/index.html\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>\ud83e\udde0 Brain Tumour Detection (H5 + PNG)</title>\n",
        "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\">\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container\">\n",
        "    <h1>\ud83e\udde0 Brain Tumour Identification using Deep Learning</h1>\n",
        "    <p class=\"subtitle\">Upload MRI (.h5 or .png) \u2192 Detect & Highlight tumour regions</p>\n",
        "\n",
        "    <form method=\"post\" enctype=\"multipart/form-data\" class=\"upload-form\">\n",
        "        <input type=\"file\" name=\"image\" accept=\".h5,image/*\" required>\n",
        "        <button type=\"submit\">\ud83d\udd0d Identify Tumour</button>\n",
        "    </form>\n",
        "\n",
        "    {% if original %}\n",
        "    <div class=\"results\">\n",
        "        <div class=\"image-box\">\n",
        "            <h3>Original MRI</h3>\n",
        "            <img src=\"{{ url_for('uploads', filename=original) }}\" width=\"300\">\n",
        "        </div>\n",
        "        <div class=\"image-box\">\n",
        "            <h3>Segmented Result</h3>\n",
        "            <img src=\"{{ url_for('results', filename=segmented) }}\" width=\"300\">\n",
        "        </div>\n",
        "        <div class=\"image-box\">\n",
        "            <h3>Tumour Mask</h3>\n",
        "            <img src=\"{{ url_for('results', filename=mask_file) }}\" width=\"300\">\n",
        "        </div>\n",
        "    </div>\n",
        "    {% endif %}\n",
        "</div>\n",
        "</body>\n",
        "</html>\n"
      ],
      "metadata": {
        "id": "oGUIGYgzwclk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# \ud83d\udcd8 **13 \u2014 Add Styling (CSS)**\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "This step:\n",
        "\n",
        "* Designs color theme\n",
        "* Styles upload container & buttons\n",
        "* Formats results section\n",
        "\n",
        "This is saved in **static/style.css**\n",
        "\n",
        "---\n",
        "\n",
        "## \u2705 **CELL 12 \u2014 Create style.css**\n",
        "\n"
      ],
      "metadata": {
        "id": "HdntKzyWwLI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile static/style.css\n",
        "body {\n",
        "    font-family: 'Segoe UI', sans-serif;\n",
        "    background: linear-gradient(135deg, #4b6cb7, #182848);\n",
        "    color: white;\n",
        "    text-align: center;\n",
        "    padding: 40px;\n",
        "}\n",
        "h1 { font-size: 2.5em; margin-bottom: 10px; }\n",
        ".subtitle { opacity: 0.8; margin-bottom: 20px; }\n",
        ".upload-form {\n",
        "    margin: 20px auto;\n",
        "    background: rgba(255,255,255,0.1);\n",
        "    padding: 25px;\n",
        "    border-radius: 15px;\n",
        "    width: 400px;\n",
        "}\n",
        "button {\n",
        "    padding: 12px 20px;\n",
        "    background: #22c55e;\n",
        "    border: none;\n",
        "    border-radius: 10px;\n",
        "    font-size: 1em;\n",
        "    color: white;\n",
        "    cursor: pointer;\n",
        "}\n",
        ".results {\n",
        "    margin-top: 30px;\n",
        "    display: flex;\n",
        "    justify-content: center;\n",
        "    gap: 20px;\n",
        "}\n",
        ".image-box {\n",
        "    background: rgba(255,255,255,0.1);\n",
        "    padding: 15px;\n",
        "    border-radius: 10px;\n",
        "}\n",
        "img { border-radius: 10px; }\n",
        ".error {\n",
        "    background: rgba(239, 68, 68, 0.3);\n",
        "    border: 2px solid #ef4444;\n",
        "    padding: 10px;\n",
        "    margin: 20px auto;\n",
        "    width: 400px;\n",
        "    border-radius: 10px;\n",
        "}\n"
      ],
      "metadata": {
        "id": "fI5y7OMTwjh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# \ud83d\udcd8 **5 \u2014 Run Flask App + ngrok Deployment**\n",
        "\n",
        "This step:\n",
        "\n",
        "* Stops existing processes\n",
        "* Starts Flask backend\n",
        "* Opens ngrok HTTPS public URL\n",
        "\n",
        "---\n",
        "\n",
        "# \ud83d\udcd8 **Authenticate ngrok (IMPORTANT)**\n",
        "\n",
        "ngrok provides a **public HTTPS URL**.\n",
        "\n",
        "Your token is removed for safety.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd11 **How to Use ngrok**\n",
        "\n",
        "#### **Step 1 \u2014 Get your Token**\n",
        "\n",
        "\ud83d\udc49 [https://dashboard.ngrok.com/get-started/your-authtoken](https://dashboard.ngrok.com/get-started/your-authtoken)\n",
        "\n",
        "#### **Step 2 \u2014 Add Token in Notebook**\n",
        "\n",
        "```python\n",
        "#from pyngrok import ngrok, conf\n",
        "#conf.get_default().auth_token = \"YOUR_TOKEN_HERE\"\n",
        "```\n",
        "\n",
        "#### **Step 3 \u2014 Start Tunnel**\n",
        "\n",
        "```python\n",
        "#public_url = ngrok.connect(8000)\n",
        "#print(\"\ud83c\udf0d Public URL:\", public_url)\n",
        "```\n",
        "\n",
        "A public HTTPS link will appear \u2014 you can open your app from anywhere.\n",
        "\n",
        "---\n",
        "\n",
        "## \u2705 **CELL 5: Run Server & ngrok**\n",
        "\n"
      ],
      "metadata": {
        "id": "azGcVxw-sMeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f flask || echo \"No Flask running\"\n",
        "!pkill -f ngrok || echo \"No ngrok running\"\n",
        "!nohup python app.py > flask.log 2>&1 &\n",
        "\n",
        "from pyngrok import ngrok, conf\n",
        "conf.get_default().auth_token = \"PASTE_YOUR_TOKEN_HERE\"\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"\ud83c\udf0d Public URL:\", public_url)\n",
        "!sleep 3 && tail -n 10 flask.log\n"
      ],
      "metadata": {
        "id": "SjU4dxKJwnDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# \ud83d\udcd8 **16 \u2014 Notebook Completed Successfully**\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "\ud83c\udf89 Your **Brain Tumor Segmentation System** is now fully implemented!\n",
        "\n",
        "You can now:\n",
        "\n",
        "* Upload MRI image or H5 file\n",
        "* Perform segmentation\n",
        "* View tumor overlay\n",
        "* Deploy publicly using ngrok\n",
        "* Demonstrate in viva/exam/company interview\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HXG4BEpEBIuP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}