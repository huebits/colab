{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2tpdHdA2H/mP3z3Fh0X88"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MORpc1ijDbnG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run this in Kaggle"
      ],
      "metadata": {
        "id": "DdfsfillDskn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP-qfjm2DkLD"
      },
      "source": [
        "\ud83d\udcd8 1\ufe0f\u20e3 Project Introduction\n",
        "\n",
        "# \ud83d\udd0d Altered Fingerprint Detection & Restoration for Forensic Analysis\n",
        "\n",
        "This notebook builds two complete deep learning pipelines:\n",
        "\n",
        "1) **Fingerprint Alteration Detection (Real vs Altered)**\n",
        "   - Dataset split by subject (prevents leakage)\n",
        "   - ResNet50 classifier with weighted loss\n",
        "   - Validation/test evaluation\n",
        "   - Random prediction visualizations\n",
        "\n",
        "2) **Fingerprint Restoration using Pix2Pix GAN**\n",
        "   - Pairs altered \u2192 real images\n",
        "   - U-Net generator + PatchGAN discriminator\n",
        "   - Adversarial + L1 training loop\n",
        "   - PSNR, SSIM, MSE metrics\n",
        "   - Artifact visualization and monitoring\n",
        "\n",
        "Additionally, we create:\n",
        "- A small downloadable subset for the Colab-based web application  \n",
        "- Saved model weights for both classifier and GAN  \n",
        "\n",
        "This notebook runs end-to-end on Kaggle GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpSUXlMtDkLN"
      },
      "source": [
        "\ud83d\udcd8 Dataset Attachment & Setup Instructions\n",
        "\n",
        "## \ud83d\udcc1 Dataset Setup: SOCOFing Fingerprint Dataset\n",
        "\n",
        "Before running any code in this notebook:\n",
        "\n",
        "1. Go to the dataset page:  \n",
        "   https://www.kaggle.com/datasets/ruizgara/socofing\n",
        "\n",
        "2. Click **\u201cDownload\u201d** or **\u201cAdd to your Kaggle Datasets\u201d** \u2014 whichever applies.  \n",
        "   If you download manually, upload the resulting ZIP file to Kaggle (via \u201cUpload Data\u201d) and extract  \n",
        "   so that the following folder structure is available:\n",
        "\n",
        "   /kaggle/input/socofing/SOCOFing/  \n",
        "       \u251c\u2500\u2500 Real/  \n",
        "       \u2514\u2500\u2500 Altered/  \n",
        "            \u251c\u2500\u2500 Altered-Easy/  \n",
        "            \u251c\u2500\u2500 Altered-Medium/  \n",
        "            \u2514\u2500\u2500 Altered-Hard/\n",
        "\n",
        "3. If using the \u201cAdd to your Kaggle Datasets\u201d method, ensure that the dataset alias:  \n",
        "   **ruizgara/socofing** is attached to your notebook.\n",
        "\n",
        "4. Confirm that inside the notebook the following paths exist:  \n",
        "   ```text\n",
        "   /kaggle/input/socofing/SOCOFing/Real  \n",
        "   /kaggle/input/socofing/SOCOFing/Altered/Altered-Easy  \n",
        "   /kaggle/input/socofing/SOCOFing/Altered/Altered-Medium  \n",
        "   /kaggle/input/socofing/SOCOFing/Altered/Altered-Hard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T04:10:45.886523Z",
          "iopub.status.busy": "2025-11-11T04:10:45.886303Z",
          "iopub.status.idle": "2025-11-11T04:12:47.011765Z",
          "shell.execute_reply": "2025-11-11T04:12:47.010877Z",
          "shell.execute_reply.started": "2025-11-11T04:10:45.886503Z"
        },
        "trusted": true,
        "id": "K_xs6yA-DkLQ"
      },
      "outputs": [],
      "source": [
        "!pip install -U pip --quiet\n",
        "!pip install numpy==1.26.4 scipy==1.14.1 scikit-learn==1.6.0 matplotlib==3.9.2 \\\n",
        "              opencv-python==4.10.0.84 pillow==10.4.0 pandas==2.2.3 tqdm==4.66.4 \\\n",
        "              albumentations==1.4.13 scikit-image==0.24.0 lpips==0.1.4 \\\n",
        "              torch torchvision torchaudio --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s_5qL_ADkLV"
      },
      "source": [
        "\ud83d\udcd8 2\ufe0f\u20e3 Environment Setup\n",
        "\n",
        "This cell installs all necessary libraries:\n",
        "\n",
        "- NumPy, SciPy, scikit-learn \u2192 scientific computing  \n",
        "- OpenCV, Pillow \u2192 image processing  \n",
        "- Albumentations \u2192 augmentations  \n",
        "- LPIPS \u2192 perceptual loss (optional)  \n",
        "- PyTorch + torchvision \u2192 model training  \n",
        "- tqdm \u2192 progress bars  \n",
        "\n",
        "Once installed, we verify versions and confirm GPU usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T04:13:10.690487Z",
          "iopub.status.busy": "2025-11-11T04:13:10.689661Z",
          "iopub.status.idle": "2025-11-11T04:13:15.058082Z",
          "shell.execute_reply": "2025-11-11T04:13:15.057190Z",
          "shell.execute_reply.started": "2025-11-11T04:13:10.690450Z"
        },
        "trusted": true,
        "id": "BVONpa0ADkLW"
      },
      "outputs": [],
      "source": [
        "import numpy, scipy, sklearn, matplotlib, torch\n",
        "print(\"NumPy:\", numpy.__version__)\n",
        "print(\"SciPy:\", scipy.__version__)\n",
        "print(\"sklearn:\", sklearn.__version__)\n",
        "print(\"Matplotlib:\", matplotlib.__version__)\n",
        "print(\"Torch:\", torch.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I9aao5RDkLa"
      },
      "source": [
        "\ud83d\udcd8 3\ufe0f\u20e3 Dataset Paths & Basic Setup\n",
        "\n",
        "The SOCOFing dataset contains:\n",
        "\n",
        "- Real fingerprints  \n",
        "- Altered fingerprints (Easy, Medium, Hard)  \n",
        "  Types: CR (central rotation), OB (obliteration), ZW (z-warp)\n",
        "\n",
        "We collect all BMP file paths and label them:\n",
        "- real  \n",
        "- altered_easy  \n",
        "- altered_medium  \n",
        "- altered_hard  \n",
        "\n",
        "We also extract subject IDs to ensure proper train/val/test splitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T04:13:21.221209Z",
          "iopub.status.busy": "2025-11-11T04:13:21.220725Z",
          "iopub.status.idle": "2025-11-11T04:13:25.193564Z",
          "shell.execute_reply": "2025-11-11T04:13:25.192866Z",
          "shell.execute_reply.started": "2025-11-11T04:13:21.221183Z"
        },
        "trusted": true,
        "id": "FVONkxjPDkLc"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 1\ufe0f\u20e3 SETUP ENVIRONMENT\n",
        "# ===============================\n",
        "import os, glob, random, shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models, utils\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T04:39:55.909436Z",
          "iopub.status.busy": "2025-11-11T04:39:55.908656Z",
          "iopub.status.idle": "2025-11-11T04:39:56.084728Z",
          "shell.execute_reply": "2025-11-11T04:39:56.083908Z",
          "shell.execute_reply.started": "2025-11-11T04:39:55.909409Z"
        },
        "trusted": true,
        "id": "xHqHNKMTDkLf"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 2\ufe0f\u20e3 DATASET PATHS\n",
        "# ===============================\n",
        "base = \"/kaggle/input/socofing/SOCOFing\"\n",
        "real_path = f\"{base}/Real\"\n",
        "alter_easy = f\"{base}/Altered/Altered-Easy\"\n",
        "alter_med  = f\"{base}/Altered/Altered-Medium\"\n",
        "alter_hard = f\"{base}/Altered/Altered-Hard\"\n",
        "\n",
        "# Collect image paths and labels\n",
        "def collect_paths(folder, label):\n",
        "    files = glob.glob(os.path.join(folder, \"*.BMP\"))\n",
        "    return [(f, label) for f in files]\n",
        "\n",
        "data = []\n",
        "data += collect_paths(real_path, \"real\")\n",
        "data += collect_paths(alter_easy, \"altered_easy\")\n",
        "data += collect_paths(alter_med, \"altered_medium\")\n",
        "data += collect_paths(alter_hard, \"altered_hard\")\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"path\", \"label\"])\n",
        "df[\"subject_id\"] = df[\"path\"].apply(lambda x: os.path.basename(x).split(\"__\")[0])\n",
        "df.sample(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl-Z3ByXDkLi"
      },
      "source": [
        "\ud83d\udcd8 4\ufe0f\u20e3 Splitting Dataset by Subject ID\n",
        "\n",
        "We prevent data leakage by splitting based on subject_id.\n",
        "\n",
        "This ensures:\n",
        "- Training fingerprints and test fingerprints never belong to the same person  \n",
        "- More realistic forensic evaluation  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T04:40:01.131323Z",
          "iopub.status.busy": "2025-11-11T04:40:01.131036Z",
          "iopub.status.idle": "2025-11-11T04:40:01.163948Z",
          "shell.execute_reply": "2025-11-11T04:40:01.162888Z",
          "shell.execute_reply.started": "2025-11-11T04:40:01.131304Z"
        },
        "trusted": true,
        "id": "AVfr0CuiDkLj"
      },
      "outputs": [],
      "source": [
        "# Split at subject-level to avoid data leakage\n",
        "unique_subjects = df[\"subject_id\"].unique()\n",
        "train_ids, test_ids = train_test_split(unique_subjects, test_size=0.2, random_state=42)\n",
        "val_ids, test_ids = train_test_split(test_ids, test_size=0.5, random_state=42)\n",
        "\n",
        "train_df = df[df[\"subject_id\"].isin(train_ids)]\n",
        "val_df = df[df[\"subject_id\"].isin(val_ids)]\n",
        "test_df = df[df[\"subject_id\"].isin(test_ids)]\n",
        "\n",
        "print(\"Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T04:40:17.716129Z",
          "iopub.status.busy": "2025-11-11T04:40:17.715335Z",
          "iopub.status.idle": "2025-11-11T04:40:17.727788Z",
          "shell.execute_reply": "2025-11-11T04:40:17.726707Z",
          "shell.execute_reply.started": "2025-11-11T04:40:17.716105Z"
        },
        "trusted": true,
        "id": "pV3bpjLiDkLl"
      },
      "outputs": [],
      "source": [
        "print(\"Train distribution:\")\n",
        "print(train_df['label'].value_counts())\n",
        "print(\"\\nValidation distribution:\")\n",
        "print(val_df['label'].value_counts())\n",
        "print(\"\\nTest distribution:\")\n",
        "print(test_df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQVJsQX9DkLn"
      },
      "source": [
        "\ud83d\udcd8 5\ufe0f\u20e3 Class Weight Calculation\n",
        "\n",
        "The dataset contains **many more 'real'** samples than altered.\n",
        "\n",
        "To balance the model:\n",
        "- Compute class weights  \n",
        "- Apply weighted cross-entropy loss  \n",
        "\n",
        "This prevents bias toward predicting \"real\" every time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T04:40:26.046096Z",
          "iopub.status.busy": "2025-11-11T04:40:26.045305Z",
          "iopub.status.idle": "2025-11-11T04:40:26.067363Z",
          "shell.execute_reply": "2025-11-11T04:40:26.066520Z",
          "shell.execute_reply.started": "2025-11-11T04:40:26.046069Z"
        },
        "trusted": true,
        "id": "_mrWGgINDkLp"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# \u2696\ufe0f CLASS WEIGHTS (Option A \u2013 Weighted Loss)\n",
        "# ===============================\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Define the class labels (0 = real, 1 = altered)\n",
        "classes = np.array([0, 1])\n",
        "\n",
        "# Compute weights based on the dataset distribution\n",
        "real_count = len(df[df['label'] == 'real'])\n",
        "altered_count = len(df) - real_count\n",
        "\n",
        "print(\"Real samples:\", real_count)\n",
        "print(\"Altered samples:\", altered_count)\n",
        "\n",
        "labels = [0]*real_count + [1]*altered_count\n",
        "weights = compute_class_weight('balanced', classes=classes, y=labels)\n",
        "\n",
        "# Display computed weights\n",
        "print(\"Computed Class Weights \u2192\", weights)\n",
        "# Example output: [4.45, 0.55]  (meaning class 0 gets higher importance)\n",
        "\n",
        "# Convert to torch tensor for use in loss function later\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVQAoaYkDkLr"
      },
      "source": [
        "\ud83d\udcd8 6\ufe0f\u20e3 Sample Visualization from Each Category\n",
        "\n",
        "This cell shows one random image from:\n",
        "\n",
        "- Real\n",
        "- Altered-Easy\n",
        "- Altered-Medium\n",
        "- Altered-Hard\n",
        "\n",
        "Purpose:\n",
        "Quick sanity check to visually confirm dataset integrity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T04:40:32.540600Z",
          "iopub.status.busy": "2025-11-11T04:40:32.540018Z",
          "iopub.status.idle": "2025-11-11T04:40:32.937471Z",
          "shell.execute_reply": "2025-11-11T04:40:32.936604Z",
          "shell.execute_reply.started": "2025-11-11T04:40:32.540577Z"
        },
        "trusted": true,
        "id": "nJuNgY7aDkLr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Randomly pick samples from each class\n",
        "categories = ['real', 'altered_easy', 'altered_medium', 'altered_hard']\n",
        "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
        "\n",
        "for i, cat in enumerate(categories):\n",
        "    path = random.choice(df[df['label'] == cat]['path'].values)\n",
        "    img = Image.open(path).convert('L')\n",
        "    axes[i].imshow(img, cmap='gray')\n",
        "    axes[i].set_title(cat)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T04:40:38.746629Z",
          "iopub.status.busy": "2025-11-11T04:40:38.745765Z",
          "iopub.status.idle": "2025-11-11T04:40:39.141254Z",
          "shell.execute_reply": "2025-11-11T04:40:39.140268Z",
          "shell.execute_reply.started": "2025-11-11T04:40:38.746603Z"
        },
        "trusted": true,
        "id": "g5_RD2xODkLs"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Randomly pick samples from each class\n",
        "categories = ['real', 'altered_easy', 'altered_medium', 'altered_hard']\n",
        "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
        "\n",
        "for i, cat in enumerate(categories):\n",
        "    path = random.choice(df[df['label'] == cat]['path'].values)\n",
        "    img = Image.open(path).convert('L')\n",
        "    axes[i].imshow(img, cmap='gray')\n",
        "    axes[i].set_title(cat)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T04:40:45.063983Z",
          "iopub.status.busy": "2025-11-11T04:40:45.063644Z",
          "iopub.status.idle": "2025-11-11T04:40:45.087426Z",
          "shell.execute_reply": "2025-11-11T04:40:45.086645Z",
          "shell.execute_reply.started": "2025-11-11T04:40:45.063961Z"
        },
        "trusted": true,
        "id": "NrBmJ-KHDkLt"
      },
      "outputs": [],
      "source": [
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(\"Dataframe shuffled and ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W-BLFZEDkLu"
      },
      "source": [
        "\ud83d\udcd8 7\ufe0f\u20e3 Dataset Class for Classification\n",
        "\n",
        "We create a PyTorch Dataset class that:\n",
        "\n",
        "- Loads & resizes fingerprint images  \n",
        "- Converts to RGB (ResNet50 requirement)  \n",
        "- Normalizes using ImageNet mean/std  \n",
        "- Maps labels \u2192 0 (real), 1 (altered)\n",
        "\n",
        "This class feeds into the DataLoader.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T04:41:05.563154Z",
          "iopub.status.busy": "2025-11-11T04:41:05.562871Z",
          "iopub.status.idle": "2025-11-11T04:41:05.579977Z",
          "shell.execute_reply": "2025-11-11T04:41:05.579264Z",
          "shell.execute_reply.started": "2025-11-11T04:41:05.563135Z"
        },
        "trusted": true,
        "id": "YCwjUySDDkLv"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 4\ufe0f\u20e3 DATASET CLASS\n",
        "# ===============================\n",
        "class FingerprintDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.label_map = {'real': 0, 'altered_easy': 1, 'altered_medium': 1, 'altered_hard': 1}\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row.path).convert(\"RGB\").resize((224,224))  # <-- changed to RGB\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = self.label_map[row.label]\n",
        "        return img, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_data = FingerprintDataset(train_df, transform)\n",
        "val_data = FingerprintDataset(val_df, transform)\n",
        "test_data = FingerprintDataset(test_df, transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32)\n",
        "test_loader = DataLoader(test_data, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71mM1rt3DkLw"
      },
      "source": [
        "\ud83d\udcd8 8\ufe0f\u20e3 Classification Model Setup (ResNet50)\n",
        "\n",
        "We fine-tune ResNet50:\n",
        "\n",
        "- Pretrained on ImageNet  \n",
        "- Replace final FC layer \u2192 2 classes  \n",
        "- Use weighted loss  \n",
        "- Adam optimizer + LR scheduler  \n",
        "\n",
        "This model detects whether a fingerprint is real or altered.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T04:41:48.511849Z",
          "iopub.status.busy": "2025-11-11T04:41:48.511510Z",
          "iopub.status.idle": "2025-11-11T04:41:49.114951Z",
          "shell.execute_reply": "2025-11-11T04:41:49.113920Z",
          "shell.execute_reply.started": "2025-11-11T04:41:48.511798Z"
        },
        "trusted": true,
        "id": "yeI2IaUxDkLx"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 5\ufe0f\u20e3 CLASSIFICATION MODEL (with Weighted Loss)\n",
        "# ===============================\n",
        "\n",
        "# Load a pretrained ResNet50\n",
        "model = models.resnet50(weights=\"IMAGENET1K_V1\")   # updated param name for PyTorch \u22652.0\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)      # 2 output classes (real / altered)\n",
        "model = model.to(device)\n",
        "\n",
        "# Use the weighted loss computed earlier\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "# Optional learning-rate scheduler (helps stabilize training)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "print(\"Model, loss, and optimizer ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krxS8Q0fDkLy"
      },
      "source": [
        "\ud83d\udcd8 9\ufe0f\u20e3 Training Loop with Early Stopping\n",
        "\n",
        "Training features:\n",
        "\n",
        "- Live training accuracy  \n",
        "- Validation accuracy each epoch  \n",
        "- Early stopping to prevent overfitting  \n",
        "- Save best-weight checkpoint to /kaggle/working  \n",
        "\n",
        "Output:\n",
        "fingerprint_classifier.pth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T04:47:57.854428Z",
          "iopub.status.busy": "2025-11-11T04:47:57.854127Z",
          "iopub.status.idle": "2025-11-11T06:19:03.625940Z",
          "shell.execute_reply": "2025-11-11T06:19:03.625134Z",
          "shell.execute_reply.started": "2025-11-11T04:47:57.854408Z"
        },
        "trusted": true,
        "id": "JkG5PGQQDkLz"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 6\ufe0f\u20e3 TRAINING LOOP (Live Accuracy + Early Stopping)\n",
        "# ===============================\n",
        "\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=15, patience=3):\n",
        "    best_acc = 0\n",
        "    no_improve = 0\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, correct_train, total_train = 0.0, 0, 0\n",
        "        start = time.time()\n",
        "\n",
        "        # ======== TRAINING ========\n",
        "        for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\", leave=False):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct_train += (preds == labels).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        train_acc = correct_train / total_train\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # ======== VALIDATION ========\n",
        "        model.eval()\n",
        "        preds, true = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\", leave=False):\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                pred = torch.argmax(outputs, dim=1)\n",
        "                preds.extend(pred.cpu().numpy())\n",
        "                true.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_acc = accuracy_score(true, preds)\n",
        "        scheduler.step()\n",
        "\n",
        "        # ======== Logging ========\n",
        "        history['train_loss'].append(avg_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:02d}/{epochs} | \"\n",
        "              f\"Train Loss: {avg_loss:.4f} | \"\n",
        "              f\"Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Acc: {val_acc:.4f} | \"\n",
        "              f\"Time: {(time.time()-start)/60:.2f} min\")\n",
        "\n",
        "        # ======== Early Stopping ========\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            no_improve = 0\n",
        "            torch.save(model.state_dict(), \"/kaggle/working/fingerprint_classifier.pth\")\n",
        "            print(f\"\u2705 Best model updated (Val Acc: {best_acc:.4f})\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            print(f\"\u26a0\ufe0f No improvement for {no_improve} epoch(s).\")\n",
        "\n",
        "        if no_improve >= patience:\n",
        "            print(f\"\u23f9 Early stopping triggered (no improvement for {patience} epochs).\")\n",
        "            break\n",
        "\n",
        "    print(\"\\nTraining complete. Best validation accuracy:\", round(best_acc, 4))\n",
        "    return history\n",
        "\n",
        "# Run training\n",
        "history = train_model(model, train_loader, val_loader, epochs=15, patience=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFX-9nvFDkL0"
      },
      "source": [
        "\ud83d\udcd8 \ud83d\udd1f Evaluate on Test Dataset\n",
        "\n",
        "We reload the best model and compute:\n",
        "\n",
        "- Confusion matrix  \n",
        "- Precision  \n",
        "- Recall  \n",
        "- F1-score  \n",
        "- Class-wise report  \n",
        "\n",
        "This validates forensic-level performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T06:24:38.633733Z",
          "iopub.status.busy": "2025-11-11T06:24:38.633303Z",
          "iopub.status.idle": "2025-11-11T06:25:45.703026Z",
          "shell.execute_reply": "2025-11-11T06:25:45.702143Z",
          "shell.execute_reply.started": "2025-11-11T06:24:38.633704Z"
        },
        "trusted": true,
        "id": "SdigWMd5DkL0"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 7\ufe0f\u20e3 EVALUATION ON TEST SET\n",
        "# ===============================\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import torch\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "model.load_state_dict(torch.load(\"/kaggle/working/fingerprint_classifier.pth\"))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        preds = outputs.argmax(1).cpu().numpy()\n",
        "        y_pred.extend(preds)\n",
        "        y_true.extend(labels.numpy())\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=[\"real\", \"altered\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2vE9rRmDkL1"
      },
      "source": [
        "\ud83d\udcd8 1\ufe0f\u20e31\ufe0f\u20e3 Random Sample Predictions\n",
        "\n",
        "Show 6 random fingerprint samples with:\n",
        "\n",
        "- True label  \n",
        "- Predicted label  \n",
        "\n",
        "Useful for debugging & demo screenshots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T06:27:47.109502Z",
          "iopub.status.busy": "2025-11-11T06:27:47.109160Z",
          "iopub.status.idle": "2025-11-11T06:27:47.847999Z",
          "shell.execute_reply": "2025-11-11T06:27:47.846922Z",
          "shell.execute_reply.started": "2025-11-11T06:27:47.109479Z"
        },
        "trusted": true,
        "id": "imYBPRCWDkL1"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# \ud83d\udd0d RANDOM SAMPLE PREDICTION DEMO\n",
        "# ===============================\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import random\n",
        "\n",
        "# Load model (best checkpoint)\n",
        "model.load_state_dict(torch.load(\"/kaggle/working/fingerprint_classifier.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Define preprocessing (same as training)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Randomly select samples from test_df\n",
        "sample_df = test_df.sample(6, random_state=42).reset_index(drop=True)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "for i, row in enumerate(sample_df.itertuples()):\n",
        "    img = Image.open(row.path).convert(\"RGB\").resize((224,224))\n",
        "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        pred = torch.argmax(output, dim=1).item()\n",
        "        label_pred = \"Altered\" if pred == 1 else \"Real\"\n",
        "        label_true = \"Altered\" if \"altered\" in row.label else \"Real\"\n",
        "\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title(f\"True: {label_true}\\nPred: {label_pred}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.suptitle(\"Random Fingerprint Classification Samples\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o51or7xQDkL2"
      },
      "source": [
        "\ud83e\uddfe Current Model Summary\n",
        "\n",
        "Model file:\n",
        "fingerprint_classifier.pth\n",
        "\n",
        "Purpose:\n",
        "Detects whether a fingerprint is Real or Altered.\n",
        "\n",
        "Architecture:\n",
        "ResNet50 (pretrained on ImageNet)\n",
        "\u2192 fine-tuned for binary classification (real=0, altered=1)\n",
        "\n",
        "Performance:\n",
        "\n",
        "Validation Accuracy: 99.85%\n",
        "\n",
        "Test Accuracy: ~99.96%\n",
        "\n",
        "Balanced precision and recall for both classes \u2705\n",
        "\n",
        "Early stopping to prevent overfitting \u2705"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D0QdsLpDkL4"
      },
      "source": [
        "\ud83d\udcd8 1\ufe0f\u20e32\ufe0f\u20e3 GAN Pairing Logic\n",
        "\n",
        "Pix2Pix GAN requires paired data:\n",
        "(altered_fingerprint, real_fingerprint)\n",
        "\n",
        "This cell:\n",
        "- Extracts subject ID + finger type  \n",
        "- Matches altered \u2192 real pairs intelligently  \n",
        "- Creates a clean list of exact pairs  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZnkdAYZDkL4"
      },
      "source": [
        "\ud83d\udcd8 1\ufe0f\u20e33\ufe0f\u20e3 GAN Dataset\n",
        "\n",
        "Creates a dataset tailored for Pix2Pix:\n",
        "\n",
        "- Loads paired images  \n",
        "- Converts to grayscale  \n",
        "- Normalizes to [-1, 1]  \n",
        "- Outputs (altered_tensor, real_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T06:30:34.248379Z",
          "iopub.status.busy": "2025-11-11T06:30:34.248066Z",
          "iopub.status.idle": "2025-11-11T06:30:34.441843Z",
          "shell.execute_reply": "2025-11-11T06:30:34.441011Z",
          "shell.execute_reply.started": "2025-11-11T06:30:34.248356Z"
        },
        "trusted": true,
        "id": "FiZdzMy4DkL5"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 8\ufe0f\u20e3 CREATE PAIRED DATA (Altered \u2192 Real) \u2014 Improved Version\n",
        "# ===============================\n",
        "import os, glob\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "# Improved pairing: match both subject ID and finger type\n",
        "def extract_id_finger(path):\n",
        "    parts = os.path.basename(path).split(\"__\")\n",
        "    return \"__\".join(parts[:2])  # e.g., \"100__M_Left_index_finger\"\n",
        "\n",
        "# Build real-image lookup table\n",
        "real_dict = {extract_id_finger(p): p for p in glob.glob(f\"{real_path}/*.BMP\")}\n",
        "\n",
        "# Collect altered\u2192real pairs\n",
        "pairs = []\n",
        "for alt_path in (\n",
        "    glob.glob(f\"{alter_easy}/*.BMP\") +\n",
        "    glob.glob(f\"{alter_med}/*.BMP\") +\n",
        "    glob.glob(f\"{alter_hard}/*.BMP\")\n",
        "):\n",
        "    # Clean alteration suffixes (e.g., _CR, _OB, _ZW)\n",
        "    key = extract_id_finger(\n",
        "        alt_path.replace(\"_CR\", \"\").replace(\"_OB\", \"\").replace(\"_ZW\", \"\")\n",
        "    )\n",
        "    if key in real_dict:\n",
        "        pairs.append((alt_path, real_dict[key]))\n",
        "\n",
        "print(\"\u2705 Paired samples found:\", len(pairs))\n",
        "\n",
        "# ===============================\n",
        "# \ud83d\udd27 Dataset for GAN Restoration\n",
        "# ===============================\n",
        "class RestorationDataset(Dataset):\n",
        "    def __init__(self, pairs, transform=None):\n",
        "        self.pairs = pairs\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        altered_path, real_path = self.pairs[idx]\n",
        "        a_img = Image.open(altered_path).convert(\"L\").resize((256, 256))\n",
        "        r_img = Image.open(real_path).convert(\"L\").resize((256, 256))\n",
        "        if self.transform:\n",
        "            a_img = self.transform(a_img)\n",
        "            r_img = self.transform(r_img)\n",
        "        return a_img, r_img\n",
        "\n",
        "# Normalization \u2192 [-1, 1] range for GAN stability\n",
        "transform_gan = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Create dataset and dataloader\n",
        "rest_data = RestorationDataset(pairs, transform_gan)\n",
        "train_loader_gan = DataLoader(rest_data, batch_size=4, shuffle=True)\n",
        "\n",
        "print(\"\u2705 Restoration dataset ready with\", len(rest_data), \"samples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYPQQXEbDkL7"
      },
      "source": [
        "\ud83d\udcd8 1\ufe0f\u20e34\ufe0f\u20e3 Pix2Pix Architecture\n",
        "\n",
        "Pix2Pix uses:\n",
        "\n",
        "- U-Net Generator  \n",
        "- PatchGAN Discriminator  \n",
        "\n",
        "Both are recreated here exactly as in the original paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T06:32:19.339069Z",
          "iopub.status.busy": "2025-11-11T06:32:19.338694Z",
          "iopub.status.idle": "2025-11-11T06:32:20.420235Z",
          "shell.execute_reply": "2025-11-11T06:32:20.419302Z",
          "shell.execute_reply.started": "2025-11-11T06:32:19.339045Z"
        },
        "trusted": true,
        "id": "qsbY0APpDkL8"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 9\ufe0f\u20e3 PIX2PIX COMPONENTS: Generator (U-Net) + PatchGAN Discriminator\n",
        "# ===============================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Device should already be set earlier\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------------------\n",
        "# Helper: weight initialization\n",
        "# ---------------------------\n",
        "def init_weights(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        if getattr(m, 'bias', None) is not None:\n",
        "            nn.init.constant_(m.bias.data, 0.0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "# ---------------------------\n",
        "# U-Net Generator (grayscale in/out)\n",
        "# ---------------------------\n",
        "class UNetGenerator(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, features=64):\n",
        "        super().__init__()\n",
        "        # Encoder blocks: Conv -> BN -> LeakyReLU (no BN on first)\n",
        "        self.down1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features, 4, 2, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )  # 256 -> 128\n",
        "        self.down2 = nn.Sequential(\n",
        "            nn.Conv2d(features, features*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*2),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )  # 128 -> 64\n",
        "        self.down3 = nn.Sequential(\n",
        "            nn.Conv2d(features*2, features*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*4),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )  # 64 -> 32\n",
        "        self.down4 = nn.Sequential(\n",
        "            nn.Conv2d(features*4, features*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )  # 32 -> 16\n",
        "        self.down5 = nn.Sequential(\n",
        "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )  # 16 -> 8\n",
        "        self.down6 = nn.Sequential(\n",
        "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )  # 8 -> 4\n",
        "        self.down7 = nn.Sequential(\n",
        "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )  # 4 -> 2\n",
        "        self.down8 = nn.Sequential(\n",
        "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )  # 2 -> 1 (bottleneck)\n",
        "\n",
        "        # Decoder blocks: ConvTranspose -> BN -> ReLU -> dropout optional -> concat skip\n",
        "        def up(in_ch, out_ch, dropout=False):\n",
        "            layers = [nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1, bias=False),\n",
        "                      nn.BatchNorm2d(out_ch),\n",
        "                      nn.ReLU(inplace=True)]\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout(0.5))\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        self.up1 = up(features*8, features*8, dropout=True)   # 1 -> 2\n",
        "        self.up2 = up(features*16, features*8, dropout=True)  # 2 -> 4\n",
        "        self.up3 = up(features*16, features*8, dropout=True)  # 4 -> 8\n",
        "        self.up4 = up(features*16, features*8)                # 8 -> 16\n",
        "        self.up5 = up(features*16, features*4)                # 16 -> 32\n",
        "        self.up6 = up(features*8, features*2)                 # 32 -> 64\n",
        "        self.up7 = up(features*4, features)                   # 64 -> 128\n",
        "        self.up8 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(features*2, out_channels, 4, 2, 1, bias=True),\n",
        "            nn.Tanh()\n",
        "        )  # 128 -> 256\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.down1(x)\n",
        "        e2 = self.down2(e1)\n",
        "        e3 = self.down3(e2)\n",
        "        e4 = self.down4(e3)\n",
        "        e5 = self.down5(e4)\n",
        "        e6 = self.down6(e5)\n",
        "        e7 = self.down7(e6)\n",
        "        e8 = self.down8(e7)\n",
        "\n",
        "        d1 = self.up1(e8)\n",
        "        d1 = torch.cat([d1, e7], dim=1)\n",
        "        d2 = self.up2(d1)\n",
        "        d2 = torch.cat([d2, e6], dim=1)\n",
        "        d3 = self.up3(d2)\n",
        "        d3 = torch.cat([d3, e5], dim=1)\n",
        "        d4 = self.up4(d3)\n",
        "        d4 = torch.cat([d4, e4], dim=1)\n",
        "        d5 = self.up5(d4)\n",
        "        d5 = torch.cat([d5, e3], dim=1)\n",
        "        d6 = self.up6(d5)\n",
        "        d6 = torch.cat([d6, e2], dim=1)\n",
        "        d7 = self.up7(d6)\n",
        "        d7 = torch.cat([d7, e1], dim=1)\n",
        "        out = self.up8(d7)\n",
        "        return out\n",
        "\n",
        "# ---------------------------\n",
        "# PatchGAN Discriminator\n",
        "# ---------------------------\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels=2, features=64):\n",
        "        super().__init__()\n",
        "        # in_channels = input(1) concat target/generated(1) = 2\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features, 4, 2, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(features, features*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(features*2, features*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(features*4, features*8, 4, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(features*8, 1, 4, 1, 1, bias=True)  # output one-channel patch map\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # concatenate along channel dim: (B,2,256,256)\n",
        "        xy = torch.cat([x, y], dim=1)\n",
        "        return self.model(xy)\n",
        "\n",
        "# ---------------------------\n",
        "# Instantiate and init\n",
        "# ---------------------------\n",
        "gen = UNetGenerator(in_channels=1, out_channels=1).to(device)\n",
        "disc = PatchDiscriminator(in_channels=2).to(device)\n",
        "gen.apply(init_weights)\n",
        "disc.apply(init_weights)\n",
        "\n",
        "# ---------------------------\n",
        "# Losses + optimizers + schedulers\n",
        "# ---------------------------\n",
        "bce_loss = nn.BCEWithLogitsLoss()\n",
        "l1_loss = nn.L1Loss()\n",
        "\n",
        "lr = 2e-4\n",
        "opt_g = Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "opt_d = Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# Optional schedulers\n",
        "scheduler_g = torch.optim.lr_scheduler.StepLR(opt_g, step_size=30, gamma=0.5)\n",
        "scheduler_d = torch.optim.lr_scheduler.StepLR(opt_d, step_size=30, gamma=0.5)\n",
        "\n",
        "print(\"Generator params:\", sum(p.numel() for p in gen.parameters()))\n",
        "print(\"Discriminator params:\", sum(p.numel() for p in disc.parameters()))\n",
        "\n",
        "# ---------------------------\n",
        "# Smoke test: run a batch through gen + disc to validate shapes\n",
        "# ---------------------------\n",
        "batch = next(iter(train_loader_gan))  # expects dataset already created\n",
        "altered_batch, real_batch = batch\n",
        "altered_batch = altered_batch.to(device)    # shape (B,1,256,256)\n",
        "real_batch = real_batch.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    fake = gen(altered_batch)               # should be (B,1,256,256)\n",
        "    d_real = disc(altered_batch, real_batch) # patch map\n",
        "    d_fake = disc(altered_batch, fake)       # patch map\n",
        "\n",
        "print(\"Altered:\", altered_batch.shape)\n",
        "print(\"Real:\", real_batch.shape)\n",
        "print(\"Fake:\", fake.shape)\n",
        "print(\"D_real:\", d_real.shape, \"D_fake:\", d_fake.shape)\n",
        "\n",
        "# Save initial generator checkpoint (optional)\n",
        "torch.save(gen.state_dict(), \"/kaggle/working/generator_init.pth\")\n",
        "torch.save(disc.state_dict(), \"/kaggle/working/discriminator_init.pth\")\n",
        "print(\"Pix2Pix components ready. Run the training loop next.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUCW2R1nDkL_"
      },
      "source": [
        "\ud83d\udcd8 1\ufe0f\u20e35\ufe0f\u20e3 Pix2Pix Training Loop\n",
        "\n",
        "GAN training logic:\n",
        "\n",
        "- Train discriminator on real vs fake pairs  \n",
        "- Train generator using adversarial + L1 loss  \n",
        "- Track PSNR, SSIM, L1  \n",
        "- Save best generator and checkpoints  \n",
        "\n",
        "This part runs longer (100 epochs recommended).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T06:33:38.788964Z",
          "iopub.status.busy": "2025-11-11T06:33:38.788399Z",
          "iopub.status.idle": "2025-11-11T07:55:17.770936Z",
          "shell.execute_reply": "2025-11-11T07:55:17.769662Z",
          "shell.execute_reply.started": "2025-11-11T06:33:38.788938Z"
        },
        "trusted": true,
        "id": "J3yaIbcbDkMA"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# \ud83d\udd25 PIX2PIX TRAINING LOOP (Adversarial + L1) with Val Monitoring & Visuals\n",
        "# ===============================\n",
        "import time\n",
        "import os\n",
        "from torch.utils.data import random_split\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----- Hyperparams -----\n",
        "epochs = 100\n",
        "batch_size = 4               # already used; keep same\n",
        "lr = 2e-4\n",
        "lambda_l1 = 100.0            # pix2pix typical\n",
        "save_every = 5               # save checkpoints every N epochs\n",
        "viz_every = 5                # visualize every N epochs\n",
        "patience = 8                 # early stop on val L1 stagnation\n",
        "\n",
        "# ----- Prepare train/val split -----\n",
        "val_frac = 0.1\n",
        "n_total = len(rest_data)\n",
        "n_val = int(n_total * val_frac)\n",
        "n_train = n_total - n_val\n",
        "train_ds, val_ds = random_split(rest_data, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train pairs: {len(train_ds)} | Val pairs: {len(val_ds)}\")\n",
        "\n",
        "# Ensure optimizers/schedulers are set (they were created earlier but recreate for safety)\n",
        "opt_g = Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "opt_d = Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "scheduler_g = torch.optim.lr_scheduler.StepLR(opt_g, step_size=30, gamma=0.5)\n",
        "scheduler_d = torch.optim.lr_scheduler.StepLR(opt_d, step_size=30, gamma=0.5)\n",
        "\n",
        "bce_loss = nn.BCEWithLogitsLoss()\n",
        "l1_loss = nn.L1Loss()\n",
        "\n",
        "# helpers\n",
        "def tensor_to_image(tensor):\n",
        "    # tensor in [-1,1], shape (1,H,W) or (C,H,W) -> uint8 grayscale\n",
        "    img = tensor.cpu().numpy()\n",
        "    img = (img * 0.5 + 0.5)  # -> [0,1]\n",
        "    img = np.clip(img, 0, 1)\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    if img.shape[0] == 1:\n",
        "        return img[0]\n",
        "    return np.transpose(img, (1,2,0))\n",
        "\n",
        "def compute_metrics_batch(fake, real):\n",
        "    # fake, real: torch tensors shape (B,1,H,W) in [-1,1]\n",
        "    fake_np = (fake.cpu().numpy() * 0.5 + 0.5)\n",
        "    real_np = (real.cpu().numpy() * 0.5 + 0.5)\n",
        "    psnr_vals, ssim_vals = [], []\n",
        "    for i in range(fake_np.shape[0]):\n",
        "        f = (fake_np[i,0]*255).astype(np.uint8)\n",
        "        r = (real_np[i,0]*255).astype(np.uint8)\n",
        "        try:\n",
        "            psnr_vals.append(psnr(r, f, data_range=255))\n",
        "            ssim_vals.append(ssim(r, f, data_range=255))\n",
        "        except:\n",
        "            psnr_vals.append(0)\n",
        "            ssim_vals.append(0)\n",
        "    return np.mean(psnr_vals), np.mean(ssim_vals)\n",
        "\n",
        "# training state\n",
        "best_val_l1 = float('inf')\n",
        "no_improve = 0\n",
        "\n",
        "history = {'g_loss': [], 'd_loss': [], 'val_l1': [], 'val_psnr': [], 'val_ssim': []}\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    gen.train(); disc.train()\n",
        "    t0 = time.time()\n",
        "    running_g_loss = 0.0\n",
        "    running_d_loss = 0.0\n",
        "\n",
        "    for altered, real in train_loader:\n",
        "        altered = altered.to(device)   # (B,1,256,256)\n",
        "        real = real.to(device)\n",
        "\n",
        "        # ---------------------\n",
        "        # Train Discriminator\n",
        "        # ---------------------\n",
        "        opt_d.zero_grad()\n",
        "        fake = gen(altered)\n",
        "\n",
        "        # real labels = 1, fake labels = 0\n",
        "        d_real = disc(altered, real)\n",
        "        d_fake = disc(altered, fake.detach())\n",
        "\n",
        "        loss_d_real = bce_loss(d_real, torch.ones_like(d_real))\n",
        "        loss_d_fake = bce_loss(d_fake, torch.zeros_like(d_fake))\n",
        "        loss_d = (loss_d_real + loss_d_fake) * 0.5\n",
        "        loss_d.backward()\n",
        "        opt_d.step()\n",
        "\n",
        "        # ---------------------\n",
        "        # Train Generator\n",
        "        # ---------------------\n",
        "        opt_g.zero_grad()\n",
        "        fake = gen(altered)\n",
        "        d_fake_for_g = disc(altered, fake)\n",
        "        loss_g_gan = bce_loss(d_fake_for_g, torch.ones_like(d_fake_for_g))\n",
        "        loss_g_l1 = l1_loss(fake, real) * lambda_l1\n",
        "        loss_g = loss_g_gan + loss_g_l1\n",
        "        loss_g.backward()\n",
        "        opt_g.step()\n",
        "\n",
        "        running_g_loss += loss_g.item()\n",
        "        running_d_loss += loss_d.item()\n",
        "\n",
        "    # step schedulers\n",
        "    scheduler_g.step()\n",
        "    scheduler_d.step()\n",
        "\n",
        "    avg_g_loss = running_g_loss / len(train_loader)\n",
        "    avg_d_loss = running_d_loss / len(train_loader)\n",
        "\n",
        "    # ---- validation (compute average L1, PSNR, SSIM) ----\n",
        "    gen.eval(); disc.eval()\n",
        "    val_l1s, val_psnrs, val_ssims = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for altered, real in val_loader:\n",
        "            altered = altered.to(device); real = real.to(device)\n",
        "            fake = gen(altered)\n",
        "            val_l1s.append(l1_loss(fake, real).item())\n",
        "            psnr_b, ssim_b = compute_metrics_batch(fake, real)\n",
        "            val_psnrs.append(psnr_b); val_ssims.append(ssim_b)\n",
        "\n",
        "    mean_val_l1 = float(np.mean(val_l1s))\n",
        "    mean_val_psnr = float(np.mean(val_psnrs))\n",
        "    mean_val_ssim = float(np.mean(val_ssims))\n",
        "\n",
        "    history['g_loss'].append(avg_g_loss)\n",
        "    history['d_loss'].append(avg_d_loss)\n",
        "    history['val_l1'].append(mean_val_l1)\n",
        "    history['val_psnr'].append(mean_val_psnr)\n",
        "    history['val_ssim'].append(mean_val_ssim)\n",
        "\n",
        "    print(f\"Epoch {epoch:03d}/{epochs} | G_loss: {avg_g_loss:.4f} | D_loss: {avg_d_loss:.4f} | \"\n",
        "          f\"Val L1: {mean_val_l1:.6f} | PSNR: {mean_val_psnr:.3f} | SSIM: {mean_val_ssim:.4f} | Time: {(time.time()-t0)/60:.2f} min\")\n",
        "\n",
        "    # save checkpoints\n",
        "    if epoch % save_every == 0:\n",
        "        torch.save(gen.state_dict(), f\"/kaggle/working/generator_epoch{epoch}.pth\")\n",
        "        torch.save(disc.state_dict(), f\"/kaggle/working/discriminator_epoch{epoch}.pth\")\n",
        "        print(f\"\ud83d\udd16 Saved checkpoint at epoch {epoch}\")\n",
        "\n",
        "    # save best by val L1 (lower better)\n",
        "    if mean_val_l1 < best_val_l1:\n",
        "        best_val_l1 = mean_val_l1\n",
        "        no_improve = 0\n",
        "        torch.save(gen.state_dict(), \"/kaggle/working/generator_best.pth\")\n",
        "        print(f\"\u2705 New best generator (val L1: {best_val_l1:.6f})\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        print(f\"\u26a0\ufe0f No improvement for {no_improve} epoch(s) (best L1: {best_val_l1:.6f})\")\n",
        "\n",
        "    # visualize some samples occasionally\n",
        "    if epoch % viz_every == 0 or epoch == 1:\n",
        "        gen.eval()\n",
        "        altered_sample, real_sample = next(iter(val_loader))\n",
        "        with torch.no_grad():\n",
        "            fake_sample = gen(altered_sample.to(device)).cpu()\n",
        "        altered_sample = altered_sample.cpu()\n",
        "        real_sample = real_sample.cpu()\n",
        "\n",
        "        n_show = min(3, altered_sample.shape[0])\n",
        "        plt.figure(figsize=(9,3))\n",
        "        for i in range(n_show):\n",
        "            a = tensor_to_image(altered_sample[i])\n",
        "            f = tensor_to_image(fake_sample[i])\n",
        "            r = tensor_to_image(real_sample[i])\n",
        "            plt.subplot(3, n_show, i+1); plt.imshow(a, cmap='gray'); plt.title(\"Altered\"); plt.axis('off')\n",
        "            plt.subplot(3, n_show, n_show+i+1); plt.imshow(f, cmap='gray'); plt.title(\"Restored\"); plt.axis('off')\n",
        "            plt.subplot(3, n_show, 2*n_show+i+1); plt.imshow(r, cmap='gray'); plt.title(\"Real\"); plt.axis('off')\n",
        "        plt.suptitle(f\"Epoch {epoch} samples (Altered \u2192 Restored \u2192 Real)\")\n",
        "        plt.show()\n",
        "\n",
        "    # early stopping on val L1 stagnation\n",
        "    if no_improve >= patience:\n",
        "        print(f\"\u23f9 Early stopping triggered (no val-L1 improvement for {patience} epochs).\")\n",
        "        break\n",
        "\n",
        "# end training\n",
        "print(\"Training finished. Best val L1:\", best_val_l1)\n",
        "torch.save(gen.state_dict(), \"/kaggle/working/generator_final.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJYAOEHkDkMB"
      },
      "source": [
        "\ud83d\udcd8 1\ufe0f\u20e36\ufe0f\u20e3 GAN Inference \u2014 Restore Fingerprint\n",
        "\n",
        "Loads trained generator and performs:\n",
        "\n",
        "altered \u2192 restored\n",
        "\n",
        "Also displays:\n",
        "- Altered input  \n",
        "- GAN output  \n",
        "- Ground truth real fingerprint  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T09:23:55.648699Z",
          "iopub.status.busy": "2025-11-11T09:23:55.648092Z",
          "iopub.status.idle": "2025-11-11T09:23:56.638069Z",
          "shell.execute_reply": "2025-11-11T09:23:56.637216Z",
          "shell.execute_reply.started": "2025-11-11T09:23:55.648675Z"
        },
        "trusted": true,
        "id": "EjqNzXO2DkMC"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# \ud83d\udd0d Fingerprint Restoration Inference (Exact Pix2Pix Architecture)\n",
        "# ===============================\n",
        "import torch, os\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ----------------------------\n",
        "# 1\ufe0f\u20e3 Define UNetGenerator (same as training)\n",
        "# ----------------------------\n",
        "class UNetGenerator(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, features=64):\n",
        "        super().__init__()\n",
        "        # Encoder\n",
        "        self.down1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features, 4, 2, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down2 = nn.Sequential(\n",
        "            nn.Conv2d(features, features*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*2),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down3 = nn.Sequential(\n",
        "            nn.Conv2d(features*2, features*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*4),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down4 = nn.Sequential(\n",
        "            nn.Conv2d(features*4, features*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down5 = nn.Sequential(\n",
        "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down6 = nn.Sequential(\n",
        "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down7 = nn.Sequential(\n",
        "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down8 = nn.Sequential(\n",
        "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Decoder (mirrors encoder)\n",
        "        def up(in_ch, out_ch, dropout=False):\n",
        "            layers = [nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1, bias=False),\n",
        "                      nn.BatchNorm2d(out_ch),\n",
        "                      nn.ReLU(inplace=True)]\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout(0.5))\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        self.up1 = up(features*8, features*8, dropout=True)\n",
        "        self.up2 = up(features*16, features*8, dropout=True)\n",
        "        self.up3 = up(features*16, features*8, dropout=True)\n",
        "        self.up4 = up(features*16, features*8)\n",
        "        self.up5 = up(features*16, features*4)\n",
        "        self.up6 = up(features*8, features*2)\n",
        "        self.up7 = up(features*4, features)\n",
        "        self.up8 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(features*2, out_channels, 4, 2, 1, bias=True),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.down1(x)\n",
        "        e2 = self.down2(e1)\n",
        "        e3 = self.down3(e2)\n",
        "        e4 = self.down4(e3)\n",
        "        e5 = self.down5(e4)\n",
        "        e6 = self.down6(e5)\n",
        "        e7 = self.down7(e6)\n",
        "        e8 = self.down8(e7)\n",
        "\n",
        "        d1 = self.up1(e8); d1 = torch.cat([d1, e7], dim=1)\n",
        "        d2 = self.up2(d1); d2 = torch.cat([d2, e6], dim=1)\n",
        "        d3 = self.up3(d2); d3 = torch.cat([d3, e5], dim=1)\n",
        "        d4 = self.up4(d3); d4 = torch.cat([d4, e4], dim=1)\n",
        "        d5 = self.up5(d4); d5 = torch.cat([d5, e3], dim=1)\n",
        "        d6 = self.up6(d5); d6 = torch.cat([d6, e2], dim=1)\n",
        "        d7 = self.up7(d6); d7 = torch.cat([d7, e1], dim=1)\n",
        "        out = self.up8(d7)\n",
        "        return out\n",
        "\n",
        "# ----------------------------\n",
        "# 2\ufe0f\u20e3 Load Trained Generator\n",
        "# ----------------------------\n",
        "gen = UNetGenerator(in_channels=1, out_channels=1).to(device)\n",
        "checkpoint_path = \"/kaggle/input/fingerprints-models/other/default/1/generator_best.pth\"\n",
        "gen.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=True)\n",
        "gen.eval()\n",
        "print(\"\u2705 Generator loaded successfully (exact training architecture).\")\n",
        "\n",
        "# ----------------------------\n",
        "# 3\ufe0f\u20e3 Transform\n",
        "# ----------------------------\n",
        "transform_gan = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# ----------------------------\n",
        "# 4\ufe0f\u20e3 Load Example Fingerprints\n",
        "# ----------------------------\n",
        "altered_path = \"/kaggle/input/socofing/SOCOFing/Altered/Altered-Easy/100__M_Left_index_finger_CR.BMP\"\n",
        "real_path = \"/kaggle/input/socofing/SOCOFing/Real/100__M_Left_index_finger.BMP\"\n",
        "\n",
        "altered = Image.open(altered_path).convert(\"L\")\n",
        "real = Image.open(real_path).convert(\"L\")\n",
        "\n",
        "altered_tensor = transform_gan(altered).unsqueeze(0).to(device)\n",
        "\n",
        "# ----------------------------\n",
        "# 5\ufe0f\u20e3 Inference\n",
        "# ----------------------------\n",
        "with torch.no_grad():\n",
        "    restored = gen(altered_tensor).cpu()[0,0].numpy()\n",
        "\n",
        "# Denormalize for visualization\n",
        "restored = (restored * 0.5 + 0.5)\n",
        "altered = np.array(altered.resize((256,256)), dtype=np.float32) / 255.0\n",
        "real = np.array(real.resize((256,256)), dtype=np.float32) / 255.0\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,3,1); plt.imshow(altered, cmap='gray'); plt.title(\"Altered Input\"); plt.axis('off')\n",
        "plt.subplot(1,3,2); plt.imshow(restored, cmap='gray'); plt.title(\"Restored (GAN Output)\"); plt.axis('off')\n",
        "plt.subplot(1,3,3); plt.imshow(real, cmap='gray'); plt.title(\"Ground Truth (Real)\"); plt.axis('off')\n",
        "plt.suptitle(\"Fingerprint Restoration: Altered \u2192 Restored \u2192 Real\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6apOk7pDkMD"
      },
      "source": [
        "\ud83d\udcd8 1\ufe0f\u20e37\ufe0f\u20e3 Quantitative Evaluation\n",
        "\n",
        "We evaluate restoration quality using:\n",
        "\n",
        "- PSNR (clarity)  \n",
        "- SSIM (structural integrity)  \n",
        "- MSE (error)  \n",
        "\n",
        "Then display several test comparisons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T09:26:38.219450Z",
          "iopub.status.busy": "2025-11-11T09:26:38.218756Z",
          "iopub.status.idle": "2025-11-11T09:26:39.381582Z",
          "shell.execute_reply": "2025-11-11T09:26:39.380722Z",
          "shell.execute_reply.started": "2025-11-11T09:26:38.219424Z"
        },
        "trusted": true,
        "id": "Wkt65zNkDkME"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# \ud83d\udd22 Quantitative Evaluation (PSNR, SSIM, MSE) - Standalone Version\n",
        "# ===============================\n",
        "import os, torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim, mean_squared_error as mse\n",
        "\n",
        "# Folder paths\n",
        "altered_root = \"/kaggle/input/socofing/SOCOFing/Altered/Altered-Easy\"\n",
        "real_root = \"/kaggle/input/socofing/SOCOFing/Real\"\n",
        "\n",
        "# Example subset of test pairs (same finger, different types)\n",
        "test_samples = [\n",
        "    \"100__M_Left_index_finger_CR.BMP\",\n",
        "    \"101__M_Left_index_finger_CR.BMP\",\n",
        "    \"102__M_Left_index_finger_CR.BMP\",\n",
        "    \"103__M_Left_index_finger_CR.BMP\",\n",
        "    \"104__M_Left_index_finger_CR.BMP\"\n",
        "]\n",
        "\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "psnr_scores, ssim_scores, mse_scores = [], [], []\n",
        "\n",
        "print(\"Evaluating fingerprint restoration on\", len(test_samples), \"pairs...\")\n",
        "gen.eval()\n",
        "\n",
        "for fname in test_samples:\n",
        "    altered_path = os.path.join(altered_root, fname)\n",
        "    real_path = os.path.join(real_root, fname.replace(\"_CR\", \"\"))  # matching real file\n",
        "\n",
        "    if not os.path.exists(altered_path) or not os.path.exists(real_path):\n",
        "        print(\"\u26a0\ufe0f Missing:\", fname)\n",
        "        continue\n",
        "\n",
        "    altered = Image.open(altered_path).convert(\"L\")\n",
        "    real = Image.open(real_path).convert(\"L\")\n",
        "\n",
        "    altered_tensor = transform_eval(altered).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        restored = gen(altered_tensor).cpu()[0,0].numpy()\n",
        "\n",
        "    # Denormalize and resize\n",
        "    restored = (restored * 0.5 + 0.5)\n",
        "    altered_resized = np.array(altered.resize((256,256)), dtype=np.float32) / 255.0\n",
        "    real_resized = np.array(real.resize((256,256)), dtype=np.float32) / 255.0\n",
        "\n",
        "    # Convert for metric computation\n",
        "    f = (restored * 255).astype(np.uint8)\n",
        "    r = (real_resized * 255).astype(np.uint8)\n",
        "    psnr_val = psnr(r, f, data_range=255)\n",
        "    ssim_val = ssim(r, f, data_range=255)\n",
        "    mse_val = mse(r, f)\n",
        "\n",
        "    psnr_scores.append(psnr_val)\n",
        "    ssim_scores.append(ssim_val)\n",
        "    mse_scores.append(mse_val)\n",
        "\n",
        "    # Visualization per sample\n",
        "    plt.figure(figsize=(9,3))\n",
        "    plt.subplot(1,3,1); plt.imshow(altered_resized, cmap='gray'); plt.title(\"Altered\"); plt.axis('off')\n",
        "    plt.subplot(1,3,2); plt.imshow(restored, cmap='gray'); plt.title(\"Restored\"); plt.axis('off')\n",
        "    plt.subplot(1,3,3); plt.imshow(real_resized, cmap='gray'); plt.title(\"Real\"); plt.axis('off')\n",
        "    plt.suptitle(f\"{fname}\\nPSNR: {psnr_val:.2f} dB | SSIM: {ssim_val:.4f} | MSE: {mse_val:.6f}\", fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Print overall averages\n",
        "print(\"=============================================\")\n",
        "print(f\"\ud83d\udd39 Mean PSNR : {np.mean(psnr_scores):.3f} dB\")\n",
        "print(f\"\ud83d\udd39 Mean SSIM : {np.mean(ssim_scores):.4f}\")\n",
        "print(f\"\ud83d\udd39 Mean MSE  : {np.mean(mse_scores):.6f}\")\n",
        "print(\"=============================================\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epsXGToBDkMS"
      },
      "source": [
        "| Metric   | Your Result | Typical Range                        | Interpretation                                                                                            |\n",
        "| :------- | :---------- | :----------------------------------- | :-------------------------------------------------------------------------------------------------------- |\n",
        "| **PSNR** | 21.89 dB    | 20\u201325 (good), >30 (excellent)        | Your generator is producing clear restorations \u2014 not perfect, but visually meaningful ridge recovery.     |\n",
        "| **SSIM** | **0.943**   | >0.85 = good, >0.90 = excellent      | Excellent structural similarity \u2014 ridge patterns and textures are well-aligned with the real fingerprint. |\n",
        "| **MSE**  | 433.62      | Lower is better (depends on scaling) | Very reasonable for normalized 8-bit grayscale images (0\u2013255).                                            |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17KYkoVADkMT"
      },
      "source": [
        "\ud83d\udcd8 1\ufe0f\u20e38\ufe0f\u20e3 Create Subset for Web App\n",
        "\n",
        "Creates a compact dataset used in the Colab Web App:\n",
        "\n",
        "- Random sample from each folder  \n",
        "- Organized into proper directories  \n",
        "- Zipped for download  \n",
        "\n",
        "Output:\n",
        "SOCOFing_subset.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T10:41:57.127011Z",
          "iopub.status.busy": "2025-11-11T10:41:57.126742Z",
          "iopub.status.idle": "2025-11-11T10:41:58.621870Z",
          "shell.execute_reply": "2025-11-11T10:41:58.620718Z",
          "shell.execute_reply.started": "2025-11-11T10:41:57.126990Z"
        },
        "trusted": true,
        "id": "9KUAglaKDkMU"
      },
      "outputs": [],
      "source": [
        "import os, shutil, random, zipfile, glob\n",
        "\n",
        "# Original dataset base\n",
        "base = \"/kaggle/input/socofing/SOCOFing\"\n",
        "\n",
        "# Target subset folder\n",
        "subset_base = \"/kaggle/working/SOCOFing_subset/SOCOFing\"\n",
        "os.makedirs(subset_base, exist_ok=True)\n",
        "\n",
        "# Define subfolders to include\n",
        "folders = [\n",
        "    \"Altered/Altered-Easy\",\n",
        "    \"Altered/Altered-Medium\",\n",
        "    \"Altered/Altered-Hard\",\n",
        "    \"Real\"\n",
        "]\n",
        "\n",
        "# Number of files per folder\n",
        "num_samples = 15  # You can increase or decrease as needed\n",
        "\n",
        "# Copy small subset\n",
        "for folder in folders:\n",
        "    src = os.path.join(base, folder)\n",
        "    dst = os.path.join(subset_base, folder)\n",
        "    os.makedirs(dst, exist_ok=True)\n",
        "    all_files = glob.glob(os.path.join(src, \"*.BMP\"))\n",
        "    sample_files = random.sample(all_files, min(num_samples, len(all_files)))\n",
        "    for file in sample_files:\n",
        "        shutil.copy(file, dst)\n",
        "    print(f\"\u2705 Copied {len(sample_files)} files from {folder}\")\n",
        "\n",
        "# Zip the subset\n",
        "zip_path = \"/kaggle/working/SOCOFing_subset.zip\"\n",
        "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, _, files in os.walk(\"/kaggle/working/SOCOFing_subset\"):\n",
        "        for file in files:\n",
        "            full_path = os.path.join(root, file)\n",
        "            zipf.write(full_path, os.path.relpath(full_path, \"/kaggle/working\"))\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf Subset created and zipped at: {zip_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "0vnJdSx2DkMU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dCMaJ5jjDwZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NYPShtjODwWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t7VL96I_DwUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MXf5kyy7DwSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cL8fjJECDwQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run this in Colab"
      ],
      "metadata": {
        "id": "xH9ciYVuDxAv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YSYR1xsqDyXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_EUM8vNDk-V"
      },
      "source": [
        "\ud83d\udcd8 1\ufe0f\u20e3 Notebook Introduction & Setup Instructions\n",
        "\n",
        "# \ud83d\udd90\ufe0f Fingerprint AI Web App \u2014 Classification + Restoration (Colab Version)\n",
        "\n",
        "This notebook loads the *trained models* generated from the Kaggle training notebook:\n",
        "\n",
        "- **fingerprint_classifier.pth** (ResNet50)\n",
        "- **generator_best.pth** (Pix2Pix GAN)\n",
        "- **generator_final.pth** (fallback)\n",
        "\n",
        "You will upload these model files to your Google Drive before running the app.\n",
        "\n",
        "### \ud83d\udd27 What this Colab Notebook Does\n",
        "\u2022 Loads classification & restoration models  \n",
        "\u2022 Builds a complete Flask web application  \n",
        "\u2022 Provides endpoints for:\n",
        "   - Fingerprint Classification  \n",
        "   - Altered Fingerprint Restoration (GAN)  \n",
        "\u2022 Launches public URL using ngrok  \n",
        "\n",
        "### \ud83d\udcc1 Required Folder Structure in Google Drive\n",
        "Inside your Google Drive, create:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XIXLLc5Dk-c"
      },
      "source": [
        "\ud83e\udde9 2\ufe0f\u20e3 Install Dependencies + Create Folders\n",
        "\n",
        "## \ud83e\udde9 Install Required Libraries & Prepare Workspace\n",
        "\n",
        "This cell installs all required dependencies for:\n",
        "- Flask web server\n",
        "- Torch + torchvision\n",
        "- Image processing (Pillow, OpenCV)\n",
        "- ngrok tunneling for public URL\n",
        "\n",
        "It also creates the folders:\n",
        "- templates/\n",
        "- static/\n",
        "- uploads/\n",
        "\n",
        "These are needed for the app structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqwwQOCBYZQP"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 1\ufe0f\u20e3 Install Dependencies\n",
        "# ===============================\n",
        "!pip install -q flask torch torchvision pillow opencv-python-headless pyngrok --upgrade\n",
        "!mkdir -p templates static uploads\n",
        "print(\"\u2705 Dependencies installed & folders created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKx-N9orDk-f"
      },
      "source": [
        "\ud83d\udd17 3\ufe0f\u20e3 Mount Google Drive & Verify Model Files\n",
        "\n",
        "## \ud83d\udd17 Mount Google Drive & Confirm Model Availability\n",
        "\n",
        "Before proceeding, make sure the trained models are already inside your Drive folder:\n",
        "\n",
        "Fingerprint Classification & Restoration/\n",
        "  \u251c\u2500\u2500 fingerprint_classifier.pth\n",
        "  \u251c\u2500\u2500 generator_best.pth\n",
        "  \u2514\u2500\u2500 generator_final.pth\n",
        "\n",
        "The following code:\n",
        "- Mounts your Google Drive\n",
        "- Lists and confirms model file availability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWYVvkHKS4Ls"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdznCQI-WqOP"
      },
      "outputs": [],
      "source": [
        "!ls \"/content/drive/My Drive/Colab Notebooks/On-Going Projects/Fingerprints Project-83/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0koEaDfWPkh"
      },
      "outputs": [],
      "source": [
        "!ls \"/content/drive/My Drive/Colab Notebooks/On-Going Projects/Fingerprints Project-83/Fingerprint Classification & Restoration/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R0aXIMmDk-i"
      },
      "source": [
        "\ud83c\udfd7\ufe0f 4\ufe0f\u20e3 Creating the Flask App (app.py)\n",
        "\n",
        "## \ud83c\udfd7\ufe0f Building Flask Backend (app.py)\n",
        "\n",
        "This cell creates the entire Flask backend including:\n",
        "\n",
        "- Upload API\n",
        "- Classification route\n",
        "- Restoration route\n",
        "- Model loading functions\n",
        "- Image preprocessing (matching Kaggle training)\n",
        "- Saving restored fingerprint outputs\n",
        "\n",
        "\u26a0\ufe0f VERY IMPORTANT:\n",
        "Do NOT modify the architecture of:\n",
        "- ResNet50 classifier\n",
        "- UNet generator\n",
        "\n",
        "They must stay identical to Kaggle models.\n",
        "\n",
        "Paste the following code into app.py using `%%writefile`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWpCwye3Dk-i"
      },
      "source": [
        "\ud83e\udde0 5\ufe0f\u20e3 Load Models (Classifier + GAN Generator)\n",
        "\n",
        "## \ud83e\udde0 Loading Pretrained Models\n",
        "\n",
        "This section loads the models you trained in your Kaggle notebook.\n",
        "\n",
        "\u2714 ResNet50 (real vs altered classification)  \n",
        "\u2714 Pix2Pix U-Net Generator (altered \u2192 restored)  \n",
        "\n",
        "These paths point to your Google Drive folder.  \n",
        "If generator_best fails, the notebook automatically loads generator_final.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1MhdaHYYcUU"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Flask App Setup \u2014 Fixed Architecture & Image Processing\n",
        "# ===============================\n",
        "%%writefile app.py\n",
        "import os, uuid, torch, numpy as np\n",
        "from flask import Flask, render_template, request, redirect, url_for, send_from_directory, flash, session\n",
        "from werkzeug.utils import secure_filename\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "import torch.nn as nn\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.secret_key = \"fingerprint_ai_\" + str(uuid.uuid4())[:8]\n",
        "app.config[\"UPLOAD_FOLDER\"] = \"uploads\"\n",
        "os.makedirs(app.config[\"UPLOAD_FOLDER\"], exist_ok=True)\n",
        "app.config[\"MAX_CONTENT_LENGTH\"] = 50 * 1024 * 1024\n",
        "\n",
        "def get_device():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device = get_device()\n",
        "print(\"\ud83d\udd39 Using device:\", device)\n",
        "\n",
        "def read_image(path, mode=\"RGB\"):\n",
        "    return Image.open(path).convert(mode)\n",
        "\n",
        "def save_image(tensor, path):\n",
        "    \"\"\"Fixed image saving to match Kaggle output\"\"\"\n",
        "    img = tensor.detach().cpu().squeeze(0).numpy()\n",
        "\n",
        "    # Handle single channel grayscale (from GAN)\n",
        "    if img.ndim == 3 and img.shape[0] == 1:\n",
        "        img = img[0]  # Extract the single channel\n",
        "\n",
        "    # Denormalize from [-1, 1] to [0, 1]\n",
        "    img = (img * 0.5 + 0.5)\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "    # Convert to uint8 [0, 255]\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "\n",
        "    # Save as grayscale\n",
        "    Image.fromarray(img, mode='L').save(path)\n",
        "\n",
        "BASE_DIR = \"/content/drive/My Drive/Colab Notebooks/On-Going Projects/Fingerprints Project-83/Fingerprint Classification & Restoration\"\n",
        "CLASSIFIER_PATH = os.path.join(BASE_DIR, \"fingerprint_classifier.pth\")\n",
        "GEN_BEST_PATH = os.path.join(BASE_DIR, \"generator_best.pth\")\n",
        "GEN_FINAL_PATH = os.path.join(BASE_DIR, \"generator_final.pth\")\n",
        "\n",
        "# -----------------------------\n",
        "# 1\ufe0f\u20e3 Classifier \u2014 ResNet50 (MATCHING KAGGLE)\n",
        "# -----------------------------\n",
        "def load_classifier():\n",
        "    \"\"\"Load ResNet50 classifier exactly as trained in Kaggle\"\"\"\n",
        "    model = models.resnet50(weights=None)  # No pretrained weights\n",
        "    model.fc = nn.Linear(model.fc.in_features, 2)  # Binary classification\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Load state dict\n",
        "    state_dict = torch.load(CLASSIFIER_PATH, map_location=device)\n",
        "    model.load_state_dict(state_dict, strict=True)\n",
        "    model.eval()\n",
        "    print(\"\u2705 Loaded ResNet50 classifier successfully.\")\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# 2\ufe0f\u20e3 Pix2Pix UNet Generator (exact Kaggle version)\n",
        "# -----------------------------\n",
        "class UNetGenerator(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, features=64):\n",
        "        super().__init__()\n",
        "        self.down1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features, 4, 2, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down2 = nn.Sequential(\n",
        "            nn.Conv2d(features, features*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*2),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down3 = nn.Sequential(\n",
        "            nn.Conv2d(features*2, features*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*4),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down4 = nn.Sequential(\n",
        "            nn.Conv2d(features*4, features*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down5 = nn.Sequential(\n",
        "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down6 = nn.Sequential(\n",
        "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down7 = nn.Sequential(\n",
        "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down8 = nn.Sequential(\n",
        "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        def up(in_ch, out_ch, dropout=False):\n",
        "            layers = [nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1, bias=False),\n",
        "                      nn.BatchNorm2d(out_ch),\n",
        "                      nn.ReLU(inplace=True)]\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout(0.5))\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        self.up1 = up(features*8, features*8, dropout=True)\n",
        "        self.up2 = up(features*16, features*8, dropout=True)\n",
        "        self.up3 = up(features*16, features*8, dropout=True)\n",
        "        self.up4 = up(features*16, features*8)\n",
        "        self.up5 = up(features*16, features*4)\n",
        "        self.up6 = up(features*8, features*2)\n",
        "        self.up7 = up(features*4, features)\n",
        "        self.up8 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(features*2, out_channels, 4, 2, 1, bias=True),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.down1(x)\n",
        "        e2 = self.down2(e1)\n",
        "        e3 = self.down3(e2)\n",
        "        e4 = self.down4(e3)\n",
        "        e5 = self.down5(e4)\n",
        "        e6 = self.down6(e5)\n",
        "        e7 = self.down7(e6)\n",
        "        e8 = self.down8(e7)\n",
        "        d1 = self.up1(e8); d1 = torch.cat([d1, e7], dim=1)\n",
        "        d2 = self.up2(d1); d2 = torch.cat([d2, e6], dim=1)\n",
        "        d3 = self.up3(d2); d3 = torch.cat([d3, e5], dim=1)\n",
        "        d4 = self.up4(d3); d4 = torch.cat([d4, e4], dim=1)\n",
        "        d5 = self.up5(d4); d5 = torch.cat([d5, e3], dim=1)\n",
        "        d6 = self.up6(d5); d6 = torch.cat([d6, e2], dim=1)\n",
        "        d7 = self.up7(d6); d7 = torch.cat([d7, e1], dim=1)\n",
        "        return self.up8(d7)\n",
        "\n",
        "# -----------------------------\n",
        "# Load Models\n",
        "# -----------------------------\n",
        "def load_models():\n",
        "    print(\"\ud83d\udce6 Loading models...\")\n",
        "\n",
        "    classifier = load_classifier()\n",
        "\n",
        "    generator = UNetGenerator(in_channels=1, out_channels=1).to(device)\n",
        "    try:\n",
        "        generator.load_state_dict(torch.load(GEN_BEST_PATH, map_location=device), strict=True)\n",
        "        print(\"\u2705 Loaded generator_best.pth\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f Error loading best: {e}\")\n",
        "        generator.load_state_dict(torch.load(GEN_FINAL_PATH, map_location=device), strict=True)\n",
        "        print(\"\u26a0\ufe0f Loaded generator_final.pth instead\")\n",
        "\n",
        "    generator.eval()\n",
        "    return classifier, generator\n",
        "\n",
        "classifier, generator = load_models()\n",
        "\n",
        "# ## \ud83d\udd0d Prediction Functions\n",
        "\n",
        "# These functions:\n",
        "\n",
        "# 1. `classify_fingerprint()`\n",
        "#    - Performs Kaggle-matched preprocessing: resize \u2192 normalize \u2192 RGB \u2192 ResNet50\n",
        "#    - Returns label + confidence score\n",
        "\n",
        "# 2. `restore_fingerprint()`\n",
        "#    - Converts altered fingerprint to grayscale\n",
        "#    - Normalizes to [-1, 1]\n",
        "#    - Runs through Pix2Pix generator\n",
        "#    - Returns restored fingerprint tensor\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Prediction Functions (FIXED TRANSFORMS)\n",
        "# -----------------------------\n",
        "def classify_fingerprint(image_path):\n",
        "    \"\"\"Classification with exact Kaggle preprocessing\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    # Load and convert to RGB (as in Kaggle training)\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    x = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = classifier(x)\n",
        "        probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
        "\n",
        "    label = \"REAL\" if np.argmax(probs) == 0 else \"ALTERED\"\n",
        "    conf = float(np.max(probs))\n",
        "    return label, conf\n",
        "\n",
        "def restore_fingerprint(image_path):\n",
        "    \"\"\"Restoration with exact Kaggle preprocessing\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    # Load as grayscale (as in Kaggle)\n",
        "    image = Image.open(image_path).convert(\"L\")\n",
        "    x = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        restored = generator(x)\n",
        "\n",
        "    return restored\n",
        "\n",
        "# -----------------------------\n",
        "# Routes\n",
        "# -----------------------------\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/classify\", methods=[\"GET\", \"POST\"])\n",
        "def classify_page():\n",
        "    result = None\n",
        "    filename = None\n",
        "\n",
        "    if request.method == \"POST\":\n",
        "        f = request.files.get(\"image\")\n",
        "        if not f or not f.filename:\n",
        "            flash(\"Please upload a fingerprint image.\", \"warning\")\n",
        "            return redirect(url_for(\"classify_page\"))\n",
        "\n",
        "        # Save uploaded file\n",
        "        fname = f\"{uuid.uuid4().hex}_{secure_filename(f.filename)}\"\n",
        "        path = os.path.join(app.config[\"UPLOAD_FOLDER\"], fname)\n",
        "        f.save(path)\n",
        "\n",
        "        # Classify\n",
        "        label, conf = classify_fingerprint(path)\n",
        "        result = {\"label\": label, \"confidence\": conf}\n",
        "        filename = fname\n",
        "\n",
        "        # Store in session for restoration\n",
        "        session[\"last_uploaded\"] = path\n",
        "        if label == \"ALTERED\":\n",
        "            session[\"last_altered\"] = path\n",
        "\n",
        "    return render_template(\"classify.html\", result=result, filename=filename)\n",
        "\n",
        "@app.route(\"/restore\", methods=[\"GET\", \"POST\"])\n",
        "def restore_page():\n",
        "    if request.method == \"POST\":\n",
        "        # Handle direct upload for restoration\n",
        "        f = request.files.get(\"image\")\n",
        "        if not f or not f.filename:\n",
        "            flash(\"Please upload a fingerprint image.\", \"warning\")\n",
        "            return redirect(url_for(\"restore_page\"))\n",
        "\n",
        "        fname = f\"{uuid.uuid4().hex}_{secure_filename(f.filename)}\"\n",
        "        altered_path = os.path.join(app.config[\"UPLOAD_FOLDER\"], fname)\n",
        "        f.save(altered_path)\n",
        "        session[\"last_altered\"] = altered_path\n",
        "    else:\n",
        "        # Use the last altered image from classification\n",
        "        altered_path = session.get(\"last_altered\", None)\n",
        "\n",
        "    if not altered_path or not os.path.exists(altered_path):\n",
        "        flash(\"No altered fingerprint found. Please upload one or classify first.\", \"warning\")\n",
        "        return render_template(\"restore.html\", altered_fname=None, restored_fname=None)\n",
        "\n",
        "    # Perform restoration\n",
        "    restored_tensor = restore_fingerprint(altered_path)\n",
        "    restored_fname = \"restored_\" + os.path.basename(altered_path)\n",
        "    restored_path = os.path.join(app.config[\"UPLOAD_FOLDER\"], restored_fname)\n",
        "    save_image(restored_tensor, restored_path)\n",
        "\n",
        "    return render_template(\"restore.html\",\n",
        "                         altered_fname=os.path.basename(altered_path),\n",
        "                         restored_fname=os.path.basename(restored_path))\n",
        "\n",
        "@app.route(\"/uploads/<path:fname>\")\n",
        "def uploads(fname):\n",
        "    return send_from_directory(app.config[\"UPLOAD_FOLDER\"], fname)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\ud83d\ude80 Starting Fingerprint AI Web Server...\")\n",
        "    app.run(host=\"0.0.0.0\", port=8000, debug=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otaTHAzkDk-k"
      },
      "source": [
        "\ud83c\udf10 7\ufe0f\u20e3 Templates (HTML files)\n",
        "\n",
        "## \ud83c\udf10 Web App Frontend (HTML Templates)\n",
        "\n",
        "The following cells create:\n",
        "- base.html (global layout)\n",
        "- index.html (homepage)\n",
        "- classify.html (classification UI)\n",
        "- restore.html (restoration UI)\n",
        "\n",
        "Paste each template exactly into its respective file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK8HmUmje-fU"
      },
      "outputs": [],
      "source": [
        "%%writefile templates/base.html\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "  <meta charset=\"UTF-8\">\n",
        "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "  <title>{% block title %}Fingerprint AI{% endblock %}</title>\n",
        "  <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\">\n",
        "  <link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n",
        "  <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap\" rel=\"stylesheet\">\n",
        "</head>\n",
        "<body>\n",
        "  <!-- Animated Background -->\n",
        "  <div class=\"bg-animation\">\n",
        "    <div class=\"fingerprint-pattern\"></div>\n",
        "    <div class=\"gradient-orb orb-1\"></div>\n",
        "    <div class=\"gradient-orb orb-2\"></div>\n",
        "    <div class=\"gradient-orb orb-3\"></div>\n",
        "  </div>\n",
        "\n",
        "  <!-- Navigation -->\n",
        "  <nav class=\"navbar\">\n",
        "    <div class=\"nav-container\">\n",
        "      <a href=\"{{ url_for('home') }}\" class=\"logo\">\n",
        "        <span class=\"logo-icon\">\ud83d\udd2c</span>\n",
        "        <span class=\"logo-text\">Fingerprint<span class=\"ai\">AI</span></span>\n",
        "      </a>\n",
        "      <div class=\"nav-links\">\n",
        "        <a href=\"{{ url_for('home') }}\" class=\"nav-link {% if request.endpoint == 'home' %}active{% endif %}\">\n",
        "          <span class=\"nav-icon\">\ud83c\udfe0</span> Home\n",
        "        </a>\n",
        "        <a href=\"{{ url_for('classify_page') }}\" class=\"nav-link {% if request.endpoint == 'classify_page' %}active{% endif %}\">\n",
        "          <span class=\"nav-icon\">\ud83d\udd0d</span> Classify\n",
        "        </a>\n",
        "        <a href=\"{{ url_for('restore_page') }}\" class=\"nav-link {% if request.endpoint == 'restore_page' %}active{% endif %}\">\n",
        "          <span class=\"nav-icon\">\u2728</span> Restore\n",
        "        </a>\n",
        "      </div>\n",
        "    </div>\n",
        "  </nav>\n",
        "\n",
        "  <!-- Flash Messages -->\n",
        "  {% with messages = get_flashed_messages(with_categories=true) %}\n",
        "    {% if messages %}\n",
        "      <div class=\"flash-container\">\n",
        "        {% for category, message in messages %}\n",
        "          <div class=\"flash-message flash-{{ category }}\">\n",
        "            <span class=\"flash-icon\">{% if category == 'warning' %}\u26a0\ufe0f{% else %}\u2139\ufe0f{% endif %}</span>\n",
        "            <span>{{ message }}</span>\n",
        "            <button class=\"flash-close\" onclick=\"this.parentElement.remove()\">\u2715</button>\n",
        "          </div>\n",
        "        {% endfor %}\n",
        "      </div>\n",
        "    {% endif %}\n",
        "  {% endwith %}\n",
        "\n",
        "  <!-- Main Content -->\n",
        "  <main class=\"main-container\">\n",
        "    {% block content %}{% endblock %}\n",
        "  </main>\n",
        "\n",
        "  <!-- Footer -->\n",
        "  <footer class=\"footer\">\n",
        "    <div class=\"footer-content\">\n",
        "      <p class=\"footer-text\">\n",
        "        <span class=\"footer-icon\">\ud83e\uddec</span>\n",
        "        Developed by <strong>Tarun</strong> | Fingerprint Forensics AI\n",
        "      </p>\n",
        "      <p class=\"footer-subtext\">Powered by ResNet50 & Pix2Pix GAN</p>\n",
        "    </div>\n",
        "  </footer>\n",
        "\n",
        "  <script>\n",
        "    // File upload drag & drop enhancement\n",
        "    document.addEventListener('DOMContentLoaded', function() {\n",
        "      const uploadZones = document.querySelectorAll('.upload-zone');\n",
        "\n",
        "      uploadZones.forEach(zone => {\n",
        "        const input = zone.querySelector('input[type=\"file\"]');\n",
        "        const preview = zone.querySelector('.upload-preview');\n",
        "\n",
        "        ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n",
        "          zone.addEventListener(eventName, preventDefaults, false);\n",
        "        });\n",
        "\n",
        "        function preventDefaults(e) {\n",
        "          e.preventDefault();\n",
        "          e.stopPropagation();\n",
        "        }\n",
        "\n",
        "        ['dragenter', 'dragover'].forEach(eventName => {\n",
        "          zone.addEventListener(eventName, () => zone.classList.add('drag-active'), false);\n",
        "        });\n",
        "\n",
        "        ['dragleave', 'drop'].forEach(eventName => {\n",
        "          zone.addEventListener(eventName, () => zone.classList.remove('drag-active'), false);\n",
        "        });\n",
        "\n",
        "        if (input) {\n",
        "          input.addEventListener('change', function(e) {\n",
        "            if (this.files && this.files[0]) {\n",
        "              const reader = new FileReader();\n",
        "              reader.onload = function(e) {\n",
        "                if (preview) {\n",
        "                  preview.innerHTML = `<img src=\"${e.target.result}\" alt=\"Preview\" style=\"max-width: 100%; border-radius: 8px;\">`;\n",
        "                }\n",
        "              };\n",
        "              reader.readAsDataURL(this.files[0]);\n",
        "\n",
        "              // Update upload text\n",
        "              const uploadText = zone.querySelector('.upload-text');\n",
        "              if (uploadText) {\n",
        "                uploadText.textContent = `Selected: ${this.files[0].name}`;\n",
        "              }\n",
        "            }\n",
        "          });\n",
        "        }\n",
        "      });\n",
        "\n",
        "      // Auto-hide flash messages after 5 seconds\n",
        "      setTimeout(() => {\n",
        "        document.querySelectorAll('.flash-message').forEach(msg => {\n",
        "          msg.style.animation = 'slideOut 0.3s ease';\n",
        "          setTimeout(() => msg.remove(), 300);\n",
        "        });\n",
        "      }, 5000);\n",
        "    });\n",
        "  </script>\n",
        "</body>\n",
        "</html>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbzgGs7ZYcQ5"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 3\ufe0f\u20e3 Templates \u2014 index.html\n",
        "# ===============================\n",
        "%%writefile templates/index.html\n",
        "{% extends \"base.html\" %}\n",
        "\n",
        "{% block title %}Home - Fingerprint AI{% endblock %}\n",
        "\n",
        "{% block content %}\n",
        "<div class=\"hero-section\">\n",
        "  <div class=\"hero-content\">\n",
        "    <div class=\"hero-badge\">\n",
        "      <span class=\"badge-dot\"></span>\n",
        "      AI-Powered Forensic Analysis\n",
        "    </div>\n",
        "\n",
        "    <h1 class=\"hero-title\">\n",
        "      Fingerprint Detection &\n",
        "      <span class=\"gradient-text\">Restoration</span>\n",
        "    </h1>\n",
        "\n",
        "    <p class=\"hero-subtitle\">\n",
        "      Advanced deep learning models for forensic fingerprint analysis.\n",
        "      Detect alterations with <strong>99.96% accuracy</strong> and restore\n",
        "      damaged prints using state-of-the-art Pix2Pix GAN technology.\n",
        "    </p>\n",
        "\n",
        "    <div class=\"hero-buttons\">\n",
        "      <a href=\"{{ url_for('classify_page') }}\" class=\"btn btn-primary\">\n",
        "        <span class=\"btn-icon\">\ud83d\udd0d</span>\n",
        "        Start Classification\n",
        "        <span class=\"btn-arrow\">\u2192</span>\n",
        "      </a>\n",
        "      <a href=\"{{ url_for('restore_page') }}\" class=\"btn btn-secondary\">\n",
        "        <span class=\"btn-icon\">\u2728</span>\n",
        "        Restore Fingerprint\n",
        "      </a>\n",
        "    </div>\n",
        "  </div>\n",
        "\n",
        "  <div class=\"hero-visual\">\n",
        "    <div class=\"fingerprint-showcase\">\n",
        "      <div class=\"showcase-card card-1\">\n",
        "        <div class=\"card-icon\">\ud83d\udd90\ufe0f</div>\n",
        "        <h3>Real Fingerprint</h3>\n",
        "        <p>Authentic ridge patterns</p>\n",
        "      </div>\n",
        "      <div class=\"showcase-arrow\">\u2192</div>\n",
        "      <div class=\"showcase-card card-2\">\n",
        "        <div class=\"card-icon\">\u26a0\ufe0f</div>\n",
        "        <h3>Altered Print</h3>\n",
        "        <p>Detected modifications</p>\n",
        "      </div>\n",
        "      <div class=\"showcase-arrow\">\u2192</div>\n",
        "      <div class=\"showcase-card card-3\">\n",
        "        <div class=\"card-icon\">\u2705</div>\n",
        "        <h3>Restored Print</h3>\n",
        "        <p>AI reconstruction</p>\n",
        "      </div>\n",
        "    </div>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<div class=\"features-section\">\n",
        "  <h2 class=\"section-title\">\n",
        "    <span class=\"title-icon\">\u26a1</span>\n",
        "    Key Features\n",
        "  </h2>\n",
        "\n",
        "  <div class=\"features-grid\">\n",
        "    <div class=\"feature-card\">\n",
        "      <div class=\"feature-icon\">\ud83c\udfaf</div>\n",
        "      <h3>High Accuracy</h3>\n",
        "      <p>ResNet50-based classifier achieving 99.96% test accuracy on altered fingerprint detection</p>\n",
        "      <div class=\"feature-stat\">99.96%</div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"feature-card\">\n",
        "      <div class=\"feature-icon\">\ud83e\udde0</div>\n",
        "      <h3>Deep Learning</h3>\n",
        "      <p>Powered by advanced neural networks trained on thousands of forensic fingerprint samples</p>\n",
        "      <div class=\"feature-stat\">50K+</div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"feature-card\">\n",
        "      <div class=\"feature-icon\">\u26a1</div>\n",
        "      <h3>Fast Processing</h3>\n",
        "      <p>Real-time classification and restoration with GPU acceleration for instant results</p>\n",
        "      <div class=\"feature-stat\">&lt;3s</div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"feature-card\">\n",
        "      <div class=\"feature-icon\">\ud83d\udd2c</div>\n",
        "      <h3>Forensic Grade</h3>\n",
        "      <p>Meets forensic analysis standards with quantitative metrics (PSNR, SSIM, MSE)</p>\n",
        "      <div class=\"feature-stat\">0.943</div>\n",
        "    </div>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<div class=\"workflow-section\">\n",
        "  <h2 class=\"section-title\">\n",
        "    <span class=\"title-icon\">\ud83d\udd04</span>\n",
        "    How It Works\n",
        "  </h2>\n",
        "\n",
        "  <div class=\"workflow-steps\">\n",
        "    <div class=\"workflow-step\">\n",
        "      <div class=\"step-number\">1</div>\n",
        "      <div class=\"step-content\">\n",
        "        <h3>Upload Fingerprint</h3>\n",
        "        <p>Upload your fingerprint image (BMP, PNG, JPG formats supported)</p>\n",
        "      </div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"workflow-connector\"></div>\n",
        "\n",
        "    <div class=\"workflow-step\">\n",
        "      <div class=\"step-number\">2</div>\n",
        "      <div class=\"step-content\">\n",
        "        <h3>AI Analysis</h3>\n",
        "        <p>ResNet50 classifier detects if the print is real or altered</p>\n",
        "      </div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"workflow-connector\"></div>\n",
        "\n",
        "    <div class=\"workflow-step\">\n",
        "      <div class=\"step-number\">3</div>\n",
        "      <div class=\"step-content\">\n",
        "        <h3>Restoration (Optional)</h3>\n",
        "        <p>Pix2Pix GAN reconstructs altered fingerprints to original state</p>\n",
        "      </div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"workflow-connector\"></div>\n",
        "\n",
        "    <div class=\"workflow-step\">\n",
        "      <div class=\"step-number\">4</div>\n",
        "      <div class=\"step-content\">\n",
        "        <h3>Download Results</h3>\n",
        "        <p>View and download classification results and restored images</p>\n",
        "      </div>\n",
        "    </div>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<div class=\"cta-section\">\n",
        "  <div class=\"cta-content\">\n",
        "    <h2>Ready to Analyze Your Fingerprints?</h2>\n",
        "    <p>Experience the power of AI-driven forensic analysis</p>\n",
        "    <a href=\"{{ url_for('classify_page') }}\" class=\"btn btn-primary btn-large\">\n",
        "      <span class=\"btn-icon\">\ud83d\ude80</span>\n",
        "      Get Started Now\n",
        "      <span class=\"btn-arrow\">\u2192</span>\n",
        "    </a>\n",
        "  </div>\n",
        "</div>\n",
        "{% endblock %}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ9DC3kNYcMh"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 4\ufe0f\u20e3 Templates \u2014 classify.html\n",
        "# ===============================\n",
        "%%writefile templates/classify.html\n",
        "{% extends \"base.html\" %}\n",
        "\n",
        "{% block title %}Classify - Fingerprint AI{% endblock %}\n",
        "\n",
        "{% block content %}\n",
        "<div class=\"page-container\">\n",
        "  <div class=\"page-header\">\n",
        "    <div class=\"page-icon\">\ud83d\udd0d</div>\n",
        "    <h1 class=\"page-title\">Fingerprint Classification</h1>\n",
        "    <p class=\"page-description\">\n",
        "      Upload a fingerprint image to detect if it's <strong>Real</strong> or <strong>Altered</strong>\n",
        "    </p>\n",
        "  </div>\n",
        "\n",
        "  <div class=\"content-grid\">\n",
        "    <!-- Upload Section -->\n",
        "    <div class=\"upload-section\">\n",
        "      <form method=\"post\" enctype=\"multipart/form-data\" class=\"upload-form\">\n",
        "        <div class=\"upload-zone\" id=\"uploadZone\">\n",
        "          <input type=\"file\" name=\"image\" accept=\"image/*\" required id=\"fileInput\">\n",
        "          <div class=\"upload-content\">\n",
        "            <div class=\"upload-icon-large\">\ud83d\udd90\ufe0f</div>\n",
        "            <h3 class=\"upload-title\">Drop your fingerprint here</h3>\n",
        "            <p class=\"upload-text\">or click to browse</p>\n",
        "            <p class=\"upload-hint\">Supports BMP, PNG, JPG formats</p>\n",
        "          </div>\n",
        "          <div class=\"upload-preview\"></div>\n",
        "        </div>\n",
        "\n",
        "        <button type=\"submit\" class=\"btn btn-primary btn-block\">\n",
        "          <span class=\"btn-icon\">\ud83d\udd2c</span>\n",
        "          Analyze Fingerprint\n",
        "          <span class=\"btn-arrow\">\u2192</span>\n",
        "        </button>\n",
        "      </form>\n",
        "    </div>\n",
        "\n",
        "    <!-- Results Section -->\n",
        "    {% if result %}\n",
        "    <div class=\"results-section\">\n",
        "      <!-- Preview Image -->\n",
        "      {% if filename %}\n",
        "      <div class=\"preview-card\">\n",
        "        <h3 class=\"card-title\">\n",
        "          <span class=\"card-icon\">\ud83d\udcf8</span>\n",
        "          Uploaded Image\n",
        "        </h3>\n",
        "        <div class=\"image-container\">\n",
        "          <img src=\"{{ url_for('uploads', fname=filename) }}\" alt=\"Fingerprint\" class=\"preview-image\">\n",
        "        </div>\n",
        "      </div>\n",
        "      {% endif %}\n",
        "\n",
        "      <!-- Classification Result -->\n",
        "      <div class=\"result-card\">\n",
        "        <h3 class=\"card-title\">\n",
        "          <span class=\"card-icon\">\ud83c\udfaf</span>\n",
        "          Classification Result\n",
        "        </h3>\n",
        "\n",
        "        <div class=\"result-display\">\n",
        "          <div class=\"result-badge {{ 'badge-real' if result.label == 'REAL' else 'badge-altered' }}\">\n",
        "            <span class=\"badge-icon\">{{ '\u2705' if result.label == 'REAL' else '\u26a0\ufe0f' }}</span>\n",
        "            <span class=\"badge-label\">{{ result.label }}</span>\n",
        "          </div>\n",
        "\n",
        "          <div class=\"confidence-meter\">\n",
        "            <div class=\"confidence-label\">\n",
        "              <span>Confidence</span>\n",
        "              <span class=\"confidence-value\">{{ (result.confidence * 100)|round(2) }}%</span>\n",
        "            </div>\n",
        "            <div class=\"confidence-bar\">\n",
        "              <div class=\"confidence-fill {{ 'fill-real' if result.label == 'REAL' else 'fill-altered' }}\"\n",
        "                   style=\"width: {{ (result.confidence * 100)|round(2) }}%\"></div>\n",
        "            </div>\n",
        "          </div>\n",
        "\n",
        "          <div class=\"result-description\">\n",
        "            {% if result.label == 'REAL' %}\n",
        "              <p class=\"desc-icon\">\u2713</p>\n",
        "              <p class=\"desc-text\">\n",
        "                This fingerprint appears to be <strong>authentic</strong> with no detected alterations.\n",
        "                The ridge patterns match expected characteristics of genuine prints.\n",
        "              </p>\n",
        "            {% else %}\n",
        "              <p class=\"desc-icon\">!</p>\n",
        "              <p class=\"desc-text\">\n",
        "                This fingerprint shows signs of <strong>alteration</strong>. Detected modifications\n",
        "                may include scars, burns, abrasions, or other intentional changes.\n",
        "              </p>\n",
        "            {% endif %}\n",
        "          </div>\n",
        "\n",
        "          {% if result.label == 'ALTERED' %}\n",
        "          <div class=\"action-buttons\">\n",
        "            <a href=\"{{ url_for('restore_page') }}\" class=\"btn btn-success btn-block\">\n",
        "              <span class=\"btn-icon\">\u2728</span>\n",
        "              Restore This Fingerprint\n",
        "              <span class=\"btn-arrow\">\u2192</span>\n",
        "            </a>\n",
        "          </div>\n",
        "          {% endif %}\n",
        "        </div>\n",
        "      </div>\n",
        "\n",
        "      <!-- Technical Details -->\n",
        "      <div class=\"details-card\">\n",
        "        <h3 class=\"card-title\">\n",
        "          <span class=\"card-icon\">\ud83d\udcca</span>\n",
        "          Technical Details\n",
        "        </h3>\n",
        "        <div class=\"details-grid\">\n",
        "          <div class=\"detail-item\">\n",
        "            <span class=\"detail-label\">Model</span>\n",
        "            <span class=\"detail-value\">ResNet50</span>\n",
        "          </div>\n",
        "          <div class=\"detail-item\">\n",
        "            <span class=\"detail-label\">Classes</span>\n",
        "            <span class=\"detail-value\">Real / Altered</span>\n",
        "          </div>\n",
        "          <div class=\"detail-item\">\n",
        "            <span class=\"detail-label\">Accuracy</span>\n",
        "            <span class=\"detail-value\">99.96%</span>\n",
        "          </div>\n",
        "          <div class=\"detail-item\">\n",
        "            <span class=\"detail-label\">Input Size</span>\n",
        "            <span class=\"detail-value\">224\u00d7224</span>\n",
        "          </div>\n",
        "        </div>\n",
        "      </div>\n",
        "    </div>\n",
        "    {% else %}\n",
        "    <!-- Instructions (shown when no result) -->\n",
        "    <div class=\"instructions-card\">\n",
        "      <div class=\"instruction-icon\">\ud83d\udca1</div>\n",
        "      <h3>Instructions</h3>\n",
        "      <ul class=\"instruction-list\">\n",
        "        <li><strong>Step 1:</strong> Upload a fingerprint image using the form on the left</li>\n",
        "        <li><strong>Step 2:</strong> Click \"Analyze Fingerprint\" to run the classification</li>\n",
        "        <li><strong>Step 3:</strong> View the results showing if the print is Real or Altered</li>\n",
        "        <li><strong>Step 4:</strong> If altered, click \"Restore\" to reconstruct the original print</li>\n",
        "      </ul>\n",
        "\n",
        "      <div class=\"info-box\">\n",
        "        <span class=\"info-icon\">\u2139\ufe0f</span>\n",
        "        <p>\n",
        "          Our AI model has been trained on the SOCOFing dataset with over 50,000 fingerprint samples,\n",
        "          achieving state-of-the-art accuracy in detecting alterations.\n",
        "        </p>\n",
        "      </div>\n",
        "    </div>\n",
        "    {% endif %}\n",
        "  </div>\n",
        "</div>\n",
        "{% endblock %}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tb4yFaUvYcKW"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 5\ufe0f\u20e3 Templates \u2014 restore.html (Auto-display version)\n",
        "# ===============================\n",
        "%%writefile templates/restore.html\n",
        "{% extends \"base.html\" %}\n",
        "\n",
        "{% block title %}Restore - Fingerprint AI{% endblock %}\n",
        "\n",
        "{% block content %}\n",
        "<div class=\"page-container\">\n",
        "  <div class=\"page-header\">\n",
        "    <div class=\"page-icon\">\u2728</div>\n",
        "    <h1 class=\"page-title\">Fingerprint Restoration</h1>\n",
        "    <p class=\"page-description\">\n",
        "      Reconstruct altered fingerprints using advanced <strong>Pix2Pix GAN</strong> technology\n",
        "    </p>\n",
        "  </div>\n",
        "\n",
        "  {% if not altered_fname and not restored_fname %}\n",
        "  <!-- Upload Option for Direct Restoration -->\n",
        "  <div class=\"content-grid\">\n",
        "    <div class=\"upload-section\">\n",
        "      <div class=\"info-banner\">\n",
        "        <span class=\"banner-icon\">\ud83d\udca1</span>\n",
        "        <p>You can either upload a new altered fingerprint here, or classify one first from the Classification page.</p>\n",
        "      </div>\n",
        "\n",
        "      <form method=\"post\" enctype=\"multipart/form-data\" class=\"upload-form\">\n",
        "        <div class=\"upload-zone\" id=\"uploadZone\">\n",
        "          <input type=\"file\" name=\"image\" accept=\"image/*\" required id=\"fileInput\">\n",
        "          <div class=\"upload-content\">\n",
        "            <div class=\"upload-icon-large\">\ud83d\udd90\ufe0f</div>\n",
        "            <h3 class=\"upload-title\">Drop altered fingerprint here</h3>\n",
        "            <p class=\"upload-text\">or click to browse</p>\n",
        "            <p class=\"upload-hint\">Upload the fingerprint you want to restore</p>\n",
        "          </div>\n",
        "          <div class=\"upload-preview\"></div>\n",
        "        </div>\n",
        "\n",
        "        <button type=\"submit\" class=\"btn btn-success btn-block\">\n",
        "          <span class=\"btn-icon\">\u2728</span>\n",
        "          Restore Fingerprint\n",
        "          <span class=\"btn-arrow\">\u2192</span>\n",
        "        </button>\n",
        "      </form>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"instructions-card\">\n",
        "      <div class=\"instruction-icon\">\ud83d\udd2c</div>\n",
        "      <h3>How Restoration Works</h3>\n",
        "      <div class=\"process-steps\">\n",
        "        <div class=\"process-step\">\n",
        "          <div class=\"step-badge\">1</div>\n",
        "          <div class=\"step-info\">\n",
        "            <h4>Input Analysis</h4>\n",
        "            <p>The U-Net generator analyzes the altered fingerprint's ridge patterns and identifies modifications</p>\n",
        "          </div>\n",
        "        </div>\n",
        "        <div class=\"process-step\">\n",
        "          <div class=\"step-badge\">2</div>\n",
        "          <div class=\"step-info\">\n",
        "            <h4>Pattern Reconstruction</h4>\n",
        "            <p>The GAN reconstructs original ridge structures using learned patterns from thousands of authentic prints</p>\n",
        "          </div>\n",
        "        </div>\n",
        "        <div class=\"process-step\">\n",
        "          <div class=\"step-badge\">3</div>\n",
        "          <div class=\"step-info\">\n",
        "            <h4>Quality Enhancement</h4>\n",
        "            <p>Final output is refined to match forensic standards with high PSNR (~22dB) and SSIM (0.943)</p>\n",
        "          </div>\n",
        "        </div>\n",
        "      </div>\n",
        "\n",
        "      <div class=\"metrics-box\">\n",
        "        <h4>Performance Metrics</h4>\n",
        "        <div class=\"metrics-grid\">\n",
        "          <div class=\"metric-item\">\n",
        "            <span class=\"metric-label\">PSNR</span>\n",
        "            <span class=\"metric-value\">21.89 dB</span>\n",
        "          </div>\n",
        "          <div class=\"metric-item\">\n",
        "            <span class=\"metric-label\">SSIM</span>\n",
        "            <span class=\"metric-value\">0.943</span>\n",
        "          </div>\n",
        "          <div class=\"metric-item\">\n",
        "            <span class=\"metric-label\">MSE</span>\n",
        "            <span class=\"metric-value\">433.62</span>\n",
        "          </div>\n",
        "        </div>\n",
        "      </div>\n",
        "    </div>\n",
        "  </div>\n",
        "  {% endif %}\n",
        "\n",
        "  {% if altered_fname and restored_fname %}\n",
        "  <!-- Restoration Results -->\n",
        "  <div class=\"restoration-results\">\n",
        "    <div class=\"results-header\">\n",
        "      <div class=\"success-badge\">\n",
        "        <span class=\"badge-icon\">\u2705</span>\n",
        "        <span>Restoration Complete</span>\n",
        "      </div>\n",
        "      <p class=\"results-subtitle\">Compare the altered and restored fingerprints below</p>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"comparison-container\">\n",
        "      <!-- Before (Altered) -->\n",
        "      <div class=\"comparison-card card-before\">\n",
        "        <div class=\"card-header\">\n",
        "          <span class=\"card-icon\">\u26a0\ufe0f</span>\n",
        "          <h3>Altered Input</h3>\n",
        "        </div>\n",
        "        <div class=\"image-wrapper\">\n",
        "          <img src=\"{{ url_for('uploads', fname=altered_fname) }}\" alt=\"Altered Fingerprint\" class=\"comparison-image\">\n",
        "          <div class=\"image-label label-before\">BEFORE</div>\n",
        "        </div>\n",
        "        <div class=\"card-footer\">\n",
        "          <p>Original altered fingerprint with modifications</p>\n",
        "        </div>\n",
        "      </div>\n",
        "\n",
        "      <!-- Comparison Arrow -->\n",
        "      <div class=\"comparison-arrow\">\n",
        "        <div class=\"arrow-icon\">\u2192</div>\n",
        "        <div class=\"arrow-label\">AI Restoration</div>\n",
        "      </div>\n",
        "\n",
        "      <!-- After (Restored) -->\n",
        "      <div class=\"comparison-card card-after\">\n",
        "        <div class=\"card-header\">\n",
        "          <span class=\"card-icon\">\u2728</span>\n",
        "          <h3>Restored Output</h3>\n",
        "        </div>\n",
        "        <div class=\"image-wrapper\">\n",
        "          <img src=\"{{ url_for('uploads', fname=restored_fname) }}\" alt=\"Restored Fingerprint\" class=\"comparison-image\">\n",
        "          <div class=\"image-label label-after\">AFTER</div>\n",
        "        </div>\n",
        "        <div class=\"card-footer\">\n",
        "          <p>Reconstructed fingerprint with enhanced ridges</p>\n",
        "        </div>\n",
        "      </div>\n",
        "    </div>\n",
        "\n",
        "    <!-- Analysis Details -->\n",
        "    <div class=\"analysis-section\">\n",
        "      <h3 class=\"section-subtitle\">\n",
        "        <span class=\"subtitle-icon\">\ud83d\udcca</span>\n",
        "        Restoration Analysis\n",
        "      </h3>\n",
        "\n",
        "      <div class=\"analysis-grid\">\n",
        "        <div class=\"analysis-card\">\n",
        "          <div class=\"analysis-icon\">\ud83c\udfaf</div>\n",
        "          <h4>Accuracy</h4>\n",
        "          <p>Ridge patterns reconstructed with high fidelity matching authentic fingerprint structures</p>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"analysis-card\">\n",
        "          <div class=\"analysis-icon\">\ud83d\udd0d</div>\n",
        "          <h4>Quality Metrics</h4>\n",
        "          <p>SSIM score of 0.943 indicates excellent structural similarity to original patterns</p>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"analysis-card\">\n",
        "          <div class=\"analysis-icon\">\u26a1</div>\n",
        "          <h4>Processing Time</h4>\n",
        "          <p>Restoration completed in under 3 seconds using GPU-accelerated inference</p>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"analysis-card\">\n",
        "          <div class=\"analysis-icon\">\ud83d\udd2c</div>\n",
        "          <h4>Model Architecture</h4>\n",
        "          <p>Pix2Pix GAN with U-Net generator trained on 10,000+ fingerprint pairs</p>\n",
        "        </div>\n",
        "      </div>\n",
        "    </div>\n",
        "\n",
        "    <!-- Action Buttons -->\n",
        "    <div class=\"action-section\">\n",
        "      <a href=\"{{ url_for('uploads', fname=restored_fname) }}\" download class=\"btn btn-primary\">\n",
        "        <span class=\"btn-icon\">\ud83d\udcbe</span>\n",
        "        Download Restored Image\n",
        "      </a>\n",
        "      <a href=\"{{ url_for('restore_page') }}\" class=\"btn btn-secondary\">\n",
        "        <span class=\"btn-icon\">\ud83d\udd04</span>\n",
        "        Restore Another\n",
        "      </a>\n",
        "      <a href=\"{{ url_for('classify_page') }}\" class=\"btn btn-outline\">\n",
        "        <span class=\"btn-icon\">\ud83d\udd0d</span>\n",
        "        Classify New Image\n",
        "      </a>\n",
        "    </div>\n",
        "\n",
        "    <!-- Technical Note -->\n",
        "    <div class=\"tech-note\">\n",
        "      <span class=\"note-icon\">\u2139\ufe0f</span>\n",
        "      <p>\n",
        "        <strong>Note:</strong> The restored fingerprint is a reconstruction based on learned patterns\n",
        "        from authentic prints. While highly accurate (PSNR: 21.89 dB), it should be used for forensic\n",
        "        reference purposes. Always compare with the original altered print for investigation.\n",
        "      </p>\n",
        "    </div>\n",
        "  </div>\n",
        "  {% endif %}\n",
        "</div>\n",
        "{% endblock %}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ipuB9Q-Dk-n"
      },
      "source": [
        "\ud83c\udfa8 8\ufe0f\u20e3 CSS Stylesheet\n",
        "\n",
        "## \ud83c\udfa8 Modern UI Styling\n",
        "\n",
        "This stylesheet contains:\n",
        "- Dark theme\n",
        "- Gradients\n",
        "- Animations\n",
        "- Responsiveness\n",
        "- Card layouts for classification & restoration\n",
        "\n",
        "Place it inside static/style.css\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EGmnrCaYcHy"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 6\ufe0f\u20e3 CSS \u2014 static/style.css\n",
        "# ===============================\n",
        "%%writefile static/style.css\n",
        "/* ==========================================\n",
        "   FINGERPRINT AI - MODERN UI STYLES\n",
        "   ========================================== */\n",
        "\n",
        "/* CSS Variables */\n",
        ":root {\n",
        "  /* Color Palette */\n",
        "  --bg-primary: #0a0e27;\n",
        "  --bg-secondary: #131842;\n",
        "  --bg-card: rgba(255, 255, 255, 0.03);\n",
        "  --bg-card-hover: rgba(255, 255, 255, 0.06);\n",
        "\n",
        "  /* Gradients */\n",
        "  --gradient-primary: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "  --gradient-success: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);\n",
        "  --gradient-warning: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);\n",
        "  --gradient-info: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);\n",
        "\n",
        "  /* Text Colors */\n",
        "  --text-primary: #ffffff;\n",
        "  --text-secondary: #a0aec0;\n",
        "  --text-muted: #718096;\n",
        "\n",
        "  /* Accent Colors */\n",
        "  --accent-primary: #667eea;\n",
        "  --accent-success: #38ef7d;\n",
        "  --accent-warning: #f5576c;\n",
        "  --accent-info: #00f2fe;\n",
        "\n",
        "  /* Borders */\n",
        "  --border-color: rgba(255, 255, 255, 0.1);\n",
        "  --border-hover: rgba(255, 255, 255, 0.2);\n",
        "\n",
        "  /* Shadows */\n",
        "  --shadow-sm: 0 2px 8px rgba(0, 0, 0, 0.1);\n",
        "  --shadow-md: 0 4px 16px rgba(0, 0, 0, 0.2);\n",
        "  --shadow-lg: 0 10px 40px rgba(0, 0, 0, 0.3);\n",
        "  --shadow-xl: 0 20px 60px rgba(0, 0, 0, 0.4);\n",
        "\n",
        "  /* Spacing */\n",
        "  --spacing-xs: 0.5rem;\n",
        "  --spacing-sm: 1rem;\n",
        "  --spacing-md: 1.5rem;\n",
        "  --spacing-lg: 2rem;\n",
        "  --spacing-xl: 3rem;\n",
        "\n",
        "  /* Borders & Radius */\n",
        "  --radius-sm: 8px;\n",
        "  --radius-md: 12px;\n",
        "  --radius-lg: 16px;\n",
        "  --radius-xl: 24px;\n",
        "\n",
        "  /* Transitions */\n",
        "  --transition-fast: 0.2s ease;\n",
        "  --transition-base: 0.3s ease;\n",
        "  --transition-slow: 0.5s ease;\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   GLOBAL STYLES\n",
        "   ========================================== */\n",
        "\n",
        "* {\n",
        "  margin: 0;\n",
        "  padding: 0;\n",
        "  box-sizing: border-box;\n",
        "}\n",
        "\n",
        "html {\n",
        "  scroll-behavior: smooth;\n",
        "}\n",
        "\n",
        "body {\n",
        "  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;\n",
        "  background: var(--bg-primary);\n",
        "  color: var(--text-primary);\n",
        "  line-height: 1.6;\n",
        "  min-height: 100vh;\n",
        "  overflow-x: hidden;\n",
        "  position: relative;\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   ANIMATED BACKGROUND\n",
        "   ========================================== */\n",
        "\n",
        ".bg-animation {\n",
        "  position: fixed;\n",
        "  top: 0;\n",
        "  left: 0;\n",
        "  width: 100%;\n",
        "  height: 100%;\n",
        "  z-index: 0;\n",
        "  overflow: hidden;\n",
        "  pointer-events: none;\n",
        "}\n",
        "\n",
        ".fingerprint-pattern {\n",
        "  position: absolute;\n",
        "  width: 100%;\n",
        "  height: 100%;\n",
        "  background-image:\n",
        "    repeating-linear-gradient(90deg, rgba(102, 126, 234, 0.03) 0px, transparent 1px, transparent 40px),\n",
        "    repeating-linear-gradient(0deg, rgba(102, 126, 234, 0.03) 0px, transparent 1px, transparent 40px);\n",
        "  opacity: 0.5;\n",
        "}\n",
        "\n",
        ".gradient-orb {\n",
        "  position: absolute;\n",
        "  border-radius: 50%;\n",
        "  filter: blur(80px);\n",
        "  opacity: 0.3;\n",
        "  animation: float 20s ease-in-out infinite;\n",
        "}\n",
        "\n",
        ".orb-1 {\n",
        "  width: 400px;\n",
        "  height: 400px;\n",
        "  background: radial-gradient(circle, #667eea, transparent);\n",
        "  top: -200px;\n",
        "  left: -200px;\n",
        "  animation-delay: 0s;\n",
        "}\n",
        "\n",
        ".orb-2 {\n",
        "  width: 500px;\n",
        "  height: 500px;\n",
        "  background: radial-gradient(circle, #764ba2, transparent);\n",
        "  bottom: -250px;\n",
        "  right: -250px;\n",
        "  animation-delay: 5s;\n",
        "}\n",
        "\n",
        ".orb-3 {\n",
        "  width: 350px;\n",
        "  height: 350px;\n",
        "  background: radial-gradient(circle, #00f2fe, transparent);\n",
        "  top: 50%;\n",
        "  left: 50%;\n",
        "  animation-delay: 10s;\n",
        "}\n",
        "\n",
        "@keyframes float {\n",
        "  0%, 100% { transform: translate(0, 0) scale(1); }\n",
        "  33% { transform: translate(50px, -50px) scale(1.1); }\n",
        "  66% { transform: translate(-50px, 50px) scale(0.9); }\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   NAVIGATION BAR\n",
        "   ========================================== */\n",
        "\n",
        ".navbar {\n",
        "  position: sticky;\n",
        "  top: 0;\n",
        "  z-index: 1000;\n",
        "  background: rgba(10, 14, 39, 0.8);\n",
        "  backdrop-filter: blur(20px);\n",
        "  border-bottom: 1px solid var(--border-color);\n",
        "  padding: var(--spacing-sm) 0;\n",
        "}\n",
        "\n",
        ".nav-container {\n",
        "  max-width: 1400px;\n",
        "  margin: 0 auto;\n",
        "  padding: 0 var(--spacing-lg);\n",
        "  display: flex;\n",
        "  justify-content: space-between;\n",
        "  align-items: center;\n",
        "}\n",
        "\n",
        ".logo {\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  gap: var(--spacing-xs);\n",
        "  text-decoration: none;\n",
        "  font-size: 1.5rem;\n",
        "  font-weight: 700;\n",
        "  color: var(--text-primary);\n",
        "  transition: var(--transition-base);\n",
        "}\n",
        "\n",
        ".logo-icon {\n",
        "  font-size: 2rem;\n",
        "  animation: pulse 2s ease-in-out infinite;\n",
        "}\n",
        "\n",
        ".logo-text {\n",
        "  background: var(--gradient-primary);\n",
        "  -webkit-background-clip: text;\n",
        "  -webkit-text-fill-color: transparent;\n",
        "  background-clip: text;\n",
        "}\n",
        "\n",
        ".logo .ai {\n",
        "  color: var(--accent-info);\n",
        "}\n",
        "\n",
        ".logo:hover {\n",
        "  transform: translateY(-2px);\n",
        "}\n",
        "\n",
        "@keyframes pulse {\n",
        "  0%, 100% { transform: scale(1); }\n",
        "  50% { transform: scale(1.1); }\n",
        "}\n",
        "\n",
        ".nav-links {\n",
        "  display: flex;\n",
        "  gap: var(--spacing-sm);\n",
        "}\n",
        "\n",
        ".nav-link {\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  gap: 0.5rem;\n",
        "  padding: 0.75rem 1.25rem;\n",
        "  text-decoration: none;\n",
        "  color: var(--text-secondary);\n",
        "  font-weight: 500;\n",
        "  border-radius: var(--radius-md);\n",
        "  transition: var(--transition-base);\n",
        "  position: relative;\n",
        "}\n",
        "\n",
        ".nav-link::before {\n",
        "  content: '';\n",
        "  position: absolute;\n",
        "  bottom: 0;\n",
        "  left: 50%;\n",
        "  width: 0;\n",
        "  height: 2px;\n",
        "  background: var(--gradient-primary);\n",
        "  transform: translateX(-50%);\n",
        "  transition: var(--transition-base);\n",
        "}\n",
        "\n",
        ".nav-link:hover {\n",
        "  color: var(--text-primary);\n",
        "  background: var(--bg-card);\n",
        "}\n",
        "\n",
        ".nav-link:hover::before,\n",
        ".nav-link.active::before {\n",
        "  width: 80%;\n",
        "}\n",
        "\n",
        ".nav-link.active {\n",
        "  color: var(--text-primary);\n",
        "  background: var(--bg-card);\n",
        "}\n",
        "\n",
        ".nav-icon {\n",
        "  font-size: 1.2rem;\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   FLASH MESSAGES\n",
        "   ========================================== */\n",
        "\n",
        ".flash-container {\n",
        "  position: fixed;\n",
        "  top: 80px;\n",
        "  right: var(--spacing-lg);\n",
        "  z-index: 2000;\n",
        "  max-width: 400px;\n",
        "  animation: slideIn 0.3s ease;\n",
        "}\n",
        "\n",
        ".flash-message {\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  gap: var(--spacing-sm);\n",
        "  padding: var(--spacing-md);\n",
        "  background: var(--bg-secondary);\n",
        "  border: 1px solid var(--border-color);\n",
        "  border-radius: var(--radius-md);\n",
        "  box-shadow: var(--shadow-lg);\n",
        "  margin-bottom: var(--spacing-sm);\n",
        "  animation: slideIn 0.3s ease;\n",
        "}\n",
        "\n",
        ".flash-warning {\n",
        "  border-left: 4px solid var(--accent-warning);\n",
        "}\n",
        "\n",
        ".flash-icon {\n",
        "  font-size: 1.5rem;\n",
        "  flex-shrink: 0;\n",
        "}\n",
        "\n",
        ".flash-close {\n",
        "  margin-left: auto;\n",
        "  background: none;\n",
        "  border: none;\n",
        "  color: var(--text-secondary);\n",
        "  cursor: pointer;\n",
        "  font-size: 1.2rem;\n",
        "  padding: 0.25rem 0.5rem;\n",
        "  border-radius: var(--radius-sm);\n",
        "  transition: var(--transition-fast);\n",
        "}\n",
        "\n",
        ".flash-close:hover {\n",
        "  background: var(--bg-card);\n",
        "  color: var(--text-primary);\n",
        "}\n",
        "\n",
        "@keyframes slideIn {\n",
        "  from {\n",
        "    transform: translateX(100%);\n",
        "    opacity: 0;\n",
        "  }\n",
        "  to {\n",
        "    transform: translateX(0);\n",
        "    opacity: 1;\n",
        "  }\n",
        "}\n",
        "\n",
        "@keyframes slideOut {\n",
        "  from {\n",
        "    transform: translateX(0);\n",
        "    opacity: 1;\n",
        "  }\n",
        "  to {\n",
        "    transform: translateX(100%);\n",
        "    opacity: 0;\n",
        "  }\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   MAIN CONTAINER\n",
        "   ========================================== */\n",
        "\n",
        ".main-container {\n",
        "  position: relative;\n",
        "  z-index: 1;\n",
        "  max-width: 1400px;\n",
        "  margin: 0 auto;\n",
        "  padding: var(--spacing-xl) var(--spacing-lg);\n",
        "  min-height: calc(100vh - 180px);\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   HERO SECTION (Homepage)\n",
        "   ========================================== */\n",
        "\n",
        ".hero-section {\n",
        "  display: grid;\n",
        "  grid-template-columns: 1fr 1fr;\n",
        "  gap: var(--spacing-xl);\n",
        "  align-items: center;\n",
        "  padding: var(--spacing-xl) 0;\n",
        "  min-height: 70vh;\n",
        "}\n",
        "\n",
        ".hero-content {\n",
        "  animation: fadeInLeft 0.8s ease;\n",
        "}\n",
        "\n",
        ".hero-badge {\n",
        "  display: inline-flex;\n",
        "  align-items: center;\n",
        "  gap: var(--spacing-xs);\n",
        "  padding: 0.5rem 1rem;\n",
        "  background: var(--bg-card);\n",
        "  border: 1px solid var(--border-color);\n",
        "  border-radius: 50px;\n",
        "  font-size: 0.875rem;\n",
        "  font-weight: 500;\n",
        "  color: var(--text-secondary);\n",
        "  margin-bottom: var(--spacing-md);\n",
        "}\n",
        "\n",
        ".badge-dot {\n",
        "  width: 8px;\n",
        "  height: 8px;\n",
        "  background: var(--accent-success);\n",
        "  border-radius: 50%;\n",
        "  animation: blink 2s ease-in-out infinite;\n",
        "}\n",
        "\n",
        "@keyframes blink {\n",
        "  0%, 100% { opacity: 1; }\n",
        "  50% { opacity: 0.3; }\n",
        "}\n",
        "\n",
        ".hero-title {\n",
        "  font-size: 3.5rem;\n",
        "  font-weight: 800;\n",
        "  line-height: 1.2;\n",
        "  margin-bottom: var(--spacing-md);\n",
        "  color: var(--text-primary);\n",
        "}\n",
        "\n",
        ".gradient-text {\n",
        "  background: var(--gradient-primary);\n",
        "  -webkit-background-clip: text;\n",
        "  -webkit-text-fill-color: transparent;\n",
        "  background-clip: text;\n",
        "}\n",
        "\n",
        ".hero-subtitle {\n",
        "  font-size: 1.25rem;\n",
        "  color: var(--text-secondary);\n",
        "  line-height: 1.8;\n",
        "  margin-bottom: var(--spacing-lg);\n",
        "}\n",
        "\n",
        ".hero-buttons {\n",
        "  display: flex;\n",
        "  gap: var(--spacing-md);\n",
        "  flex-wrap: wrap;\n",
        "}\n",
        "\n",
        "/* Hero Visual */\n",
        ".hero-visual {\n",
        "  animation: fadeInRight 0.8s ease;\n",
        "}\n",
        "\n",
        ".fingerprint-showcase {\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  justify-content: center;\n",
        "  gap: var(--spacing-md);\n",
        "  flex-wrap: wrap;\n",
        "}\n",
        "\n",
        ".showcase-card {\n",
        "  background: var(--bg-card);\n",
        "  border: 1px solid var(--border-color);\n",
        "  border-radius: var(--radius-lg);\n",
        "  padding: var(--spacing-lg);\n",
        "  text-align: center;\n",
        "  min-width: 150px;\n",
        "  transition: var(--transition-base);\n",
        "  animation: float 3s ease-in-out infinite;\n",
        "}\n",
        "\n",
        ".card-1 { animation-delay: 0s; }\n",
        ".card-2 { animation-delay: 0.5s; }\n",
        ".card-3 { animation-delay: 1s; }\n",
        "\n",
        ".showcase-card:hover {\n",
        "  background: var(--bg-card-hover);\n",
        "  border-color: var(--border-hover);\n",
        "  transform: translateY(-5px);\n",
        "}\n",
        "\n",
        ".card-icon {\n",
        "  font-size: 3rem;\n",
        "  margin-bottom: var(--spacing-sm);\n",
        "}\n",
        "\n",
        ".showcase-card h3 {\n",
        "  font-size: 1rem;\n",
        "  margin-bottom: 0.5rem;\n",
        "  color: var(--text-primary);\n",
        "}\n",
        "\n",
        ".showcase-card p {\n",
        "  font-size: 0.875rem;\n",
        "  color: var(--text-secondary);\n",
        "}\n",
        "\n",
        ".showcase-arrow {\n",
        "  font-size: 2rem;\n",
        "  color: var(--accent-primary);\n",
        "  animation: bounce 2s ease-in-out infinite;\n",
        "}\n",
        "\n",
        "@keyframes bounce {\n",
        "  0%, 100% { transform: translateX(0); }\n",
        "  50% { transform: translateX(10px); }\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   FEATURES SECTION\n",
        "   ========================================== */\n",
        "\n",
        ".features-section,\n",
        ".workflow-section {\n",
        "  margin: var(--spacing-xl) 0;\n",
        "  padding: var(--spacing-xl) 0;\n",
        "}\n",
        "\n",
        ".section-title {\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  justify-content: center;\n",
        "  gap: var(--spacing-sm);\n",
        "  font-size: 2.5rem;\n",
        "  font-weight: 700;\n",
        "  margin-bottom: var(--spacing-xl);\n",
        "  text-align: center;\n",
        "}\n",
        "\n",
        ".title-icon {\n",
        "  font-size: 2.5rem;\n",
        "}\n",
        "\n",
        ".features-grid {\n",
        "  display: grid;\n",
        "  grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));\n",
        "  gap: var(--spacing-lg);\n",
        "}\n",
        "\n",
        ".feature-card {\n",
        "  background: var(--bg-card);\n",
        "  border: 1px solid var(--border-color);\n",
        "  border-radius: var(--radius-lg);\n",
        "  padding: var(--spacing-lg);\n",
        "  text-align: center;\n",
        "  transition: var(--transition-base);\n",
        "  position: relative;\n",
        "  overflow: hidden;\n",
        "}\n",
        "\n",
        ".feature-card::before {\n",
        "  content: '';\n",
        "  position: absolute;\n",
        "  top: 0;\n",
        "  left: 0;\n",
        "  width: 100%;\n",
        "  height: 4px;\n",
        "  background: var(--gradient-primary);\n",
        "  transform: scaleX(0);\n",
        "  transition: var(--transition-base);\n",
        "}\n",
        "\n",
        ".feature-card:hover::before {\n",
        "  transform: scaleX(1);\n",
        "}\n",
        "\n",
        ".feature-card:hover {\n",
        "  background: var(--bg-card-hover);\n",
        "  border-color: var(--border-hover);\n",
        "  transform: translateY(-5px);\n",
        "  box-shadow: var(--shadow-lg);\n",
        "}\n",
        "\n",
        ".feature-icon {\n",
        "  font-size: 3rem;\n",
        "  margin-bottom: var(--spacing-md);\n",
        "  display: inline-block;\n",
        "  animation: bounce 2s ease-in-out infinite;\n",
        "}\n",
        "\n",
        ".feature-card:nth-child(1) .feature-icon { animation-delay: 0s; }\n",
        ".feature-card:nth-child(2) .feature-icon { animation-delay: 0.2s; }\n",
        ".feature-card:nth-child(3) .feature-icon { animation-delay: 0.4s; }\n",
        ".feature-card:nth-child(4) .feature-icon { animation-delay: 0.6s; }\n",
        "\n",
        ".feature-card h3 {\n",
        "  font-size: 1.5rem;\n",
        "  margin-bottom: var(--spacing-sm);\n",
        "  color: var(--text-primary);\n",
        "}\n",
        "\n",
        ".feature-card p {\n",
        "  color: var(--text-secondary);\n",
        "  line-height: 1.6;\n",
        "  margin-bottom: var(--spacing-md);\n",
        "}\n",
        "\n",
        ".feature-stat {\n",
        "  display: inline-block;\n",
        "  padding: 0.5rem 1rem;\n",
        "  background: var(--gradient-primary);\n",
        "  border-radius: 50px;\n",
        "  font-weight: 700;\n",
        "  font-size: 1.125rem;\n",
        "  color: white;\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   WORKFLOW SECTION\n",
        "   ========================================== */\n",
        "\n",
        ".workflow-steps {\n",
        "  display: flex;\n",
        "  align-items: flex-start;\n",
        "  justify-content: center;\n",
        "  gap: var(--spacing-sm);\n",
        "  flex-wrap: wrap;\n",
        "}\n",
        "\n",
        ".workflow-step {\n",
        "  flex: 1;\n",
        "  min-width: 200px;\n",
        "  max-width: 250px;\n",
        "  background: var(--bg-card);\n",
        "  border: 1px solid var(--border-color);\n",
        "  border-radius: var(--radius-lg);\n",
        "  padding: var(--spacing-lg);\n",
        "  text-align: center;\n",
        "  transition: var(--transition-base);\n",
        "}\n",
        "\n",
        ".workflow-step:hover {\n",
        "  background: var(--bg-card-hover);\n",
        "  transform: translateY(-5px);\n",
        "  box-shadow: var(--shadow-md);\n",
        "}\n",
        "\n",
        ".step-number {\n",
        "  width: 60px;\n",
        "  height: 60px;\n",
        "  margin: 0 auto var(--spacing-md);\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  justify-content: center;\n",
        "  background: var(--gradient-primary);\n",
        "  border-radius: 50%;\n",
        "  font-size: 1.5rem;\n",
        "  font-weight: 700;\n",
        "  color: white;\n",
        "}\n",
        "\n",
        ".step-content h3 {\n",
        "  font-size: 1.125rem;\n",
        "  margin-bottom: var(--spacing-sm);\n",
        "  color: var(--text-primary);\n",
        "}\n",
        "\n",
        ".step-content p {\n",
        "  font-size: 0.875rem;\n",
        "  color: var(--text-secondary);\n",
        "  line-height: 1.6;\n",
        "}\n",
        "\n",
        ".workflow-connector {\n",
        "  width: 50px;\n",
        "  height: 2px;\n",
        "  background: linear-gradient(90deg, var(--accent-primary), transparent);\n",
        "  align-self: center;\n",
        "  margin-top: 40px;\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   CTA SECTION\n",
        "   ========================================== */\n",
        "\n",
        ".cta-section {\n",
        "  background: var(--gradient-primary);\n",
        "  border-radius: var(--radius-xl);\n",
        "  padding: var(--spacing-xl);\n",
        "  text-align: center;\n",
        "  margin: var(--spacing-xl) 0;\n",
        "}\n",
        "\n",
        ".cta-content h2 {\n",
        "  font-size: 2.5rem;\n",
        "  margin-bottom: var(--spacing-md);\n",
        "  color: white;\n",
        "}\n",
        "\n",
        ".cta-content p {\n",
        "  font-size: 1.25rem;\n",
        "  margin-bottom: var(--spacing-lg);\n",
        "  color: rgba(255, 255, 255, 0.9);\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   BUTTONS\n",
        "   ========================================== */\n",
        "\n",
        ".btn {\n",
        "  display: inline-flex;\n",
        "  align-items: center;\n",
        "  gap: var(--spacing-xs);\n",
        "  padding: 0.875rem 1.75rem;\n",
        "  font-size: 1rem;\n",
        "  font-weight: 600;\n",
        "  text-decoration: none;\n",
        "  border: none;\n",
        "  border-radius: var(--radius-md);\n",
        "  cursor: pointer;\n",
        "  transition: var(--transition-base);\n",
        "  position: relative;\n",
        "  overflow: hidden;\n",
        "}\n",
        "\n",
        ".btn::before {\n",
        "  content: '';\n",
        "  position: absolute;\n",
        "  top: 50%;\n",
        "  left: 50%;\n",
        "  width: 0;\n",
        "  height: 0;\n",
        "  background: rgba(255, 255, 255, 0.2);\n",
        "  border-radius: 50%;\n",
        "  transform: translate(-50%, -50%);\n",
        "  transition: width 0.6s, height 0.6s;\n",
        "}\n",
        "\n",
        ".btn:hover::before {\n",
        "  width: 300px;\n",
        "  height: 300px;\n",
        "}\n",
        "\n",
        ".btn-primary {\n",
        "  background: var(--gradient-primary);\n",
        "  color: white;\n",
        "  box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);\n",
        "}\n",
        "\n",
        ".btn-primary:hover {\n",
        "  transform: translateY(-2px);\n",
        "  box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);\n",
        "}\n",
        "\n",
        ".btn-secondary {\n",
        "  background: var(--gradient-info);\n",
        "  color: white;\n",
        "  box-shadow: 0 4px 15px rgba(0, 242, 254, 0.4);\n",
        "}\n",
        "\n",
        ".btn-secondary:hover {\n",
        "  transform: translateY(-2px);\n",
        "  box-shadow: 0 6px 20px rgba(0, 242, 254, 0.6);\n",
        "}\n",
        "\n",
        ".btn-success {\n",
        "  background: var(--gradient-success);\n",
        "  color: white;\n",
        "  box-shadow: 0 4px 15px rgba(56, 239, 125, 0.4);\n",
        "}\n",
        "\n",
        ".btn-success:hover {\n",
        "  transform: translateY(-2px);\n",
        "  box-shadow: 0 6px 20px rgba(56, 239, 125, 0.6);\n",
        "}\n",
        "\n",
        ".btn-outline {\n",
        "  background: transparent;\n",
        "  color: var(--text-primary);\n",
        "  border: 2px solid var(--border-color);\n",
        "}\n",
        "\n",
        ".btn-outline:hover {\n",
        "  border-color: var(--accent-primary);\n",
        "  background: var(--bg-card);\n",
        "}\n",
        "\n",
        ".btn-large {\n",
        "  padding: 1.125rem 2.5rem;\n",
        "  font-size: 1.125rem;\n",
        "}\n",
        "\n",
        ".btn-block {\n",
        "  width: 100%;\n",
        "  justify-content: center;\n",
        "}\n",
        "\n",
        ".btn-icon {\n",
        "  font-size: 1.25rem;\n",
        "}\n",
        "\n",
        ".btn-arrow {\n",
        "  transition: var(--transition-base);\n",
        "}\n",
        "\n",
        ".btn:hover .btn-arrow {\n",
        "  transform: translateX(5px);\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   PAGE CONTAINER\n",
        "   ========================================== */\n",
        "\n",
        ".page-container {\n",
        "  animation: fadeInUp 0.6s ease;\n",
        "}\n",
        "\n",
        ".page-header {\n",
        "  text-align: center;\n",
        "  margin-bottom: var(--spacing-xl);\n",
        "}\n",
        "\n",
        ".page-icon {\n",
        "  font-size: 4rem;\n",
        "  margin-bottom: var(--spacing-md);\n",
        "  display: inline-block;\n",
        "  animation: bounce 2s ease-in-out infinite;\n",
        "}\n",
        "\n",
        ".page-title {\n",
        "  font-size: 3rem;\n",
        "  font-weight: 700;\n",
        "  margin-bottom: var(--spacing-sm);\n",
        "  background: var(--gradient-primary);\n",
        "  -webkit-background-clip: text;\n",
        "  -webkit-text-fill-color: transparent;\n",
        "  background-clip: text;\n",
        "}\n",
        "\n",
        ".page-description {\n",
        "  font-size: 1.25rem;\n",
        "  color: var(--text-secondary);\n",
        "  max-width: 700px;\n",
        "  margin: 0 auto;\n",
        "}\n",
        "\n",
        ".content-grid {\n",
        "  display: grid;\n",
        "  grid-template-columns: 1fr 1fr;\n",
        "  gap: var(--spacing-xl);\n",
        "  align-items: start;\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   UPLOAD SECTION\n",
        "   ========================================== */\n",
        "\n",
        ".upload-section {\n",
        "  background: var(--bg-card);\n",
        "  border: 1px solid var(--border-color);\n",
        "  border-radius: var(--radius-lg);\n",
        "  padding: var(--spacing-lg);\n",
        "}\n",
        "\n",
        ".upload-form {\n",
        "  display: flex;\n",
        "  flex-direction: column;\n",
        "  gap: var(--spacing-md);\n",
        "}\n",
        "\n",
        ".upload-zone {\n",
        "  position: relative;\n",
        "  border: 2px dashed var(--border-color);\n",
        "  border-radius: var(--radius-lg);\n",
        "  padding: var(--spacing-xl);\n",
        "  text-align: center;\n",
        "  background: var(--bg-secondary);\n",
        "  cursor: pointer;\n",
        "  transition: var(--transition-base);\n",
        "  min-height: 300px;\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  justify-content: center;\n",
        "}\n",
        "\n",
        ".upload-zone:hover,\n",
        ".upload-zone.drag-active {\n",
        "  border-color: var(--accent-primary);\n",
        "  background: rgba(102, 126, 234, 0.05);\n",
        "}\n",
        "\n",
        ".upload-zone input[type=\"file\"] {\n",
        "  position: absolute;\n",
        "  width: 100%;\n",
        "  height: 100%;\n",
        "  top: 0;\n",
        "  left: 0;\n",
        "  opacity: 0;\n",
        "  cursor: pointer;\n",
        "}\n",
        "\n",
        ".upload-content {\n",
        "  width: 100%;\n",
        "}\n",
        "\n",
        ".upload-icon-large {\n",
        "  font-size: 5rem;\n",
        "  margin-bottom: var(--spacing-md);\n",
        "  animation: pulse 2s ease-in-out infinite;\n",
        "}\n",
        "\n",
        ".upload-title {\n",
        "  font-size: 1.5rem;\n",
        "  font-weight: 600;\n",
        "  margin-bottom: var(--spacing-sm);\n",
        "  color: var(--text-primary);\n",
        "}\n",
        "\n",
        ".upload-text {\n",
        "  font-size: 1rem;\n",
        "  color: var(--text-secondary);\n",
        "  margin-bottom: var(--spacing-xs);\n",
        "}\n",
        "\n",
        ".upload-hint {\n",
        "  font-size: 0.875rem;\n",
        "  color: var(--text-muted);\n",
        "}\n",
        "\n",
        ".upload-preview {\n",
        "  margin-top: var(--spacing-md);\n",
        "}\n",
        "\n",
        ".info-banner {\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  gap: var(--spacing-sm);\n",
        "  padding: var(--spacing-md);\n",
        "  background: rgba(0, 242, 254, 0.1);\n",
        "  border: 1px solid rgba(0, 242, 254, 0.3);\n",
        "  border-radius: var(--radius-md);\n",
        "  margin-bottom: var(--spacing-md);\n",
        "}\n",
        "\n",
        ".banner-icon {\n",
        "  font-size: 1.5rem;\n",
        "  flex-shrink: 0;\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   RESULTS SECTION\n",
        "   ========================================== */\n",
        "\n",
        ".results-section {\n",
        "  display: flex;\n",
        "  flex-direction: column;\n",
        "  gap: var(--spacing-lg);\n",
        "}\n",
        "\n",
        ".preview-card,\n",
        ".result-card,\n",
        ".details-card,\n",
        ".instructions-card {\n",
        "  background: var(--bg-card);\n",
        "  border: 1px solid var(--border-color);\n",
        "  border-radius: var(--radius-lg);\n",
        "  padding: var(--spacing-lg);\n",
        "  animation: fadeIn 0.5s ease;\n",
        "}\n",
        "\n",
        ".card-title {\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  gap: var(--spacing-sm);\n",
        "  font-size: 1.25rem;\n",
        "  font-weight: 600;\n",
        "  margin-bottom: var(--spacing-md);\n",
        "  color: var(--text-primary);\n",
        "}\n",
        "\n",
        ".card-icon {\n",
        "  font-size: 1.5rem;\n",
        "}\n",
        "\n",
        ".image-container {\n",
        "  border-radius: var(--radius-md);\n",
        "  overflow: hidden;\n",
        "  border: 1px solid var(--border-color);\n",
        "}\n",
        "\n",
        ".preview-image {\n",
        "  width: 100%;\n",
        "  height: auto;\n",
        "  display: block;\n",
        "  transition: var(--transition-base);\n",
        "}\n",
        "\n",
        ".preview-image:hover {\n",
        "  transform: scale(1.02);\n",
        "}\n",
        "\n",
        "/* Result Display */\n",
        ".result-display {\n",
        "  display: flex;\n",
        "  flex-direction: column;\n",
        "  gap: var(--spacing-lg);\n",
        "}\n",
        "\n",
        ".result-badge {\n",
        "  display: inline-flex;\n",
        "  align-items: center;\n",
        "  gap: var(--spacing-sm);\n",
        "  padding: var(--spacing-md) var(--spacing-lg);\n",
        "  border-radius: var(--radius-md);\n",
        "  font-size: 1.5rem;\n",
        "  font-weight: 700;\n",
        "  text-align: center;\n",
        "  justify-content: center;\n",
        "  animation: scaleIn 0.5s ease;\n",
        "}\n",
        "\n",
        ".badge-real {\n",
        "  background: rgba(56, 239, 125, 0.2);\n",
        "  border: 2px solid var(--accent-success);\n",
        "  color: var(--accent-success);\n",
        "}\n",
        "\n",
        ".badge-altered {\n",
        "  background: rgba(245, 87, 108, 0.2);\n",
        "  border: 2px solid var(--accent-warning);\n",
        "  color: var(--accent-warning);\n",
        "}\n",
        "\n",
        ".badge-icon {\n",
        "  font-size: 2rem;\n",
        "}\n",
        "\n",
        ".badge-label {\n",
        "  font-weight: 700;\n",
        "}\n",
        "\n",
        "@keyframes scaleIn {\n",
        "  from {\n",
        "    transform: scale(0.8);\n",
        "    opacity: 0;\n",
        "  }\n",
        "  to {\n",
        "    transform: scale(1);\n",
        "    opacity: 1;\n",
        "  }\n",
        "}\n",
        "\n",
        "/* Confidence Meter */\n",
        ".confidence-meter {\n",
        "  background: var(--bg-secondary);\n",
        "  border-radius: var(--radius-md);\n",
        "  padding: var(--spacing-md);\n",
        "}\n",
        "\n",
        ".confidence-label {\n",
        "  display: flex;\n",
        "  justify-content: space-between;\n",
        "  align-items: center;\n",
        "  margin-bottom: var(--spacing-sm);\n",
        "  font-weight: 600;\n",
        "}\n",
        "\n",
        ".confidence-value {\n",
        "  font-size: 1.125rem;\n",
        "  color: var(--accent-primary);\n",
        "}\n",
        "\n",
        ".confidence-bar {\n",
        "  height: 12px;\n",
        "  background: rgba(255, 255, 255, 0.1);\n",
        "  border-radius: 50px;\n",
        "  overflow: hidden;\n",
        "}\n",
        "\n",
        ".confidence-fill {\n",
        "  height: 100%;\n",
        "  border-radius: 50px;\n",
        "  transition: width 1s ease;\n",
        "  animation: fillBar 1s ease;\n",
        "}\n",
        "\n",
        ".fill-real {\n",
        "  background: var(--gradient-success);\n",
        "}\n",
        "\n",
        ".fill-altered {\n",
        "  background: var(--gradient-warning);\n",
        "}\n",
        "\n",
        "@keyframes fillBar {\n",
        "  from { width: 0; }\n",
        "}\n",
        "\n",
        "/* Result Description */\n",
        ".result-description {\n",
        "  background: var(--bg-secondary);\n",
        "  border-radius: var(--radius-md);\n",
        "  padding: var(--spacing-md);\n",
        "  display: flex;\n",
        "  gap: var(--spacing-md);\n",
        "}\n",
        "\n",
        ".desc-icon {\n",
        "  font-size: 2rem;\n",
        "  flex-shrink: 0;\n",
        "}\n",
        "\n",
        ".desc-text {\n",
        "  color: var(--text-secondary);\n",
        "  line-height: 1.8;\n",
        "}\n",
        "\n",
        ".action-buttons {\n",
        "  display: flex;\n",
        "  gap: var(--spacing-md);\n",
        "}\n",
        "\n",
        "/* Details Card */\n",
        ".details-grid {\n",
        "  display: grid;\n",
        "  grid-template-columns: repeat(2, 1fr);\n",
        "  gap: var(--spacing-md);\n",
        "}\n",
        "\n",
        ".detail-item {\n",
        "  display: flex;\n",
        "  flex-direction: column;\n",
        "  gap: 0.25rem;\n",
        "  padding: var(--spacing-md);\n",
        "  background: var(--bg-secondary);\n",
        "  border-radius: var(--radius-md);\n",
        "}\n",
        "\n",
        ".detail-label {\n",
        "  font-size: 0.875rem;\n",
        "  color: var(--text-muted);\n",
        "  font-weight: 500;\n",
        "}\n",
        "\n",
        ".detail-value {\n",
        "  font-size: 1.125rem;\n",
        "  color: var(--text-primary);\n",
        "  font-weight: 600;\n",
        "}\n",
        "\n",
        "/* Instructions Card */\n",
        ".instructions-card {\n",
        "  text-align: center;\n",
        "}\n",
        "\n",
        ".instruction-icon {\n",
        "  font-size: 4rem;\n",
        "  margin-bottom: var(--spacing-md);\n",
        "  display: inline-block;\n",
        "  animation: pulse 2s ease-in-out infinite;\n",
        "}\n",
        "\n",
        ".instructions-card h3 {\n",
        "  font-size: 1.5rem;\n",
        "  margin-bottom: var(--spacing-md);\n",
        "  color: var(--text-primary);\n",
        "}\n",
        "\n",
        ".instruction-list {\n",
        "  list-style: none;\n",
        "  text-align: left;\n",
        "  max-width: 600px;\n",
        "  margin: 0 auto var(--spacing-lg);\n",
        "}\n",
        "\n",
        ".instruction-list li {\n",
        "  padding: var(--spacing-sm);\n",
        "  margin-bottom: var(--spacing-sm);\n",
        "  color: var(--text-secondary);\n",
        "  line-height: 1.8;\n",
        "  border-left: 3px solid var(--accent-primary);\n",
        "  padding-left: var(--spacing-md);\n",
        "}\n",
        "\n",
        ".info-box {\n",
        "  display: flex;\n",
        "  align-items: flex-start;\n",
        "  gap: var(--spacing-md);\n",
        "  padding: var(--spacing-md);\n",
        "  background: rgba(102, 126, 234, 0.1);\n",
        "  border: 1px solid rgba(102, 126, 234, 0.3);\n",
        "  border-radius: var(--radius-md);\n",
        "  text-align: left;\n",
        "}\n",
        "\n",
        ".info-icon {\n",
        "  font-size: 1.5rem;\n",
        "  flex-shrink: 0;\n",
        "}\n",
        "\n",
        ".info-box p {\n",
        "  color: var(--text-secondary);\n",
        "  line-height: 1.8;\n",
        "  margin: 0;\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   RESTORATION PAGE\n",
        "   ========================================== */\n",
        "\n",
        ".restoration-results {\n",
        "  animation: fadeIn 0.6s ease;\n",
        "}\n",
        "\n",
        ".results-header {\n",
        "  text-align: center;\n",
        "  margin-bottom: var(--spacing-xl);\n",
        "}\n",
        "\n",
        ".success-badge {\n",
        "  display: inline-flex;\n",
        "  align-items: center;\n",
        "  gap: var(--spacing-sm);\n",
        "  padding: var(--spacing-md) var(--spacing-lg);\n",
        "  background: rgba(56, 239, 125, 0.2);\n",
        "  border: 2px solid var(--accent-success);\n",
        "  border-radius: 50px;\n",
        "  font-size: 1.25rem;\n",
        "  font-weight: 600;\n",
        "  color: var(--accent-success);\n",
        "  margin-bottom: var(--spacing-md);\n",
        "}\n",
        "\n",
        ".results-subtitle {\n",
        "  font-size: 1.125rem;\n",
        "  color: var(--text-secondary);\n",
        "}\n",
        "\n",
        "/* Comparison Container */\n",
        ".comparison-container {\n",
        "  display: grid;\n",
        "  grid-template-columns: 1fr auto 1fr;\n",
        "  gap: var(--spacing-lg);\n",
        "  align-items: center;\n",
        "  margin-bottom: var(--spacing-xl);\n",
        "}\n",
        "\n",
        ".comparison-card {\n",
        "  background: var(--bg-card);\n",
        "  border: 1px solid var(--border-color);\n",
        "  border-radius: var(--radius-lg);\n",
        "  padding: var(--spacing-lg);\n",
        "  transition: var(--transition-base);\n",
        "}\n",
        "\n",
        ".comparison-card:hover {\n",
        "  background: var(--bg-card-hover);\n",
        "  border-color: var(--border-hover);\n",
        "  transform: translateY(-5px);\n",
        "  box-shadow: var(--shadow-lg);\n",
        "}\n",
        "\n",
        ".card-before {\n",
        "  animation: slideInLeft 0.6s ease;\n",
        "}\n",
        "\n",
        ".card-after {\n",
        "  animation: slideInRight 0.6s ease;\n",
        "}\n",
        "\n",
        ".card-header {\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  gap: var(--spacing-sm);\n",
        "  margin-bottom: var(--spacing-md);\n",
        "}\n",
        "\n",
        ".card-header h3 {\n",
        "  font-size: 1.25rem;\n",
        "  font-weight: 600;\n",
        "  color: var(--text-primary);\n",
        "}\n",
        "\n",
        ".image-wrapper {\n",
        "  position: relative;\n",
        "  border-radius: var(--radius-md);\n",
        "  overflow: hidden;\n",
        "  border: 1px solid var(--border-color);\n",
        "  margin-bottom: var(--spacing-md);\n",
        "}\n",
        "\n",
        ".comparison-image {\n",
        "  width: 100%;\n",
        "  height: auto;\n",
        "  display: block;\n",
        "  transition: var(--transition-base);\n",
        "}\n",
        "\n",
        ".comparison-image:hover {\n",
        "  transform: scale(1.05);\n",
        "}\n",
        "\n",
        ".image-label {\n",
        "  position: absolute;\n",
        "  top: var(--spacing-sm);\n",
        "  right: var(--spacing-sm);\n",
        "  padding: 0.5rem 1rem;\n",
        "  border-radius: 50px;\n",
        "  font-size: 0.75rem;\n",
        "  font-weight: 700;\n",
        "  text-transform: uppercase;\n",
        "  letter-spacing: 1px;\n",
        "}\n",
        "\n",
        ".label-before {\n",
        "  background: rgba(245, 87, 108, 0.9);\n",
        "  color: white;\n",
        "}\n",
        "\n",
        ".label-after {\n",
        "  background: rgba(56, 239, 125, 0.9);\n",
        "  color: white;\n",
        "}\n",
        "\n",
        ".card-footer {\n",
        "  text-align: center;\n",
        "}\n",
        "\n",
        ".card-footer p {\n",
        "  color: var(--text-secondary);\n",
        "  font-size: 0.875rem;\n",
        "}\n",
        "\n",
        "/* Comparison Arrow */\n",
        ".comparison-arrow {\n",
        "  display: flex;\n",
        "  flex-direction: column;\n",
        "  align-items: center;\n",
        "  gap: var(--spacing-sm);\n",
        "}\n",
        "\n",
        ".arrow-icon {\n",
        "  font-size: 3rem;\n",
        "  color: var(--accent-primary);\n",
        "  animation: pulse 2s ease-in-out infinite;\n",
        "}\n",
        "\n",
        ".arrow-label {\n",
        "  font-size: 0.875rem;\n",
        "  font-weight: 600;\n",
        "  color: var(--text-secondary);\n",
        "  text-transform: uppercase;\n",
        "  letter-spacing: 1px;\n",
        "}\n",
        "\n",
        "/* Analysis Section */\n",
        ".analysis-section {\n",
        "  margin: var(--spacing-xl) 0;\n",
        "}\n",
        "\n",
        ".section-subtitle {\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  gap: var(--spacing-sm);\n",
        "  font-size: 1.75rem;\n",
        "  font-weight: 600;\n",
        "  margin-bottom: var(--spacing-lg);\n",
        "  color: var(--text-primary);\n",
        "}\n",
        "\n",
        ".subtitle-icon {\n",
        "  font-size: 2rem;\n",
        "}\n",
        "\n",
        ".analysis-grid {\n",
        "  display: grid;\n",
        "  grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n",
        "  gap: var(--spacing-lg);\n",
        "}\n",
        "\n",
        ".analysis-card {\n",
        "  background: var(--bg-card);\n",
        "  border: 1px solid var(--border-color);\n",
        "  border-radius: var(--radius-lg);\n",
        "  padding: var(--spacing-lg);\n",
        "  text-align: center;\n",
        "  transition: var(--transition-base);\n",
        "}\n",
        "\n",
        ".analysis-card:hover {\n",
        "  background: var(--bg-card-hover);\n",
        "  border-color: var(--border-hover);\n",
        "  transform: translateY(-5px);\n",
        "  box-shadow: var(--shadow-md);\n",
        "}\n",
        "\n",
        ".analysis-icon {\n",
        "  font-size: 2.5rem;\n",
        "  margin-bottom: var(--spacing-md);\n",
        "  display: inline-block;\n",
        "}\n",
        "\n",
        ".analysis-card h4 {\n",
        "  font-size: 1.125rem;\n",
        "  margin-bottom: var(--spacing-sm);\n",
        "  color: var(--text-primary);\n",
        "}\n",
        "\n",
        ".analysis-card p {\n",
        "  color: var(--text-secondary);\n",
        "  line-height: 1.6;\n",
        "  font-size: 0.875rem;\n",
        "}\n",
        "\n",
        "/* Action Section */\n",
        ".action-section {\n",
        "  display: flex;\n",
        "  gap: var(--spacing-md);\n",
        "  justify-content: center;\n",
        "  flex-wrap: wrap;\n",
        "  margin: var(--spacing-xl) 0;\n",
        "}\n",
        "\n",
        "/* Technical Note */\n",
        ".tech-note {\n",
        "  display: flex;\n",
        "  align-items: flex-start;\n",
        "  gap: var(--spacing-md);\n",
        "  padding: var(--spacing-lg);\n",
        "  background: var(--bg-card);\n",
        "  border: 1px solid var(--border-color);\n",
        "  border-left: 4px solid var(--accent-info);\n",
        "  border-radius: var(--radius-md);\n",
        "}\n",
        "\n",
        ".note-icon {\n",
        "  font-size: 1.5rem;\n",
        "  flex-shrink: 0;\n",
        "}\n",
        "\n",
        ".tech-note p {\n",
        "  color: var(--text-secondary);\n",
        "  line-height: 1.8;\n",
        "  margin: 0;\n",
        "}\n",
        "\n",
        "/* Process Steps */\n",
        ".process-steps {\n",
        "  display: flex;\n",
        "  flex-direction: column;\n",
        "  gap: var(--spacing-md);\n",
        "  margin-bottom: var(--spacing-lg);\n",
        "}\n",
        "\n",
        ".process-step {\n",
        "  display: flex;\n",
        "  gap: var(--spacing-md);\n",
        "  align-items: flex-start;\n",
        "  padding: var(--spacing-md);\n",
        "  background: var(--bg-secondary);\n",
        "  border-radius: var(--radius-md);\n",
        "  transition: var(--transition-base);\n",
        "}\n",
        "\n",
        ".process-step:hover {\n",
        "  background: var(--bg-card-hover);\n",
        "  transform: translateX(5px);\n",
        "}\n",
        "\n",
        ".step-badge {\n",
        "  width: 40px;\n",
        "  height: 40px;\n",
        "  flex-shrink: 0;\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  justify-content: center;\n",
        "  background: var(--gradient-primary);\n",
        "  border-radius: 50%;\n",
        "  font-weight: 700;\n",
        "  color: white;\n",
        "}\n",
        "\n",
        ".step-info h4 {\n",
        "  font-size: 1rem;\n",
        "  margin-bottom: 0.5rem;\n",
        "  color: var(--text-primary);\n",
        "}\n",
        "\n",
        ".step-info p {\n",
        "  font-size: 0.875rem;\n",
        "  color: var(--text-secondary);\n",
        "  line-height: 1.6;\n",
        "}\n",
        "\n",
        "/* Metrics Box */\n",
        ".metrics-box {\n",
        "  padding: var(--spacing-md);\n",
        "  background: var(--bg-secondary);\n",
        "  border-radius: var(--radius-md);\n",
        "  margin-top: var(--spacing-md);\n",
        "}\n",
        "\n",
        ".metrics-box h4 {\n",
        "  font-size: 1rem;\n",
        "  margin-bottom: var(--spacing-md);\n",
        "  color: var(--text-primary);\n",
        "  text-align: center;\n",
        "}\n",
        "\n",
        ".metrics-grid {\n",
        "  display: grid;\n",
        "  grid-template-columns: repeat(3, 1fr);\n",
        "  gap: var(--spacing-md);\n",
        "}\n",
        "\n",
        ".metric-item {\n",
        "  display: flex;\n",
        "  flex-direction: column;\n",
        "  align-items: center;\n",
        "  gap: 0.25rem;\n",
        "  padding: var(--spacing-sm);\n",
        "  background: var(--bg-card);\n",
        "  border-radius: var(--radius-sm);\n",
        "}\n",
        "\n",
        ".metric-label {\n",
        "  font-size: 0.75rem;\n",
        "  color: var(--text-muted);\n",
        "  font-weight: 500;\n",
        "  text-transform: uppercase;\n",
        "  letter-spacing: 1px;\n",
        "}\n",
        "\n",
        ".metric-value {\n",
        "  font-size: 1.25rem;\n",
        "  color: var(--accent-success);\n",
        "  font-weight: 700;\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   FOOTER\n",
        "   ========================================== */\n",
        "\n",
        ".footer {\n",
        "  position: relative;\n",
        "  z-index: 1;\n",
        "  background: var(--bg-secondary);\n",
        "  border-top: 1px solid var(--border-color);\n",
        "  padding: var(--spacing-lg) 0;\n",
        "  margin-top: var(--spacing-xl);\n",
        "}\n",
        "\n",
        ".footer-content {\n",
        "  max-width: 1400px;\n",
        "  margin: 0 auto;\n",
        "  padding: 0 var(--spacing-lg);\n",
        "  text-align: center;\n",
        "}\n",
        "\n",
        ".footer-text {\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  justify-content: center;\n",
        "  gap: var(--spacing-sm);\n",
        "  font-size: 1rem;\n",
        "  color: var(--text-secondary);\n",
        "  margin-bottom: 0.5rem;\n",
        "}\n",
        "\n",
        ".footer-icon {\n",
        "  font-size: 1.25rem;\n",
        "  animation: pulse 2s ease-in-out infinite;\n",
        "}\n",
        "\n",
        ".footer-subtext {\n",
        "  font-size: 0.875rem;\n",
        "  color: var(--text-muted);\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   ANIMATIONS\n",
        "   ========================================== */\n",
        "\n",
        "@keyframes fadeIn {\n",
        "  from {\n",
        "    opacity: 0;\n",
        "  }\n",
        "  to {\n",
        "    opacity: 1;\n",
        "  }\n",
        "}\n",
        "\n",
        "@keyframes fadeInUp {\n",
        "  from {\n",
        "    opacity: 0;\n",
        "    transform: translateY(30px);\n",
        "  }\n",
        "  to {\n",
        "    opacity: 1;\n",
        "    transform: translateY(0);\n",
        "  }\n",
        "}\n",
        "\n",
        "@keyframes fadeInLeft {\n",
        "  from {\n",
        "    opacity: 0;\n",
        "    transform: translateX(-30px);\n",
        "  }\n",
        "  to {\n",
        "    opacity: 1;\n",
        "    transform: translateX(0);\n",
        "  }\n",
        "}\n",
        "\n",
        "@keyframes fadeInRight {\n",
        "  from {\n",
        "    opacity: 0;\n",
        "    transform: translateX(30px);\n",
        "  }\n",
        "  to {\n",
        "    opacity: 1;\n",
        "    transform: translateX(0);\n",
        "  }\n",
        "}\n",
        "\n",
        "@keyframes slideInLeft {\n",
        "  from {\n",
        "    opacity: 0;\n",
        "    transform: translateX(-50px);\n",
        "  }\n",
        "  to {\n",
        "    opacity: 1;\n",
        "    transform: translateX(0);\n",
        "  }\n",
        "}\n",
        "\n",
        "@keyframes slideInRight {\n",
        "  from {\n",
        "    opacity: 0;\n",
        "    transform: translateX(50px);\n",
        "  }\n",
        "  to {\n",
        "    opacity: 1;\n",
        "    transform: translateX(0);\n",
        "  }\n",
        "}\n",
        "\n",
        "/* ==========================================\n",
        "   RESPONSIVE DESIGN\n",
        "   ========================================== */\n",
        "\n",
        "@media (max-width: 1024px) {\n",
        "  .hero-section {\n",
        "    grid-template-columns: 1fr;\n",
        "    gap: var(--spacing-lg);\n",
        "  }\n",
        "\n",
        "  .content-grid {\n",
        "    grid-template-columns: 1fr;\n",
        "  }\n",
        "\n",
        "  .comparison-container {\n",
        "    grid-template-columns: 1fr;\n",
        "  }\n",
        "\n",
        "  .comparison-arrow {\n",
        "    flex-direction: row;\n",
        "    justify-content: center;\n",
        "  }\n",
        "\n",
        "  .arrow-icon {\n",
        "    transform: rotate(90deg);\n",
        "  }\n",
        "}\n",
        "\n",
        "@media (max-width: 768px) {\n",
        "  .hero-title {\n",
        "    font-size: 2.5rem;\n",
        "  }\n",
        "\n",
        "  .hero-subtitle {\n",
        "    font-size: 1rem;\n",
        "  }\n",
        "\n",
        "  .section-title {\n",
        "    font-size: 2rem;\n",
        "  }\n",
        "\n",
        "  .page-title {\n",
        "    font-size: 2rem;\n",
        "  }\n",
        "\n",
        "  .nav-container {\n",
        "    flex-direction: column;\n",
        "    gap: var(--spacing-md);\n",
        "  }\n",
        "\n",
        "  .nav-links {\n",
        "    width: 100%;\n",
        "    justify-content: center;\n",
        "  }\n",
        "\n",
        "  .workflow-steps {\n",
        "    flex-direction: column;\n",
        "  }\n",
        "\n",
        "  .workflow-connector {\n",
        "    width: 2px;\n",
        "    height: 30px;\n",
        "    background: linear-gradient(180deg, var(--accent-primary), transparent);\n",
        "    margin: 0 auto;\n",
        "  }\n",
        "\n",
        "  .features-grid {\n",
        "    grid-template-columns: 1fr;\n",
        "  }\n",
        "\n",
        "  .details-grid {\n",
        "    grid-template-columns: 1fr;\n",
        "  }\n",
        "\n",
        "  .metrics-grid {\n",
        "    grid-template-columns: 1fr;\n",
        "  }\n",
        "\n",
        "  .action-section {\n",
        "    flex-direction: column;\n",
        "  }\n",
        "\n",
        "  .action-section .btn {\n",
        "    width: 100%;\n",
        "  }\n",
        "}\n",
        "\n",
        "@media (max-width: 480px) {\n",
        "  .main-container {\n",
        "    padding: var(--spacing-md);\n",
        "  }\n",
        "\n",
        "  .hero-title {\n",
        "    font-size: 2rem;\n",
        "  }\n",
        "\n",
        "  .page-title {\n",
        "    font-size: 1.75rem;\n",
        "  }\n",
        "\n",
        "  .section-title {\n",
        "    font-size: 1.5rem;\n",
        "  }\n",
        "\n",
        "  .flash-container {\n",
        "    right: var(--spacing-sm);\n",
        "    left: var(--spacing-sm);\n",
        "    max-width: none;\n",
        "  }\n",
        "\n",
        "  .upload-zone {\n",
        "    padding: var(--spacing-md);\n",
        "    min-height: 250px;\n",
        "  }\n",
        "\n",
        "  .upload-icon-large {\n",
        "    font-size: 3rem;\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6GolDdPDk-p"
      },
      "source": [
        "\ud83d\udd0c 9\ufe0f\u20e3 Kill Any Previous Servers + Run Flask\n",
        "\n",
        "## \ud83d\udd0c Restart Services & Launch Flask App\n",
        "\n",
        "This cleans any previous Flask/ngrok processes and launches:\n",
        "- Flask server (background)\n",
        "- ngrok public URL\n",
        "\n",
        "After running this section, you will receive a public link to your app.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blKBvadIE82w"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ===============================\n",
        "# 6\ufe0f\u20e3 Kill any previous processes\n",
        "# ===============================\n",
        "!pkill -f flask || echo \"No flask running\"\n",
        "!pkill -f ngrok || echo \"No ngrok running\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBszzkWpE8zV"
      },
      "outputs": [],
      "source": [
        "!lsof -i :8000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3bTqURlE8wz"
      },
      "outputs": [],
      "source": [
        "!kill -9 35515"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzCo-n2NFA_c"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 7\ufe0f\u20e3 Run Flask in the background\n",
        "# ===============================\n",
        "!nohup python app.py > flask.log 2>&1 &\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnKt2iulDk-q"
      },
      "source": [
        "\ud83c\udf0d \ud83d\udd1f Start ngrok Tunnel\n",
        "\n",
        "## \ud83c\udf0d Generate Public URL using ngrok\n",
        "\n",
        "Replace the token with your own ngrok token before running.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8lgkAhMFA8G"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 8\ufe0f\u20e3 Start ngrok tunnel\n",
        "# ===============================\n",
        "from pyngrok import ngrok, conf\n",
        "conf.get_default().auth_token = \"<YOUR_TOKEN_HERE>\"  # \ud83d\udd11 replace with your token\n",
        "\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"\ud83c\udf0d Public URL:\", public_url)\n",
        "\n",
        "# ===============================\n",
        "# 9\ufe0f\u20e3 Check logs (optional)\n",
        "# ===============================\n",
        "!sleep 3 && tail -n 20 flask.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYLuMe03Dk-q"
      },
      "source": [
        "\ud83d\udcc4 1\ufe0f\u20e31\ufe0f\u20e3 View Flask Logs\n",
        "\n",
        "## \ud83d\udcc4 Check Flask Logs (Optional)\n",
        "\n",
        "Useful for debugging if the app does not open or crashes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dH3RvBoXE5_I"
      },
      "outputs": [],
      "source": [
        "!tail -n 50 flask.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1yuzAQCMKmh"
      },
      "outputs": [],
      "source": []
    }
  ]
}