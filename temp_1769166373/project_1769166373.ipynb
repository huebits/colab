{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+7rGyoBog0678OjI0v8vC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVZ7LU1BWi1"
      },
      "source": [
        "# \ud83d\udc41\ufe0f Diabetic Retinopathy Detection using Agentic AI & Deep Learning\n",
        "\n",
        "This notebook trains a deep-learning model to classify:\n",
        "- **DR**   \u2192 Diabetic Retinopathy  \n",
        "- **No_DR** \u2192 Healthy retina\n",
        "\n",
        "Dataset used:\n",
        "Diagnosis of Diabetic Retinopathy  \n",
        "https://www.kaggle.com/datasets/pkdarabi/diagnosis-of-diabetic-retinopathy\n",
        "\n",
        "The notebook includes:\n",
        "\n",
        "\u2714 Dataset loading from Kaggle  \n",
        "\u2714 Train/Validation/Test directory structure  \n",
        "\u2714 Image augmentation  \n",
        "\u2714 CNN Model (custom)  \n",
        "\u2714 Performance evaluation + confusion matrix  \n",
        "\u2714 Model saving as `.h5` for deployment  \n",
        "\u2714 Instructions to download/export trained models  \n",
        "\n",
        "This is the **Kaggle training notebook**.  \n",
        "Later, you will integrate this trained model into your **Colab Web App**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL59AHy9BWi8"
      },
      "source": [
        "## \ud83d\udcc2 Dataset Usage Instructions (Kaggle)\n",
        "\n",
        "This dataset contains three folders:\n",
        "\n",
        "- train/\n",
        "- valid/\n",
        "- test/\n",
        "\n",
        "Each folder contains two classes:\n",
        "- DR     \u2192 Eyes with Diabetic Retinopathy  \n",
        "- No_DR  \u2192 Healthy retina images\n",
        "\n",
        "After adding the dataset to your Kaggle Notebook:\n",
        "\n",
        "1. Click **Add Data \u2192 Search \"Diagnosis of Diabetic Retinopathy\"**\n",
        "2. Attach the dataset.\n",
        "3. Kaggle automatically mounts it under:\n",
        "   /kaggle/input/diagnosis-of-diabetic-retinopathy/\n",
        "\n",
        "Inside this folder you will find:\n",
        "\n",
        "Diagnosis of Diabetic Retinopathy/\n",
        " \u251c\u2500\u2500 train/\n",
        " \u251c\u2500\u2500 valid/\n",
        " \u251c\u2500\u2500 test/\n",
        "\n",
        "You must set your `base_dir` accordingly:\n",
        "\n",
        "base_dir = '/kaggle/input/diagnosis-of-diabetic-retinopathy/Diagnosis of Diabetic Retinopathy'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpGyvXowBWi-"
      },
      "source": [
        "## \ud83d\uddbc\ufe0f Image Preprocessing & Augmentation\n",
        "\n",
        "We resize all images to **224 \u00d7 224** and normalize pixel values to **[0, 1]**.\n",
        "\n",
        "Training data uses augmentation:\n",
        "- rotation\n",
        "- zooming\n",
        "- shifting\n",
        "- brightness changes\n",
        "- horizontal flips\n",
        "\n",
        "Validation & Test sets use **only rescaling**, no augmentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-10-27T05:24:49.438138Z",
          "iopub.status.busy": "2025-10-27T05:24:49.437975Z",
          "iopub.status.idle": "2025-10-27T05:25:05.044089Z",
          "shell.execute_reply": "2025-10-27T05:25:05.043485Z",
          "shell.execute_reply.started": "2025-10-27T05:24:49.438123Z"
        },
        "id": "FqtKYrWTFvP_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools\n",
        "\n",
        "# Paths \u2014 adjust as per your folder structure\n",
        "base_dir = '/kaggle/input/diagnosis-of-diabetic-retinopathy/Diagnosis of Diabetic Retinopathy'   # change this to your root folder\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir   = os.path.join(base_dir, 'valid')\n",
        "test_dir  = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Image parameters\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH  = 224\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 2    # DR vs No_DR\n",
        "\n",
        "# Data generators with augmentation for training\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    shear_range=0.15,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen= ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# ## \ud83e\udde0 CNN Model Architecture\n",
        "\n",
        "# We build a custom Convolutional Neural Network with:\n",
        "\n",
        "# - 3 Convolutional blocks (32 \u2192 64 \u2192 128 filters)\n",
        "# - Batch Normalization after each conv layer\n",
        "# - Max Pooling to reduce spatial size\n",
        "# - Dense layer with 128 neurons\n",
        "# - Dropout (0.5) to prevent overfitting\n",
        "# - Sigmoid output (binary classification)\n",
        "\n",
        "# Optimizer: Adam\n",
        "# Loss: Binary Crossentropy\n",
        "# Metrics: Accuracy\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'     # since two classes\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=False    # for evaluation, keep ordering\n",
        ")\n",
        "\n",
        "print(\"Classes:\", train_generator.class_indices)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xs6s72pBWjH"
      },
      "source": [
        "## \ud83e\udde0 CNN Model Architecture\n",
        "\n",
        "We build a custom Convolutional Neural Network with:\n",
        "\n",
        "- 3 Convolutional blocks (32 \u2192 64 \u2192 128 filters)\n",
        "- Batch Normalization after each conv layer\n",
        "- Max Pooling to reduce spatial size\n",
        "- Dense layer with 128 neurons\n",
        "- Dropout (0.5) to prevent overfitting\n",
        "- Sigmoid output (binary classification)\n",
        "\n",
        "Optimizer: Adam  \n",
        "Loss: Binary Crossentropy  \n",
        "Metrics: Accuracy  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-27T05:25:15.416936Z",
          "iopub.status.busy": "2025-10-27T05:25:15.416384Z",
          "iopub.status.idle": "2025-10-27T05:25:17.249253Z",
          "shell.execute_reply": "2025-10-27T05:25:17.248508Z",
          "shell.execute_reply.started": "2025-10-27T05:25:15.416910Z"
        },
        "id": "uEgV5i7xFvQE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model_cnn = keras.Sequential([\n",
        "    layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "\n",
        "    layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_cnn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_cnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCi89yKoBWjJ"
      },
      "source": [
        "## \ud83d\udcc9 Learning Rate Scheduler\n",
        "\n",
        "We use ReduceLROnPlateau to automatically lower the LR when validation loss stagnates:\n",
        "\n",
        "- factor = 0.2  \n",
        "- patience = 2  \n",
        "- min_lr = 1e-6  \n",
        "\n",
        "This helps stabilize training and improve convergence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-27T05:25:19.993919Z",
          "iopub.status.busy": "2025-10-27T05:25:19.993649Z",
          "iopub.status.idle": "2025-10-27T05:25:19.999878Z",
          "shell.execute_reply": "2025-10-27T05:25:19.999177Z",
          "shell.execute_reply.started": "2025-10-27T05:25:19.993901Z"
        },
        "id": "HLmWULNhFvQF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=2,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI_PehPZBWjL"
      },
      "source": [
        "## \ud83d\ude80 Model Training\n",
        "\n",
        "We train for **20 epochs** (can be increased for better accuracy).  \n",
        "Both training & validation accuracy will be plotted at the end if needed.\n",
        "\n",
        "Use GPU runtime on Kaggle for much faster training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-27T05:25:21.455211Z",
          "iopub.status.busy": "2025-10-27T05:25:21.454632Z",
          "iopub.status.idle": "2025-10-27T05:35:19.445394Z",
          "shell.execute_reply": "2025-10-27T05:35:19.444604Z",
          "shell.execute_reply.started": "2025-10-27T05:25:21.455187Z"
        },
        "id": "M2JQXwR9FvQG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "history_cnn = model_cnn.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=20,              # increase epochs\n",
        "    callbacks=[reduce_lr]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AH0Mo-kBWjM"
      },
      "source": [
        "## \ud83d\udcca Model Evaluation (Test Set)\n",
        "\n",
        "We evaluate the model on the test set to measure:\n",
        "\n",
        "- Precision\n",
        "- Recall\n",
        "- F1 Score\n",
        "- Overall Accuracy\n",
        "\n",
        "We also compute a **confusion matrix** to visualize correct vs incorrect predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-27T05:42:01.224983Z",
          "iopub.status.busy": "2025-10-27T05:42:01.224283Z",
          "iopub.status.idle": "2025-10-27T05:42:02.280199Z",
          "shell.execute_reply": "2025-10-27T05:42:02.279529Z",
          "shell.execute_reply.started": "2025-10-27T05:42:01.224955Z"
        },
        "id": "db6EghnrFvQG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "# Get predictions\n",
        "test_steps = test_generator.samples // BATCH_SIZE + 1\n",
        "pred_probs = model_cnn.predict(test_generator, steps=test_steps)\n",
        "pred_labels = (pred_probs > 0.5).astype(int).reshape(-1)\n",
        "\n",
        "true_labels = test_generator.classes  # since shuffle=False\n",
        "class_names = list(test_generator.class_indices.keys())\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(true_labels, pred_labels, target_names=class_names))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Purples')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"CNN Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOYVz1LUBWjN"
      },
      "source": [
        "## \ud83d\udcbe Saving the Trained Model (for Colab Web App)\n",
        "\n",
        "The final model is saved in .h5 format:\n",
        "\n",
        "model_cnn.save('cnn_advanced_model.h5')\n",
        "\n",
        "This file will appear in:\n",
        "- /kaggle/working/cnn_advanced_model.h5\n",
        "\n",
        "### \ud83d\udce5 Downloading from Kaggle:\n",
        "\n",
        "Go to the right sidebar \u2192 \"Output Files\" \u2192 download cnn_advanced_model.h5\n",
        "\n",
        "### \ud83d\udccc This model will be used later in the **Colab Web App**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-27T05:43:49.223765Z",
          "iopub.status.busy": "2025-10-27T05:43:49.222887Z",
          "iopub.status.idle": "2025-10-27T05:43:49.766651Z",
          "shell.execute_reply": "2025-10-27T05:43:49.765863Z",
          "shell.execute_reply.started": "2025-10-27T05:43:49.223739Z"
        },
        "id": "qsihMpsJFvQH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Save as HDF5\n",
        "model_cnn.save('cnn_advanced_model.h5')\n",
        "print(\"CNN model saved as cnn_advanced_model.h5\")\n",
        "\n",
        "# Load later\n",
        "loaded_cnn_h5 = load_model('cnn_advanced_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzKvTkqgFvQH",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VwESrwKqBgig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LjkbaXiQBgfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PPDYL6QtBgdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Q6jk-4tBgb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YOiD4y3gBgXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qLDnJYy7BgVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run this in Colab"
      ],
      "metadata": {
        "id": "-4BASw09BhIk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRh0oNBbBXl2"
      },
      "source": [
        "# \ud83d\udc41\ufe0f Diabetic Retinopathy Detection Web App (Agentic AI + Deep Learning)\n",
        "\n",
        "This notebook deploys a full AI-powered web application using:\n",
        "\n",
        "\u2714 Pre-trained CNN model from Kaggle (DR vs No_DR)  \n",
        "\u2714 Severity classification based on CNN feature extraction  \n",
        "\u2714 Mistral-7B-Instruct LLM (4-bit, GPU optimized)  \n",
        "\u2714 Personalized recovery steps using Agentic AI  \n",
        "\u2714 Flask + Ngrok for live public hosting  \n",
        "\n",
        "To run successfully:\n",
        "\n",
        "1. Upload your trained model file: **cnn_advanced_model.h5**\n",
        "2. Authenticate with Hugging Face (LLM access)\n",
        "3. Mount Google Drive\n",
        "4. Start Flask + Ngrok to launch the app\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz_O0jhwBXmG"
      },
      "source": [
        "## \ud83d\udce6 Install All Required Dependencies\n",
        "\n",
        "This installs:\n",
        "\n",
        "- Flask (backend)\n",
        "- PyNgrok (public URL for your app)\n",
        "- TensorFlow (loads the trained CNN model)\n",
        "- Transformers + BitsAndBytes (loads Mistral-7B 4-bit)\n",
        "- SentencePiece (required for tokenizer)\n",
        "\n",
        "These libraries enable:\n",
        "- Model loading\n",
        "- Image preprocessing\n",
        "- LLM generation\n",
        "- Web UI execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3dlTHo23fXF"
      },
      "outputs": [],
      "source": [
        "!pip install -q flask pyngrok tensorflow pillow requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr_oIFoiEiA7"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes sentencepiece\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egfwsBwsBXmM"
      },
      "source": [
        "## \ud83d\udcc2 Connect Google Drive\n",
        "\n",
        "Your trained CNN model (cnn_advanced_model.h5) must be stored in Drive.\n",
        "\n",
        "Steps:\n",
        "1. Upload the model into any folder inside MyDrive\n",
        "2. Update the model path in the code if needed\n",
        "3. Verify the model using `!ls` command\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi5MmlovIZf2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E53H4cGIZca"
      },
      "outputs": [],
      "source": [
        "# Verify model file path\n",
        "!ls \"/content/drive/MyDrive/Colab Notebooks/Varma sir projects/Diabetic Retinopathy/cnn_advanced_model.h5\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w21vBUg1BXmQ"
      },
      "source": [
        "## \ud83d\udd11 Hugging Face Token Setup (Required)\n",
        "\n",
        "The LLM (Mistral-7B) is hosted on HuggingFace.\n",
        "\n",
        "To use it:\n",
        "1. Go to: https://huggingface.co/settings/tokens\n",
        "2. Click \"New Token\"\n",
        "3. Select: Permission = Read\n",
        "4. Copy the token\n",
        "5. Paste token into: login(token=\"YOUR_TOKEN\")\n",
        "\n",
        "\u26a0\ufe0f Never share your token publicly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQ3e8L666ZvI"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"YOUR_TOKEN\")   # Replace with your HF token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ5WnM35BXmT"
      },
      "source": [
        "## \ud83d\udcc1 Create App Structure (Templates, Static, Uploads)\n",
        "\n",
        "Flask requires project folders:\n",
        "- templates \u2192 HTML files\n",
        "- static \u2192 CSS files\n",
        "- uploads \u2192 where user images go\n",
        "\n",
        "These directories will be auto-created.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zyx77_5W7Z7Y"
      },
      "outputs": [],
      "source": [
        "!mkdir -p templates static uploads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpADvsf4BXmU"
      },
      "source": [
        "## \ud83d\udd10 Store HF Token in Environment Variable\n",
        "\n",
        "To allow the LLM to authenticate internally,\n",
        "we export the HuggingFace token as HF_TOKEN.\n",
        "\n",
        "This avoids hardcoding the token inside model loading code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4R7LuBx797ex"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"your token\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGM64LyJBXmV"
      },
      "source": [
        "# \ud83e\udde0 App Backend Overview (app.py)\n",
        "\n",
        "The backend performs:\n",
        "\n",
        "1. Load pretrained CNN model from Drive\n",
        "2. Build feature extractor for advanced severity analysis\n",
        "3. Preprocess uploaded images\n",
        "4. Predict:\n",
        "   - DR vs No_DR\n",
        "   - Confidence score\n",
        "5. Extract numerical retina feature patterns\n",
        "6. Severity classification (Early \u2192 Severe)\n",
        "7. Use LLM (Mistral-7B) to generate personalized recovery plans\n",
        "8. Return full JSON response to frontend\n",
        "\n",
        "\u26a0\ufe0f Note:\n",
        "Some layers in CNN may differ. If feature extraction fails,\n",
        "fallback measures are automatically used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdDoTVMmIZZ5"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from flask import Flask, request, render_template, jsonify\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# ## \ud83e\udde0 Load Trained DR Classification Model\n",
        "\n",
        "# We load the model exported from Kaggle:\n",
        "# cnn_advanced_model.h5\n",
        "\n",
        "# The model path:\n",
        "# \"/content/drive/MyDrive/.../cnn_advanced_model.h5\"\n",
        "\n",
        "# Make sure:\n",
        "# \u2714 The path is correct\n",
        "# \u2714 The file name matches\n",
        "# \u2714 Model is inside Google Drive\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# LOAD RETINOPATHY CNN MODEL\n",
        "# ----------------------------------------------------------\n",
        "model_path = \"/content/drive/MyDrive/Colab Notebooks/Varma sir projects/Diabetic Retinopathy/cnn_advanced_model.h5\"\n",
        "base_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Build the model by passing a dummy input to establish the computational graph\n",
        "dummy_input = np.zeros((1, 224, 224, 3), dtype=np.float32)\n",
        "_ = base_model.predict(dummy_input, verbose=0)\n",
        "\n",
        "# Now create feature extractor from intermediate layer\n",
        "try:\n",
        "    # Try to get the layer before the final dense layers\n",
        "    feature_layer = base_model.layers[-4]  # Adjust if needed\n",
        "    feature_extractor = Model(\n",
        "        inputs=base_model.input,\n",
        "        outputs=[feature_layer.output, base_model.output]\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not create feature extractor: {e}\")\n",
        "    print(\"Using base model only for predictions\")\n",
        "    feature_extractor = None\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# IMAGE PREPROCESSOR\n",
        "# ----------------------------------------------------------\n",
        "def preprocess_image(image):\n",
        "    image = image.resize((224, 224))\n",
        "    image = np.array(image) / 255.0\n",
        "    return np.expand_dims(image, axis=0)\n",
        "\n",
        "\n",
        "# ## \ud83d\udd2c AI Retina Feature Extraction\n",
        "\n",
        "# We extract additional signals beyond the basic prediction:\n",
        "# - activation strength\n",
        "# - pattern complexity\n",
        "# - max feature response\n",
        "# - affected_area_ratio\n",
        "\n",
        "# These features help determine:\n",
        "# \u2714 severity\n",
        "# \u2714 depth of abnormalities\n",
        "# \u2714 AI confidence reasoning\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# EXTRACT VISUAL FEATURES FOR CONTEXT\n",
        "# ----------------------------------------------------------\n",
        "def analyze_image_features(input_arr):\n",
        "    \"\"\"\n",
        "    Extract features from the CNN to understand what patterns the model detected.\n",
        "    This gives us context about the severity and characteristics of DR.\n",
        "    \"\"\"\n",
        "    if feature_extractor is None:\n",
        "        # Fallback: use simple variance analysis on the input image\n",
        "        img_variance = np.var(input_arr)\n",
        "        img_mean = np.mean(input_arr)\n",
        "        return {\n",
        "            'activation_strength': float(img_mean),\n",
        "            'pattern_complexity': float(img_variance),\n",
        "            'max_response': 1.0,\n",
        "            'affected_area_ratio': 0.5\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        features, prediction = feature_extractor.predict(input_arr, verbose=0)\n",
        "\n",
        "        # Analyze feature activation patterns\n",
        "        feature_mean = np.mean(features)\n",
        "        feature_std = np.std(features)\n",
        "        feature_max = np.max(features)\n",
        "        high_activation_ratio = np.sum(features > 0.5) / features.size\n",
        "\n",
        "        return {\n",
        "            'activation_strength': float(feature_mean),\n",
        "            'pattern_complexity': float(feature_std),\n",
        "            'max_response': float(feature_max),\n",
        "            'affected_area_ratio': float(high_activation_ratio)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error in feature extraction: {e}\")\n",
        "        # Fallback values\n",
        "        return {\n",
        "            'activation_strength': 0.4,\n",
        "            'pattern_complexity': 0.3,\n",
        "            'max_response': 0.8,\n",
        "            'affected_area_ratio': 0.45\n",
        "        }\n",
        "\n",
        "\n",
        "# ## \ud83e\udd16 Load Mistral-7B-Instruct (4-bit)\n",
        "\n",
        "# The LLM is used to:\n",
        "# - interpret AI findings\n",
        "# - generate personalized recovery steps\n",
        "# - produce structured recommendations\n",
        "\n",
        "# We use BitsAndBytesConfig to load the model in 4-bit for speed + memory optimization.\n",
        "\n",
        "# Requirements:\n",
        "# \u2714 GPU runtime\n",
        "# \u2714 HF token set\n",
        "# \u2714 Internet on first model load\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# LOAD LLM (Mistral-7B-Instruct, 4-bit, GPU)\n",
        "# ----------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "bnb = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "print(\"Loading LLM tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "print(\"Loading LLM model (this may take a minute)...\")\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "print(\"LLM loaded successfully!\")\n",
        "\n",
        "\n",
        "# ## \u26a0\ufe0f Severity Classification Engine\n",
        "\n",
        "# Based on:\n",
        "# - CNN confidence\n",
        "# - Activation intensity\n",
        "# - Pattern complexity\n",
        "# - Affected retinal area\n",
        "\n",
        "# We classify DR as:\n",
        "# - Early/Mild\n",
        "# - Moderate\n",
        "# - Moderately Severe\n",
        "# - Severe/Advanced\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# ENHANCED SEVERITY CLASSIFICATION\n",
        "# ----------------------------------------------------------\n",
        "def classify_severity(confidence, features):\n",
        "    \"\"\"\n",
        "    Multi-dimensional severity assessment using both prediction confidence\n",
        "    and extracted visual features from the CNN.\n",
        "    \"\"\"\n",
        "    activation = features['activation_strength']\n",
        "    complexity = features['pattern_complexity']\n",
        "    affected_ratio = features['affected_area_ratio']\n",
        "\n",
        "    # Weighted severity score\n",
        "    severity_score = (\n",
        "        confidence * 0.4 +\n",
        "        min(activation * 2, 1.0) * 0.25 +  # Normalized activation\n",
        "        min(complexity * 3, 1.0) * 0.15 +   # Normalized complexity\n",
        "        affected_ratio * 0.2\n",
        "    )\n",
        "\n",
        "    if severity_score < 0.65:\n",
        "        return \"Early/Mild\", \"mild changes detected\"\n",
        "    elif severity_score < 0.80:\n",
        "        return \"Moderate\", \"moderate retinal changes observed\"\n",
        "    elif severity_score < 0.92:\n",
        "        return \"Moderately Severe\", \"significant retinal abnormalities detected\"\n",
        "    else:\n",
        "        return \"Severe/Advanced\", \"advanced retinal damage patterns identified\"\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# POST-PROCESS HELPERS\n",
        "# ----------------------------------------------------------\n",
        "def extract_bullets(text, max_bullets=10):\n",
        "    \"\"\"Extract clean bullet points from LLM output\"\"\"\n",
        "    t = text.strip()\n",
        "\n",
        "    # Remove instruction artifacts\n",
        "    t = re.sub(r\"^\\[?/?INST\\]?[:\\s]*\", \"\", t, flags=re.IGNORECASE).strip()\n",
        "    t = re.sub(r\"^(response|answer|here are|steps)\\s*:?\", \"\", t, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]\n",
        "    bullets = []\n",
        "\n",
        "    for ln in lines:\n",
        "        if re.match(r\"^(\\d+[\\.\\)]\\s+|-|\u2022|\\*)\", ln):\n",
        "            clean = re.sub(r\"^(\\d+[\\.\\)]\\s+|-|\u2022|\\*)\\s*\", \"\", ln).strip()\n",
        "            if len(clean) > 15:  # Skip very short items\n",
        "                bullets.append(clean)\n",
        "        elif len(ln) > 20 and not re.search(r\"(instruction|requirement|context)\", ln.lower()):\n",
        "            bullets.append(ln)\n",
        "\n",
        "        if len(bullets) >= max_bullets:\n",
        "            break\n",
        "\n",
        "    # If not enough bullets, split longer items\n",
        "    if len(bullets) < 6:\n",
        "        expanded = []\n",
        "        for item in bullets:\n",
        "            if ';' in item or ', and ' in item:\n",
        "                parts = re.split(r';\\s*|,\\s*and\\s+', item)\n",
        "                expanded.extend([p.strip() for p in parts if len(p.strip()) > 15])\n",
        "            else:\n",
        "                expanded.append(item)\n",
        "        bullets = expanded[:max_bullets]\n",
        "\n",
        "    # Deduplicate\n",
        "    seen = set()\n",
        "    final = []\n",
        "    for b in bullets:\n",
        "        b_lower = b.lower()\n",
        "        if b_lower not in seen and len(b) > 15:\n",
        "            final.append(b)\n",
        "            seen.add(b_lower)\n",
        "\n",
        "    return final[:max_bullets]\n",
        "\n",
        "# # \ud83e\ude7a Personalized Recovery Plan (Agentic AI)\n",
        "\n",
        "# The LLM uses image-derived biomarkers + severity label to generate:\n",
        "\n",
        "# \u2714 Medical steps\n",
        "# \u2714 Diagnostic tests required\n",
        "# \u2714 Lifestyle recommendations\n",
        "# \u2714 Warning signs\n",
        "# \u2714 Follow-up frequency\n",
        "# \u2714 Evidence-based treatment suggestions\n",
        "\n",
        "# This section produces a highly personalized explanation for each patient.\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# AGENTIC AI GENERATION WITH IMAGE CONTEXT\n",
        "# ----------------------------------------------------------\n",
        "def generate_recovery_steps(confidence, features, severity_label, severity_desc):\n",
        "    \"\"\"\n",
        "    Generate personalized recovery steps using actual image analysis context.\n",
        "    Each image will produce unique recommendations based on detected patterns.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create detailed context from feature analysis\n",
        "    activation = features['activation_strength']\n",
        "    complexity = features['pattern_complexity']\n",
        "    affected_ratio = features['affected_area_ratio'] * 100\n",
        "\n",
        "    # Determine focus areas based on detected patterns\n",
        "    focus_areas = []\n",
        "    if affected_ratio > 40:\n",
        "        focus_areas.append(\"widespread retinal involvement\")\n",
        "    if complexity > 0.3:\n",
        "        focus_areas.append(\"complex vascular changes\")\n",
        "    if activation > 0.4:\n",
        "        focus_areas.append(\"significant neural activation patterns\")\n",
        "\n",
        "    focus_text = \", \".join(focus_areas) if focus_areas else \"early retinal changes\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "You are an ophthalmology AI assistant. Generate specific, actionable recovery and management steps for a diabetic retinopathy patient.\n",
        "\n",
        "DETECTION DETAILS:\n",
        "- Severity Classification: {severity_label}\n",
        "- Pattern Analysis: {severity_desc}\n",
        "- AI Confidence: {confidence:.1%}\n",
        "- Affected Retinal Area: ~{affected_ratio:.1f}%\n",
        "- Detected Patterns: {focus_text}\n",
        "\n",
        "GENERATE 8-10 PERSONALIZED RECOMMENDATIONS:\n",
        "Focus on these categories based on the severity:\n",
        "1. Immediate medical actions (specific to {severity_label} cases)\n",
        "2. Essential diagnostic tests needed for this severity level\n",
        "3. Blood sugar management strategies\n",
        "4. Eye-specific care routines\n",
        "5. Lifestyle modifications relevant to detected patterns\n",
        "6. Warning signs to monitor\n",
        "7. Follow-up schedule appropriate for {severity_label} DR\n",
        "8. Treatment options typically used for this stage\n",
        "\n",
        "REQUIREMENTS:\n",
        "- Be specific and actionable (avoid generic advice)\n",
        "- Tailor recommendations to {severity_label} severity\n",
        "- Use clear, supportive language\n",
        "- Include specific test names (Fundus photography, OCT, Fluorescein angiography)\n",
        "- Mention realistic timeframes for follow-ups\n",
        "- Address both medical and lifestyle factors\n",
        "- End with: \"This is AI-assisted analysis, not a diagnosis. Consult an ophthalmologist immediately.\"\n",
        "\n",
        "Provide ONLY the numbered list of recommendations, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "    final_prompt = f\"<s>[INST] {user_prompt.strip()} [/INST]\"\n",
        "\n",
        "    try:\n",
        "        inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(llm_model.device)\n",
        "        input_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "        outputs = llm_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=280,\n",
        "            temperature=0.7,  # Increased for more varied outputs\n",
        "            top_p=0.92,\n",
        "            top_k=50,\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.2,  # Reduce repetition\n",
        "            eos_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 2,\n",
        "        )\n",
        "\n",
        "        out_ids = outputs[0]\n",
        "        gen_ids = out_ids[input_len:] if out_ids.shape[0] > input_len else out_ids\n",
        "        gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "        bullets = extract_bullets(gen_text, max_bullets=10)\n",
        "\n",
        "        if not bullets or len(bullets) < 3:\n",
        "            # Fallback with contextual steps\n",
        "            bullets = generate_fallback_steps(severity_label, affected_ratio)\n",
        "\n",
        "        formatted = \"\\n\".join([f\"{i+1}. {b}\" for i, b in enumerate(bullets)])\n",
        "\n",
        "        if \"not a diagnosis\" not in formatted.lower() and \"ai-assisted\" not in formatted.lower():\n",
        "            formatted += \"\\n\\n\u26a0\ufe0f This is AI-assisted analysis, not a diagnosis. Consult an ophthalmologist immediately.\"\n",
        "\n",
        "        return formatted\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"LLM generation error: {e}\")\n",
        "        return generate_fallback_steps(severity_label, affected_ratio)\n",
        "\n",
        "\n",
        "def generate_fallback_steps(severity, affected_ratio):\n",
        "    \"\"\"Fallback steps if LLM generation fails\"\"\"\n",
        "    steps = [\n",
        "        f\"Schedule an urgent appointment with a retinal specialist for {severity.lower()} diabetic retinopathy evaluation\",\n",
        "        \"Get a comprehensive dilated fundus examination within the next 1-2 weeks\",\n",
        "        f\"Request Optical Coherence Tomography (OCT) scan to assess macular involvement (approximately {affected_ratio:.0f}% area affected)\",\n",
        "        \"Monitor blood glucose levels 4 times daily and maintain HbA1c below 7.0%\",\n",
        "        \"Check blood pressure twice daily; target <130/80 mmHg to reduce progression risk\",\n",
        "        \"Begin strict dietary control: limit refined carbs, increase omega-3 rich foods\",\n",
        "        \"Implement daily moderate exercise (30-45 mins walking) after physician clearance\",\n",
        "        \"Watch for warning signs: sudden vision loss, floaters, flashing lights, dark spots\",\n",
        "        f\"For {severity.lower()} cases, expect follow-up every 3-4 months or as advised\",\n",
        "        \"Discuss laser photocoagulation or anti-VEGF therapy options with your ophthalmologist\"\n",
        "    ]\n",
        "\n",
        "    formatted = \"\\n\".join([f\"{i+1}. {s}\" for i, s in enumerate(steps)])\n",
        "    formatted += \"\\n\\n\u26a0\ufe0f This is AI-assisted analysis, not a diagnosis. Consult an ophthalmologist immediately.\"\n",
        "    return formatted\n",
        "\n",
        "# ## \ud83c\udf10 Flask Routes\n",
        "\n",
        "# Endpoints:\n",
        "\n",
        "# / \u2192 Welcome page\n",
        "# /upload \u2192 Upload image UI\n",
        "# /predict \u2192 API endpoint (POST image)\n",
        "\n",
        "# The predict route returns:\n",
        "# - Prediction label\n",
        "# - Confidence\n",
        "# - Severity\n",
        "# - Affected area\n",
        "# - Recovery steps (AI generated)\n",
        "# - Disclaimer\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# ROUTES\n",
        "# ----------------------------------------------------------\n",
        "@app.route(\"/\")\n",
        "def welcome():\n",
        "    return render_template(\"welcome.html\")\n",
        "\n",
        "\n",
        "@app.route(\"/upload\")\n",
        "def upload():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "def predict():\n",
        "    if \"image\" not in request.files:\n",
        "        return jsonify({\"error\": \"No image uploaded\"}), 400\n",
        "\n",
        "    try:\n",
        "        file = request.files[\"image\"]\n",
        "        image = Image.open(file).convert(\"RGB\")\n",
        "        input_arr = preprocess_image(image)\n",
        "\n",
        "        # Get prediction\n",
        "        pred = float(base_model.predict(input_arr, verbose=0)[0][0])\n",
        "\n",
        "        label = \"No_DR\" if pred > 0.5 else \"DR\"\n",
        "        confidence = pred if label == \"No_DR\" else (1 - pred)\n",
        "\n",
        "        response = {\n",
        "            \"label_raw\": label,\n",
        "            \"confidence\": round(confidence, 4)\n",
        "        }\n",
        "\n",
        "        if label == \"No_DR\":\n",
        "            response[\"prediction\"] = \"\ud83d\udc41\ufe0f\u2728 Retina appears healthy with no signs of diabetic retinopathy detected.\"\n",
        "            response[\"message\"] = \"Continue regular eye checkups and maintain good glucose control.\"\n",
        "        else:\n",
        "            # Extract detailed features for DR cases\n",
        "            features = analyze_image_features(input_arr)\n",
        "            severity_label, severity_desc = classify_severity(confidence, features)\n",
        "\n",
        "            response[\"prediction\"] = f\"\u26a0\ufe0f Diabetic Retinopathy Detected: {severity_label} stage\"\n",
        "            response[\"severity\"] = severity_label\n",
        "            response[\"severity_description\"] = severity_desc.capitalize()\n",
        "            response[\"affected_area\"] = f\"{features['affected_area_ratio']*100:.1f}%\"\n",
        "\n",
        "            # Generate personalized steps\n",
        "            steps = generate_recovery_steps(confidence, features, severity_label, severity_desc)\n",
        "            response[\"recovery_steps\"] = steps\n",
        "            response[\"disclaimer\"] = \"\u26a0\ufe0f This is AI-assisted analysis. Immediate consultation with a licensed ophthalmologist is strongly recommended.\"\n",
        "\n",
        "        return jsonify(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_details = traceback.format_exc()\n",
        "        print(f\"Error in predict route: {error_details}\")\n",
        "        return jsonify({\"error\": f\"Processing error: {str(e)}\"}), 500\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# RUN FLASK\n",
        "# ----------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\ud83d\ude80 Flask app starting...\")\n",
        "    print(\"=\" * 60)\n",
        "    app.run(host=\"0.0.0.0\", port=8000, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2xhdf1yBXma"
      },
      "source": [
        "## \ud83c\udfa8 Frontend Templates\n",
        "\n",
        "HTML templates include:\n",
        "\n",
        "1. welcome.html \u2192 Landing page  \n",
        "2. index.html \u2192 Image upload + analysis interface  \n",
        "3. style.css \u2192 Visual styling  \n",
        "\n",
        "These files work automatically with Flask.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj0uabiJQ5-b"
      },
      "outputs": [],
      "source": [
        "%%writefile templates/index.html\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "  <meta charset=\"UTF-8\" />\n",
        "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "  <title>DR Detection - Upload</title>\n",
        "  <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\" />\n",
        "</head>\n",
        "\n",
        "<body>\n",
        "  <div class=\"header\">\n",
        "    <h1>\ud83d\udc41\ufe0f Diabetic Retinopathy Detection</h1>\n",
        "    <p class=\"tagline\">AI-Powered Retinal Analysis</p>\n",
        "  </div>\n",
        "\n",
        "  <div class=\"container\">\n",
        "    <div class=\"upload-section\">\n",
        "      <h2>Upload Retinal Image</h2>\n",
        "      <p class=\"instruction\">Select a fundus photograph for analysis</p>\n",
        "\n",
        "      <form id=\"uploadForm\" onsubmit=\"uploadImage(event)\">\n",
        "        <div class=\"file-input-wrapper\">\n",
        "          <input type=\"file\" id=\"image\" name=\"image\" accept=\"image/*\" required onchange=\"previewImage(event)\" />\n",
        "          <label for=\"image\" class=\"file-label\">\n",
        "            <span id=\"fileName\">Choose Image File</span>\n",
        "            <span class=\"upload-icon\">\ud83d\udcc1</span>\n",
        "          </label>\n",
        "        </div>\n",
        "\n",
        "        <div id=\"imagePreview\" class=\"image-preview\"></div>\n",
        "\n",
        "        <button type=\"submit\" class=\"submit-btn\" id=\"submitBtn\">\n",
        "          <span class=\"btn-text\">\ud83d\udd0d Analyze Image</span>\n",
        "        </button>\n",
        "      </form>\n",
        "    </div>\n",
        "\n",
        "    <div id=\"resultsSection\" class=\"results-section hidden\">\n",
        "      <div class=\"result-header\">\n",
        "        <h2>Analysis Results</h2>\n",
        "      </div>\n",
        "\n",
        "      <div id=\"predictionResult\" class=\"result-content\"></div>\n",
        "    </div>\n",
        "  </div>\n",
        "\n",
        "  <script>\n",
        "    function previewImage(event) {\n",
        "      const file = event.target.files[0];\n",
        "      const preview = document.getElementById('imagePreview');\n",
        "      const fileName = document.getElementById('fileName');\n",
        "\n",
        "      if (file) {\n",
        "        fileName.textContent = file.name;\n",
        "        const reader = new FileReader();\n",
        "        reader.onload = function(e) {\n",
        "          preview.innerHTML = `<img src=\"${e.target.result}\" alt=\"Preview\" />`;\n",
        "          preview.style.display = 'block';\n",
        "        };\n",
        "        reader.readAsDataURL(file);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    function uploadImage(event) {\n",
        "      event.preventDefault();\n",
        "\n",
        "      const file = document.getElementById(\"image\").files[0];\n",
        "      if (!file) {\n",
        "        alert(\"Please select an image file\");\n",
        "        return;\n",
        "      }\n",
        "\n",
        "      const fd = new FormData();\n",
        "      fd.append(\"image\", file);\n",
        "\n",
        "      const resultsSection = document.getElementById(\"resultsSection\");\n",
        "      const resultDiv = document.getElementById(\"predictionResult\");\n",
        "      const submitBtn = document.getElementById(\"submitBtn\");\n",
        "\n",
        "      // Show loading state\n",
        "      submitBtn.disabled = true;\n",
        "      submitBtn.innerHTML = '<span class=\"btn-text\">\ud83d\udd04 Analyzing...</span>';\n",
        "      resultsSection.classList.remove('hidden');\n",
        "      resultDiv.innerHTML = `\n",
        "        <div class=\"loading\">\n",
        "          <div class=\"spinner\"></div>\n",
        "          <p>Processing retinal image...</p>\n",
        "          <p class=\"loading-sub\">Running AI analysis</p>\n",
        "        </div>\n",
        "      `;\n",
        "\n",
        "      fetch(\"/predict\", { method: \"POST\", body: fd })\n",
        "        .then(res => res.json())\n",
        "        .then(data => {\n",
        "          if (data.error) {\n",
        "            resultDiv.innerHTML = `<div class=\"error\">\u274c Error: ${data.error}</div>`;\n",
        "            return;\n",
        "          }\n",
        "\n",
        "          let html = `<div class=\"result-card\">`;\n",
        "\n",
        "          // Main prediction\n",
        "          html += `<div class=\"prediction-main\">\n",
        "            <h3>Diagnosis</h3>\n",
        "            <p class=\"prediction-text\">${data.prediction}</p>\n",
        "            <div class=\"confidence-bar\">\n",
        "              <div class=\"confidence-label\">AI Confidence</div>\n",
        "              <div class=\"progress-bar\">\n",
        "                <div class=\"progress-fill\" style=\"width: ${(data.confidence * 100).toFixed(1)}%\"></div>\n",
        "              </div>\n",
        "              <div class=\"confidence-value\">${(data.confidence * 100).toFixed(1)}%</div>\n",
        "            </div>\n",
        "          </div>`;\n",
        "\n",
        "          // If DR detected, show detailed analysis\n",
        "          if (data.recovery_steps) {\n",
        "            html += `<div class=\"severity-info\">\n",
        "              <div class=\"severity-badge severity-${data.severity.toLowerCase().replace(/\\s+/g, '-')}\">\n",
        "                ${data.severity}\n",
        "              </div>\n",
        "              <p class=\"severity-desc\">${data.severity_description}</p>\n",
        "              <p class=\"affected-area\">Affected Area: <strong>${data.affected_area}</strong></p>\n",
        "            </div>`;\n",
        "\n",
        "            html += `<div class=\"recovery-section\">\n",
        "              <h3>\ud83d\udccb Recommended Care Steps</h3>\n",
        "              <div class=\"steps-container\">\n",
        "                <pre class=\"steps-text\">${data.recovery_steps}</pre>\n",
        "              </div>\n",
        "            </div>`;\n",
        "\n",
        "            html += `<div class=\"disclaimer\">\n",
        "              ${data.disclaimer}\n",
        "            </div>`;\n",
        "          } else {\n",
        "            html += `<div class=\"healthy-message\">\n",
        "              <p>${data.message}</p>\n",
        "            </div>`;\n",
        "          }\n",
        "\n",
        "          html += `</div>`;\n",
        "\n",
        "          resultDiv.innerHTML = html;\n",
        "          submitBtn.disabled = false;\n",
        "          submitBtn.innerHTML = '<span class=\"btn-text\">\ud83d\udd0d Analyze Another Image</span>';\n",
        "        })\n",
        "        .catch(err => {\n",
        "          resultDiv.innerHTML = `<div class=\"error\">\u274c Network Error: ${err.message}</div>`;\n",
        "          submitBtn.disabled = false;\n",
        "          submitBtn.innerHTML = '<span class=\"btn-text\">\ud83d\udd0d Analyze Image</span>';\n",
        "        });\n",
        "    }\n",
        "  </script>\n",
        "\n",
        "</body>\n",
        "</html>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bxbwh15kIZXo"
      },
      "outputs": [],
      "source": [
        "%%writefile templates/welcome.html\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "  <meta charset=\"UTF-8\" />\n",
        "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "  <title>Welcome - DR Detection</title>\n",
        "  <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\" />\n",
        "</head>\n",
        "<body>\n",
        "  <div class=\"welcome-wrapper\">\n",
        "    <div class=\"welcome-container\">\n",
        "      <div class=\"welcome-icon\">\ud83d\udc41\ufe0f</div>\n",
        "      <h1 class=\"welcome-title\">Diabetic Retinopathy Detection System</h1>\n",
        "      <p class=\"welcome-subtitle\">\n",
        "        AI-Powered Early Detection for Vision Protection\n",
        "      </p>\n",
        "\n",
        "      <div class=\"feature-grid\">\n",
        "        <div class=\"feature-item\">\n",
        "          <div class=\"feature-icon\">\ud83d\udd2c</div>\n",
        "          <h3>Advanced CNN Analysis</h3>\n",
        "          <p>Deep learning model trained on thousands of retinal images</p>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"feature-item\">\n",
        "          <div class=\"feature-icon\">\ud83e\udd16</div>\n",
        "          <h3>Personalized Recommendations</h3>\n",
        "          <p>AI-generated care steps tailored to your specific condition</p>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"feature-item\">\n",
        "          <div class=\"feature-icon\">\u26a1</div>\n",
        "          <h3>Instant Results</h3>\n",
        "          <p>Get detailed analysis in seconds with confidence scoring</p>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"feature-item\">\n",
        "          <div class=\"feature-icon\">\ud83c\udfaf</div>\n",
        "          <h3>Severity Classification</h3>\n",
        "          <p>Multi-dimensional assessment of retinal damage patterns</p>\n",
        "        </div>\n",
        "      </div>\n",
        "\n",
        "      <div class=\"info-box\">\n",
        "        <h3>\ud83d\udccc About This Tool</h3>\n",
        "        <p>\n",
        "          This system uses state-of-the-art deep learning to analyze fundus photographs\n",
        "          for signs of diabetic retinopathy. Our CNN model extracts visual features and\n",
        "          classifies severity levels, while an AI assistant generates personalized care\n",
        "          recommendations based on detected patterns.\n",
        "        </p>\n",
        "        <p class=\"info-warning\">\n",
        "          \u26a0\ufe0f <strong>Important:</strong> This is a screening tool, not a diagnostic device.\n",
        "          Always consult qualified ophthalmologists for medical diagnosis and treatment.\n",
        "        </p>\n",
        "      </div>\n",
        "\n",
        "      <button onclick=\"location.href='/upload'\" class=\"welcome-btn\">\n",
        "        Begin Analysis \u2192\n",
        "      </button>\n",
        "\n",
        "      <div class=\"footer-note\">\n",
        "        <p>Powered by TensorFlow, Hugging Face Transformers & Mistral-7B</p>\n",
        "      </div>\n",
        "    </div>\n",
        "  </div>\n",
        "</body>\n",
        "</html>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWhNXW00IZVJ"
      },
      "outputs": [],
      "source": [
        "%%writefile static/style.css\n",
        "* {\n",
        "  margin: 0;\n",
        "  padding: 0;\n",
        "  box-sizing: border-box;\n",
        "}\n",
        "\n",
        "body {\n",
        "  font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;\n",
        "  background: linear-gradient(135deg, #0a1628 0%, #1a2f4d 50%, #0d1b2a 100%);\n",
        "  color: #e8f1f5;\n",
        "  min-height: 100vh;\n",
        "  line-height: 1.6;\n",
        "}\n",
        "\n",
        "/* ========== HEADER ========== */\n",
        ".header {\n",
        "  text-align: center;\n",
        "  padding: 2rem 1rem 1rem;\n",
        "  background: rgba(255, 255, 255, 0.03);\n",
        "  border-bottom: 2px solid rgba(90, 139, 214, 0.3);\n",
        "  margin-bottom: 2rem;\n",
        "}\n",
        "\n",
        ".header h1 {\n",
        "  font-size: 2.2rem;\n",
        "  margin-bottom: 0.5rem;\n",
        "  background: linear-gradient(135deg, #5a8bd6, #89c4f4);\n",
        "  -webkit-background-clip: text;\n",
        "  -webkit-text-fill-color: transparent;\n",
        "  background-clip: text;\n",
        "}\n",
        "\n",
        ".tagline {\n",
        "  font-size: 1rem;\n",
        "  color: #a8c5e0;\n",
        "  font-weight: 300;\n",
        "}\n",
        "\n",
        "/* ========== CONTAINER ========== */\n",
        ".container {\n",
        "  max-width: 900px;\n",
        "  margin: 0 auto;\n",
        "  padding: 0 1.5rem 3rem;\n",
        "}\n",
        "\n",
        "/* ========== UPLOAD SECTION ========== */\n",
        ".upload-section {\n",
        "  background: rgba(255, 255, 255, 0.05);\n",
        "  backdrop-filter: blur(10px);\n",
        "  border: 1px solid rgba(90, 139, 214, 0.3);\n",
        "  border-radius: 16px;\n",
        "  padding: 2.5rem;\n",
        "  margin-bottom: 2rem;\n",
        "  box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);\n",
        "}\n",
        "\n",
        ".upload-section h2 {\n",
        "  font-size: 1.8rem;\n",
        "  margin-bottom: 0.5rem;\n",
        "  color: #89c4f4;\n",
        "}\n",
        "\n",
        ".instruction {\n",
        "  color: #b8d4ea;\n",
        "  margin-bottom: 1.5rem;\n",
        "  font-size: 0.95rem;\n",
        "}\n",
        "\n",
        "/* ========== FILE INPUT ========== */\n",
        ".file-input-wrapper {\n",
        "  position: relative;\n",
        "  margin-bottom: 1.5rem;\n",
        "}\n",
        "\n",
        "input[type='file'] {\n",
        "  display: none;\n",
        "}\n",
        "\n",
        ".file-label {\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  justify-content: space-between;\n",
        "  width: 100%;\n",
        "  padding: 1.2rem 1.5rem;\n",
        "  background: linear-gradient(135deg, #1e3a5f, #2d4a6f);\n",
        "  border: 2px dashed #5a8bd6;\n",
        "  border-radius: 12px;\n",
        "  cursor: pointer;\n",
        "  transition: all 0.3s ease;\n",
        "  color: #c6d9ea;\n",
        "  font-size: 1rem;\n",
        "}\n",
        "\n",
        ".file-label:hover {\n",
        "  background: linear-gradient(135deg, #2d4a6f, #3a5a7f);\n",
        "  border-color: #89c4f4;\n",
        "  transform: translateY(-2px);\n",
        "}\n",
        "\n",
        ".upload-icon {\n",
        "  font-size: 1.5rem;\n",
        "}\n",
        "\n",
        "/* ========== IMAGE PREVIEW ========== */\n",
        ".image-preview {\n",
        "  display: none;\n",
        "  margin: 1.5rem 0;\n",
        "  text-align: center;\n",
        "}\n",
        "\n",
        ".image-preview img {\n",
        "  max-width: 100%;\n",
        "  max-height: 300px;\n",
        "  border-radius: 12px;\n",
        "  border: 2px solid #5a8bd6;\n",
        "  box-shadow: 0 4px 16px rgba(0, 0, 0, 0.4);\n",
        "}\n",
        "\n",
        "/* ========== SUBMIT BUTTON ========== */\n",
        ".submit-btn {\n",
        "  width: 100%;\n",
        "  padding: 1rem 2rem;\n",
        "  background: linear-gradient(135deg, #0066cc, #004c99);\n",
        "  color: white;\n",
        "  border: none;\n",
        "  border-radius: 10px;\n",
        "  font-size: 1.1rem;\n",
        "  font-weight: 600;\n",
        "  cursor: pointer;\n",
        "  transition: all 0.3s ease;\n",
        "  box-shadow: 0 4px 15px rgba(0, 102, 204, 0.4);\n",
        "}\n",
        "\n",
        ".submit-btn:hover:not(:disabled) {\n",
        "  background: linear-gradient(135deg, #0052a3, #003d7a);\n",
        "  transform: translateY(-2px);\n",
        "  box-shadow: 0 6px 20px rgba(0, 102, 204, 0.6);\n",
        "}\n",
        "\n",
        ".submit-btn:disabled {\n",
        "  opacity: 0.6;\n",
        "  cursor: not-allowed;\n",
        "}\n",
        "\n",
        "/* ========== RESULTS SECTION ========== */\n",
        ".results-section {\n",
        "  background: rgba(255, 255, 255, 0.05);\n",
        "  backdrop-filter: blur(10px);\n",
        "  border: 1px solid rgba(90, 139, 214, 0.3);\n",
        "  border-radius: 16px;\n",
        "  padding: 2rem;\n",
        "  box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);\n",
        "  animation: fadeIn 0.5s ease;\n",
        "}\n",
        "\n",
        ".results-section.hidden {\n",
        "  display: none;\n",
        "}\n",
        "\n",
        ".result-header h2 {\n",
        "  color: #89c4f4;\n",
        "  margin-bottom: 1.5rem;\n",
        "  font-size: 1.8rem;\n",
        "}\n",
        "\n",
        ".result-card {\n",
        "  background: rgba(255, 255, 255, 0.08);\n",
        "  border-radius: 12px;\n",
        "  padding: 1.5rem;\n",
        "}\n",
        "\n",
        "/* ========== PREDICTION DISPLAY ========== */\n",
        ".prediction-main {\n",
        "  margin-bottom: 2rem;\n",
        "}\n",
        "\n",
        ".prediction-main h3 {\n",
        "  color: #89c4f4;\n",
        "  margin-bottom: 1rem;\n",
        "  font-size: 1.3rem;\n",
        "}\n",
        "\n",
        ".prediction-text {\n",
        "  font-size: 1.2rem;\n",
        "  color: #e8f1f5;\n",
        "  margin-bottom: 1.5rem;\n",
        "  line-height: 1.8;\n",
        "}\n",
        "\n",
        "/* ========== CONFIDENCE BAR ========== */\n",
        ".confidence-bar {\n",
        "  margin-top: 1rem;\n",
        "}\n",
        "\n",
        ".confidence-label {\n",
        "  font-size: 0.9rem;\n",
        "  color: #b8d4ea;\n",
        "  margin-bottom: 0.5rem;\n",
        "}\n",
        "\n",
        ".progress-bar {\n",
        "  width: 100%;\n",
        "  height: 28px;\n",
        "  background: rgba(0, 0, 0, 0.3);\n",
        "  border-radius: 14px;\n",
        "  overflow: hidden;\n",
        "  border: 1px solid rgba(90, 139, 214, 0.4);\n",
        "}\n",
        "\n",
        ".progress-fill {\n",
        "  height: 100%;\n",
        "  background: linear-gradient(90deg, #0066cc, #00aaff);\n",
        "  border-radius: 14px;\n",
        "  transition: width 1s ease;\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  justify-content: flex-end;\n",
        "  padding-right: 10px;\n",
        "}\n",
        "\n",
        ".confidence-value {\n",
        "  text-align: right;\n",
        "  margin-top: 0.5rem;\n",
        "  font-weight: 600;\n",
        "  color: #89c4f4;\n",
        "  font-size: 1.1rem;\n",
        "}\n",
        "\n",
        "/* ========== SEVERITY INFO ========== */\n",
        ".severity-info {\n",
        "  background: rgba(255, 255, 255, 0.05);\n",
        "  border-left: 4px solid #ff9800;\n",
        "  padding: 1.2rem;\n",
        "  border-radius: 8px;\n",
        "  margin-bottom: 2rem;\n",
        "}\n",
        "\n",
        ".severity-badge {\n",
        "  display: inline-block;\n",
        "  padding: 0.5rem 1rem;\n",
        "  border-radius: 20px;\n",
        "  font-weight: 600;\n",
        "  margin-bottom: 0.8rem;\n",
        "  font-size: 0.95rem;\n",
        "}\n",
        "\n",
        ".severity-early-mild {\n",
        "  background: rgba(255, 193, 7, 0.2);\n",
        "  color: #ffd54f;\n",
        "  border: 1px solid #ffc107;\n",
        "}\n",
        "\n",
        ".severity-moderate {\n",
        "  background: rgba(255, 152, 0, 0.2);\n",
        "  color: #ffb74d;\n",
        "  border: 1px solid #ff9800;\n",
        "}\n",
        "\n",
        ".severity-moderately-severe {\n",
        "  background: rgba(255, 87, 34, 0.2);\n",
        "  color: #ff8a65;\n",
        "  border: 1px solid #ff5722;\n",
        "}\n",
        "\n",
        ".severity-severe-advanced {\n",
        "  background: rgba(244, 67, 54, 0.2);\n",
        "  color: #ef5350;\n",
        "  border: 1px solid #f44336;\n",
        "}\n",
        "\n",
        ".severity-desc {\n",
        "  color: #d0e3f0;\n",
        "  margin: 0.5rem 0;\n",
        "}\n",
        "\n",
        ".affected-area {\n",
        "  color: #b8d4ea;\n",
        "  font-size: 0.9rem;\n",
        "}\n",
        "\n",
        ".affected-area strong {\n",
        "  color: #89c4f4;\n",
        "}\n",
        "\n",
        "/* ========== RECOVERY STEPS ========== */\n",
        ".recovery-section {\n",
        "  margin-top: 2rem;\n",
        "  border-top: 2px solid rgba(90, 139, 214, 0.3);\n",
        "  padding-top: 2rem;\n",
        "}\n",
        "\n",
        ".recovery-section h3 {\n",
        "  color: #89c4f4;\n",
        "  margin-bottom: 1rem;\n",
        "  font-size: 1.3rem;\n",
        "}\n",
        "\n",
        ".steps-container {\n",
        "  background: rgba(255, 255, 255, 0.08);\n",
        "  border-radius: 10px;\n",
        "  padding: 1.5rem;\n",
        "  border: 1px solid rgba(90, 139, 214, 0.2);\n",
        "}\n",
        "\n",
        ".steps-text {\n",
        "  font-family: 'Segoe UI', system-ui, sans-serif;\n",
        "  white-space: pre-wrap;\n",
        "  color: #e8f1f5;\n",
        "  font-size: 0.95rem;\n",
        "  line-height: 1.8;\n",
        "  margin: 0;\n",
        "}\n",
        "\n",
        "/* ========== DISCLAIMER ========== */\n",
        ".disclaimer {\n",
        "  margin-top: 1.5rem;\n",
        "  padding: 1rem;\n",
        "  background: rgba(244, 67, 54, 0.1);\n",
        "  border: 1px solid rgba(244, 67, 54, 0.3);\n",
        "  border-radius: 8px;\n",
        "  color: #ffcdd2;\n",
        "  font-size: 0.9rem;\n",
        "  font-weight: 500;\n",
        "}\n",
        "\n",
        ".healthy-message {\n",
        "  background: rgba(76, 175, 80, 0.1);\n",
        "  border: 1px solid rgba(76, 175, 80, 0.3);\n",
        "  border-radius: 8px;\n",
        "  padding: 1.5rem;\n",
        "  margin-top: 1rem;\n",
        "  color: #c8e6c9;\n",
        "}\n",
        "\n",
        "/* ========== LOADING ========== */\n",
        ".loading {\n",
        "  text-align: center;\n",
        "  padding: 3rem 1rem;\n",
        "}\n",
        "\n",
        ".spinner {\n",
        "  border: 4px solid rgba(255, 255, 255, 0.1);\n",
        "  border-top: 4px solid #5a8bd6;\n",
        "  border-radius: 50%;\n",
        "  width: 50px;\n",
        "  height: 50px;\n",
        "  animation: spin 1s linear infinite;\n",
        "  margin: 0 auto 1rem;\n",
        "}\n",
        "\n",
        "@keyframes spin {\n",
        "  0% { transform: rotate(0deg); }\n",
        "  100% { transform: rotate(360deg); }\n",
        "}\n",
        "\n",
        ".loading p {\n",
        "  color: #b8d4ea;\n",
        "  margin: 0.5rem 0;\n",
        "}\n",
        "\n",
        ".loading-sub {\n",
        "  font-size: 0.9rem;\n",
        "  color: #8ba9c4;\n",
        "}\n",
        "\n",
        "/* ========== ERROR ========== */\n",
        ".error {\n",
        "  background: rgba(244, 67, 54, 0.15);\n",
        "  border: 1px solid #f44336;\n",
        "  border-radius: 8px;\n",
        "  padding: 1.5rem;\n",
        "  color: #ffcdd2;\n",
        "  text-align: center;\n",
        "}\n",
        "\n",
        "/* ========== WELCOME PAGE ========== */\n",
        ".welcome-wrapper {\n",
        "  min-height: 100vh;\n",
        "  display: flex;\n",
        "  align-items: center;\n",
        "  justify-content: center;\n",
        "  padding: 2rem 1rem;\n",
        "}\n",
        "\n",
        ".welcome-container {\n",
        "  max-width: 1000px;\n",
        "  text-align: center;\n",
        "  animation: fadeIn 0.8s ease;\n",
        "}\n",
        "\n",
        ".welcome-icon {\n",
        "  font-size: 5rem;\n",
        "  margin-bottom: 1rem;\n",
        "  animation: float 3s ease-in-out infinite;\n",
        "}\n",
        "\n",
        ".welcome-title {\n",
        "  font-size: 2.8rem;\n",
        "  margin-bottom: 1rem;\n",
        "  background: linear-gradient(135deg, #5a8bd6, #89c4f4, #b8d4ea);\n",
        "  -webkit-background-clip: text;\n",
        "  -webkit-text-fill-color: transparent;\n",
        "  background-clip: text;\n",
        "}\n",
        "\n",
        ".welcome-subtitle {\n",
        "  font-size: 1.3rem;\n",
        "  color: #b8d4ea;\n",
        "  margin-bottom: 3rem;\n",
        "  font-weight: 300;\n",
        "}\n",
        "\n",
        "/* ========== FEATURE GRID ========== */\n",
        ".feature-grid {\n",
        "  display: grid;\n",
        "  grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));\n",
        "  gap: 1.5rem;\n",
        "  margin: 3rem 0;\n",
        "}\n",
        "\n",
        ".feature-item {\n",
        "  background: rgba(255, 255, 255, 0.05);\n",
        "  backdrop-filter: blur(10px);\n",
        "  border: 1px solid rgba(90, 139, 214, 0.3);\n",
        "  border-radius: 12px;\n",
        "  padding: 1.8rem;\n",
        "  transition: all 0.3s ease;\n",
        "}\n",
        "\n",
        ".feature-item:hover {\n",
        "  transform: translateY(-5px);\n",
        "  background: rgba(255, 255, 255, 0.08);\n",
        "  border-color: #5a8bd6;\n",
        "  box-shadow: 0 8px 24px rgba(90, 139, 214, 0.2);\n",
        "}\n",
        "\n",
        ".feature-icon {\n",
        "  font-size: 2.5rem;\n",
        "  margin-bottom: 1rem;\n",
        "}\n",
        "\n",
        ".feature-item h3 {\n",
        "  color: #89c4f4;\n",
        "  font-size: 1.1rem;\n",
        "  margin-bottom: 0.8rem;\n",
        "}\n",
        "\n",
        ".feature-item p {\n",
        "  color: #b8d4ea;\n",
        "  font-size: 0.9rem;\n",
        "  line-height: 1.6;\n",
        "}\n",
        "\n",
        "/* ========== INFO BOX ========== */\n",
        ".info-box {\n",
        "  background: rgba(255, 255, 255, 0.05);\n",
        "  border: 1px solid rgba(90, 139, 214, 0.3);\n",
        "  border-radius: 12px;\n",
        "  padding: 2rem;\n",
        "  margin: 3rem 0;\n",
        "  text-align: left;\n",
        "}\n",
        "\n",
        ".info-box h3 {\n",
        "  color: #89c4f4;\n",
        "  margin-bottom: 1rem;\n",
        "  font-size: 1.3rem;\n",
        "}\n",
        "\n",
        ".info-box p {\n",
        "  color: #d0e3f0;\n",
        "  margin-bottom: 1rem;\n",
        "  line-height: 1.8;\n",
        "}\n",
        "\n",
        ".info-warning {\n",
        "  background: rgba(255, 152, 0, 0.1);\n",
        "  border-left: 3px solid #ff9800;\n",
        "  padding: 1rem;\n",
        "  border-radius: 6px;\n",
        "  color: #ffcc80;\n",
        "}\n",
        "\n",
        "/* ========== WELCOME BUTTON ========== */\n",
        ".welcome-btn {\n",
        "  padding: 1.2rem 3rem;\n",
        "  background: linear-gradient(135deg, #0066cc, #004c99);\n",
        "  color: white;\n",
        "  border: none;\n",
        "  border-radius: 12px;\n",
        "  font-size: 1.2rem;\n",
        "  font-weight: 600;\n",
        "  cursor: pointer;\n",
        "  margin-top: 2rem;\n",
        "  transition: all 0.3s ease;\n",
        "  box-shadow: 0 6px 20px rgba(0, 102, 204, 0.4);\n",
        "}\n",
        "\n",
        ".welcome-btn:hover {\n",
        "  background: linear-gradient(135deg, #0052a3, #003d7a);\n",
        "  transform: translateY(-3px);\n",
        "  box-shadow: 0 8px 28px rgba(0, 102, 204, 0.6);\n",
        "}\n",
        "\n",
        ".footer-note {\n",
        "  margin-top: 3rem;\n",
        "  padding-top: 2rem;\n",
        "  border-top: 1px solid rgba(90, 139, 214, 0.2);\n",
        "}\n",
        "\n",
        ".footer-note p {\n",
        "  color: #8ba9c4;\n",
        "  font-size: 0.9rem;\n",
        "}\n",
        "\n",
        "/* ========== ANIMATIONS ========== */\n",
        "@keyframes fadeIn {\n",
        "  from {\n",
        "    opacity: 0;\n",
        "    transform: translateY(20px);\n",
        "  }\n",
        "  to {\n",
        "    opacity: 1;\n",
        "    transform: translateY(0);\n",
        "  }\n",
        "}\n",
        "\n",
        "@keyframes float {\n",
        "  0%, 100% {\n",
        "    transform: translateY(0);\n",
        "  }\n",
        "  50% {\n",
        "    transform: translateY(-20px);\n",
        "  }\n",
        "}\n",
        "\n",
        "/* ========== RESPONSIVE ========== */\n",
        "@media (max-width: 768px) {\n",
        "  .header h1 {\n",
        "    font-size: 1.8rem;\n",
        "  }\n",
        "\n",
        "  .welcome-title {\n",
        "    font-size: 2rem;\n",
        "  }\n",
        "\n",
        "  .welcome-subtitle {\n",
        "    font-size: 1rem;\n",
        "  }\n",
        "\n",
        "  .feature-grid {\n",
        "    grid-template-columns: 1fr;\n",
        "  }\n",
        "\n",
        "  .upload-section {\n",
        "    padding: 1.5rem;\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2isgtuX2BXmd"
      },
      "source": [
        "\ud83d\udcd8 Kill Previous Processes\n",
        "\n",
        "This ensures Flask and ngrok do not conflict:\n",
        "\n",
        "- Stops earlier Flask sessions  \n",
        "- Stops older ngrok tunnels  \n",
        "- Prevents \"port already in use\" errors  \n",
        "\n",
        "Safe to run every time before starting server.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns0pgKBN233j"
      },
      "outputs": [],
      "source": [
        "# Kill any previous processes\n",
        "!pkill -f flask || echo \"No flask running\"\n",
        "!pkill -f ngrok || echo \"No ngrok running\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxxFWRMbBXme"
      },
      "source": [
        "\ud83d\udcd8  Checking Port 8000 (User Instructions)\n",
        "\n",
        "If server fails, port 8000 may be occupied.\n",
        "\n",
        "Run:\n",
        "!lsof -i :8000\n",
        "\n",
        "If you see:\n",
        "python   12345 LISTEN\n",
        "\n",
        "Kill it with:\n",
        "!kill -9 12345\n",
        "\n",
        "Then launch Flask again.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqESeFXK6U-p"
      },
      "outputs": [],
      "source": [
        "!lsof -i :8000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEHPAhea6b-y"
      },
      "outputs": [],
      "source": [
        "!kill -9 4209 2548\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb3smyqmBXmf"
      },
      "source": [
        "\ud83d\udcd8  Run Flask App in Background\n",
        "\n",
        "Starts backend without blocking the notebook:\n",
        "\n",
        "!nohup python app.py > flask.log 2>&1 &\n",
        "\n",
        "Logs are stored in flask.log\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro4Sq6CC23td"
      },
      "outputs": [],
      "source": [
        "# Run Flask in the background\n",
        "!nohup python app.py > flask.log 2>&1 &\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs53ZhjLBXmg"
      },
      "source": [
        "\ud83d\udcd8  Ngrok Setup\n",
        "\n",
        "Ngrok provides a public HTTPS link.\n",
        "\n",
        "Your ngrok token was removed for safety.\n",
        "\n",
        "To use ngrok:\n",
        "1. Get token \u2192 https://dashboard.ngrok.com/get-started/your-authtoken  \n",
        "2. Add inside notebook:\n",
        "\n",
        "conf.get_default().auth_token = \"YOUR_NGROK_TOKEN_HERE\"\n",
        "\n",
        "3. Start tunnel:\n",
        "\n",
        "public_url = ngrok.connect(8000)\n",
        "\n",
        "Shareable app link appears here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfDHeLqS2ucc"
      },
      "outputs": [],
      "source": [
        "# Start ngrok tunnel\n",
        "from pyngrok import ngrok, conf\n",
        "conf.get_default().auth_token = \"YOUR_NGROK_TOKEN_HERE\"\n",
        "\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"\ud83c\udf0d Public URL:\", public_url)\n",
        "\n",
        "# Optional: check logs\n",
        "!sleep 3 && tail -n 20 flask.log\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_buVfi83LO4"
      },
      "outputs": [],
      "source": [
        "!tail -n 50 flask.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYiUzDq98Jew"
      },
      "outputs": [],
      "source": [
        "# \ud83c\udf89 Deployment Successful!\n",
        "\n",
        "Your AI-powered Diabetic Retinopathy Detection System is now live.\n",
        "\n",
        "This web app includes:\n",
        "\u2714 CNN vision model\n",
        "\u2714 Severity analysis\n",
        "\u2714 Agentic AI recovery plan generator\n",
        "\u2714 Beautiful interactive UI\n",
        "\n",
        "Share your public ngrok URL to allow others to test the app.\n"
      ]
    }
  ]
}