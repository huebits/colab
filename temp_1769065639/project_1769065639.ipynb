{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udcd8 Data Preprocessing Overview\n",
        "#### Cyanobacteria Bloom Prediction in U.S. Reservoirs (1987\u20132018)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1\ufe0f\u20e3 Data Sources Used\n",
        "\n",
        "We integrated multiple long-term environmental datasets collected from **20 U.S. reservoirs** between **1987 and 2018**:\n",
        "\n",
        "* **Cyanobacteria observations**\n",
        "  Monthly cyanobacterial density (cells/ml)\n",
        "\n",
        "* **Water temperature**\n",
        "\n",
        "  * Deep-water temperature\n",
        "  * Surface-water temperature\n",
        "\n",
        "* **Water quality**\n",
        "\n",
        "  * Deep dissolved oxygen (DO)\n",
        "\n",
        "* **Nutrient variables**\n",
        "\n",
        "  * Phosphorus and nitrogen species (TP, TKN, NH\u2083, NO\u2093, etc.)\n",
        "  * Inflow nutrient concentrations\n",
        "  * Chlorophyll-a and Secchi depth\n",
        "\n",
        "* **Watershed land-cover data**\n",
        "\n",
        "  * Forest, agriculture, urban, wetlands, and other land-use classes (NLCD 2011)\n",
        "\n",
        "---\n",
        "\n",
        "### 2\ufe0f\u20e3 Temporal Aggregation\n",
        "\n",
        "* All datasets were **aligned to a monthly time scale**\n",
        "* Daily or irregular measurements were **aggregated using monthly means**\n",
        "* Each observation corresponds to:\n",
        "\n",
        "  ```\n",
        "  Reservoir \u2013 Year \u2013 Month\n",
        "  ```\n",
        "\n",
        "This ensured **temporal consistency** across all data sources.\n",
        "\n",
        "---\n",
        "\n",
        "### 3\ufe0f\u20e3 Cyanobacteria Data Processing\n",
        "\n",
        "* Raw cyanobacteria measurements were converted to **monthly mean density**\n",
        "* Data were grouped by:\n",
        "\n",
        "  * Reservoir\n",
        "  * Year\n",
        "  * Month\n",
        "* This variable serves as the **target variable** for prediction\n",
        "\n",
        "---\n",
        "\n",
        "### 4\ufe0f\u20e3 Environmental Variable Processing\n",
        "\n",
        "#### \ud83d\udd39 Deep & Surface Temperature\n",
        "\n",
        "* Temperature datasets were originally stored in **wide format**\n",
        "* Converted to **long format** and parsed correctly as dates\n",
        "* Extracted `year` and `month`\n",
        "* Aggregated to monthly means per reservoir\n",
        "\n",
        "#### \ud83d\udd39 Deep Dissolved Oxygen\n",
        "\n",
        "* Similar restructuring from wide to long format\n",
        "* Monthly averaging applied\n",
        "* Ensured alignment with temperature and cyanobacteria data\n",
        "\n",
        "---\n",
        "\n",
        "### 5\ufe0f\u20e3 Nutrient Data Integration\n",
        "\n",
        "* Nutrient concentrations were **static per reservoir**\n",
        "* Converted all values to numeric format\n",
        "* Merged directly using the reservoir identifier\n",
        "\n",
        "---\n",
        "\n",
        "### 6\ufe0f\u20e3 Land-Cover Data Integration\n",
        "\n",
        "* Watershed land-cover percentages from **NLCD 2011**\n",
        "* Represent static catchment characteristics:\n",
        "\n",
        "  * Forest cover\n",
        "  * Agricultural land\n",
        "  * Urban development\n",
        "  * Wetlands\n",
        "* Merged at the reservoir level\n",
        "* Land-cover values are repeated across months for each reservoir\n",
        "\n",
        "---\n",
        "\n",
        "### 7\ufe0f\u20e3 Data Cleaning & Standardization\n",
        "\n",
        "* Reservoir names standardized (uppercase, trimmed)\n",
        "* Invalid dates and records removed\n",
        "* Missing numeric values filled using **median imputation**\n",
        "* Column names cleaned for modeling compatibility\n",
        "\n",
        "---\n",
        "\n",
        "### 8\ufe0f\u20e3 Final Dataset Structure\n",
        "\n",
        "The final preprocessed dataset contains:\n",
        "\n",
        "* **742 observations**\n",
        "* **48 predictor variables**\n",
        "* **0 missing values**\n",
        "* Fully numeric and ML-ready format\n",
        "\n",
        "Each row represents:\n",
        "\n",
        "```\n",
        "One reservoir \u00d7 One month \u00d7 Environmental conditions \u2192 Cyanobacteria density\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 9\ufe0f\u20e3 Why This Preprocessing Matters\n",
        "\n",
        "This preprocessing pipeline ensures:\n",
        "\n",
        "* \u2705 No temporal mismatch between datasets\n",
        "* \u2705 No data leakage\n",
        "* \u2705 Physically and ecologically consistent variables\n",
        "* \u2705 Fair comparison across different ML and DL models\n",
        "\n",
        "As a result, any predictive model trained on this dataset will reflect **true environmental drivers** of cyanobacteria blooms.\n",
        "\n",
        "---\n",
        "\n",
        "### 10\ufe0f\u20e3 Ready for Modeling\n",
        "\n",
        "After preprocessing, the dataset is suitable for:\n",
        "\n",
        "* Traditional ML models (RF, XGBoost, SVR)\n",
        "* Hybrid deep learning models (CNN-LSTM)\n",
        "* Transformer-based time-series models\n",
        "* Graph Neural Networks (GNNs)\n",
        "* Reservoir-wise performance evaluation (R\u00b2, RMSE, MAE)\n",
        "\n",
        "---\n",
        "\n",
        "#### \u2705 In one sentence:\n",
        "\n",
        "> *All heterogeneous environmental datasets were cleaned, temporally aligned, aggregated, and merged into a single monthly, reservoir-wise dataset suitable for robust cyanobacteria bloom prediction.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5BaLCHdSxWL",
        "outputId": "646403d6-4e88-4b56-ad53-8611435413a6"
      },
      "outputs": [],
      "source": [
        "## Cell 1 \u2013 Import Required Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "necUUhqWS6Cd"
      },
      "outputs": [],
      "source": [
        "## Cell 2 \u2013 Load Raw Data\n",
        "\n",
        "CYANO_PATH     = \"/kaggle/input/cyanobacterial-data/Cynobacterial_Data/Cyanobacteria_data (1).xlsx\"\n",
        "DEEP_TEMP_PATH = \"/kaggle/input/cyanobacterial-data/Cynobacterial_Data/Deep_Temperatures_standardized_FINAL.xlsx\"\n",
        "SURF_TEMP_PATH = \"/kaggle/input/cyanobacterial-data/Cynobacterial_Data/Surface_Temperatures_standardized_FINAL.xlsx\"\n",
        "DEEP_DO_PATH   = \"/kaggle/input/cyanobacterial-data/Cynobacterial_Data/Deep_DO_standardized_FINAL.xlsx\"\n",
        "NUT_PATH       = \"/kaggle/input/cyanobacterial-data/Cynobacterial_Data/Means_of_reservoir_nutrients.xlsx\"\n",
        "LAND_PATH      = \"/kaggle/input/cyanobacterial-data/Cynobacterial_Data/NLCD_watershed_land_cover.xlsx\"\n",
        "\n",
        "cyano_df     = pd.read_excel(CYANO_PATH, sheet_name=\"All_data\")\n",
        "deep_temp_df = pd.read_excel(DEEP_TEMP_PATH, sheet_name=\"All_reservoirs\")\n",
        "surf_temp_df = pd.read_excel(SURF_TEMP_PATH, sheet_name=\"All_reservoirs\")\n",
        "deep_do_df   = pd.read_excel(DEEP_DO_PATH, sheet_name=\"All_reservoirs\")\n",
        "nut_df       = pd.read_excel(NUT_PATH, sheet_name=\"Means_of_variables\")\n",
        "land_df      = pd.read_excel(LAND_PATH, sheet_name=\"NLCD_2011\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##  Cell 3 \u2013 CELL 3 \u2014 Cyanobacteria Monthly Aggregation\n",
        "\n",
        "cyano_df['date'] = pd.to_datetime(cyano_df['date'], errors='coerce')\n",
        "\n",
        "cyano_agg = (\n",
        "    cyano_df\n",
        "    .groupby([\n",
        "        'reservoir',\n",
        "        cyano_df['date'].dt.year.rename('year'),\n",
        "        cyano_df['date'].dt.month.rename('month')\n",
        "    ])['density_cells/ml']\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "cyano_agg['reservoir'] = cyano_agg['reservoir'].str.strip().str.upper()\n",
        "cyano_agg.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 4 \u2014 Initialize Master DataFrame\n",
        "\n",
        "df = cyano_agg.copy()\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 5 \u2014 Process Deep Temperature\n",
        "\n",
        "deep_blocks = []\n",
        "\n",
        "for col in deep_temp_df.columns:\n",
        "    if col.startswith('reservoir'):\n",
        "        suffix = col.replace('reservoir', '')\n",
        "        try:\n",
        "            sub = deep_temp_df[\n",
        "                [\n",
        "                    f'reservoir{suffix}',\n",
        "                    f'date{suffix}',\n",
        "                    f'year{suffix}',\n",
        "                    f'month{suffix}',\n",
        "                    f'Temp{suffix}',\n",
        "                    f'slope{suffix}',\n",
        "                    f'Predicted{suffix}',\n",
        "                    f'Residual{suffix}',\n",
        "                    f'15.5th{suffix}'\n",
        "                ]\n",
        "            ].copy()\n",
        "\n",
        "            sub.columns = [\n",
        "                'reservoir','raw_date','year','month',\n",
        "                'deep_Temp','deep_slope','deep_Predicted',\n",
        "                'deep_Residual','deep_15_5th'\n",
        "            ]\n",
        "\n",
        "            deep_blocks.append(sub)\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "deep_long = pd.concat(deep_blocks, ignore_index=True)\n",
        "\n",
        "deep_long['raw_date'] = deep_long['raw_date'].astype(str).str.split('.').str[0]\n",
        "deep_long['date'] = pd.to_datetime(deep_long['raw_date'], format='%Y%m%d', errors='coerce')\n",
        "\n",
        "deep_long = deep_long.dropna(subset=['reservoir','date','deep_Temp'])\n",
        "\n",
        "deep_temp_monthly = (\n",
        "    deep_long\n",
        "    .groupby(['reservoir','year','month'])\n",
        "    .mean(numeric_only=True)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "deep_temp_monthly = deep_temp_monthly.query(\"year >= 1987 and year <= 2018\")\n",
        "deep_temp_monthly['reservoir'] = deep_temp_monthly['reservoir'].str.strip().str.upper()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 6 \u2014 Merge Deep Temperature\n",
        "\n",
        "df = df.merge(\n",
        "    deep_temp_monthly,\n",
        "    on=['reservoir','year','month'],\n",
        "    how='left'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 7 \u2014 Process Deep DO\n",
        "\n",
        "do_blocks = []\n",
        "\n",
        "do_cols = [c for c in deep_do_df.columns if 'do' in c.lower() or 'oxygen' in c.lower()]\n",
        "\n",
        "for col in deep_do_df.columns:\n",
        "    if col.startswith('reservoir'):\n",
        "        suffix = col.replace('reservoir','')\n",
        "        try:\n",
        "            do_col = next(c for c in do_cols if c.endswith(suffix))\n",
        "\n",
        "            sub = deep_do_df[\n",
        "                [\n",
        "                    f'reservoir{suffix}',\n",
        "                    f'date{suffix}',\n",
        "                    f'year{suffix}',\n",
        "                    f'month{suffix}',\n",
        "                    do_col\n",
        "                ]\n",
        "            ].copy()\n",
        "\n",
        "            sub.columns = ['reservoir','raw_date','year','month','deep_DO']\n",
        "            do_blocks.append(sub)\n",
        "\n",
        "        except (KeyError, StopIteration):\n",
        "            pass\n",
        "\n",
        "deep_do_long = pd.concat(do_blocks, ignore_index=True)\n",
        "\n",
        "deep_do_long['raw_date'] = deep_do_long['raw_date'].astype(str).str.split('.').str[0]\n",
        "deep_do_long['date'] = pd.to_datetime(deep_do_long['raw_date'], format='%Y%m%d', errors='coerce')\n",
        "\n",
        "deep_do_long = deep_do_long.dropna(subset=['reservoir','date','deep_DO'])\n",
        "\n",
        "deep_do_monthly = (\n",
        "    deep_do_long\n",
        "    .groupby(['reservoir','year','month'])\n",
        "    .mean(numeric_only=True)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "deep_do_monthly = deep_do_monthly.query(\"year >= 1987 and year <= 2018\")\n",
        "deep_do_monthly['reservoir'] = deep_do_monthly['reservoir'].str.strip().str.upper()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 8 \u2014 Merge Deep DO\n",
        "\n",
        "df = df.merge(\n",
        "    deep_do_monthly,\n",
        "    on=['reservoir','year','month'],\n",
        "    how='left'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 9 \u2014 Build Surface Temperature (Long Format)\n",
        "\n",
        "surf_blocks = []\n",
        "\n",
        "for col in surf_temp_df.columns:\n",
        "    if col.startswith('Reservoir'):\n",
        "        suffix = col.replace('Reservoir','')\n",
        "        try:\n",
        "            sub = surf_temp_df[\n",
        "                [\n",
        "                    f'Reservoir{suffix}',\n",
        "                    f'Date{suffix}',\n",
        "                    f'temperature{suffix}',\n",
        "                    f'Predicted{suffix}',\n",
        "                    f'Residual{suffix}',\n",
        "                    f'15.5th{suffix}'\n",
        "                ]\n",
        "            ].copy()\n",
        "\n",
        "            sub.columns = [\n",
        "                'reservoir','raw_date',\n",
        "                'surface_Temp','surface_Predicted',\n",
        "                'surface_Residual','surface_15_5th'\n",
        "            ]\n",
        "\n",
        "            surf_blocks.append(sub)\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "surf_long = pd.concat(surf_blocks, ignore_index=True)\n",
        "\n",
        "surf_long['raw_date'] = surf_long['raw_date'].astype(str).str.split('.').str[0]\n",
        "surf_long['date'] = pd.to_datetime(surf_long['raw_date'], format='%Y%m%d', errors='coerce')\n",
        "\n",
        "surf_long = surf_long.dropna(subset=['reservoir','date','surface_Temp'])\n",
        "\n",
        "surf_long['year'] = surf_long['date'].dt.year\n",
        "surf_long['month'] = surf_long['date'].dt.month\n",
        "surf_long['reservoir'] = surf_long['reservoir'].str.strip().str.upper()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 10 \u2014 Process & Merge Surface Temperature\n",
        "\n",
        "group_keys = ['reservoir','year','month']\n",
        "\n",
        "numeric_cols = [\n",
        "    c for c in surf_long.select_dtypes(include=np.number).columns\n",
        "    if c not in ['year','month']\n",
        "]\n",
        "\n",
        "surf_temp_monthly = (\n",
        "    surf_long\n",
        "    .groupby(group_keys)[numeric_cols]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "surf_temp_monthly = surf_temp_monthly.query(\"year >= 1987 and year <= 2018\")\n",
        "\n",
        "df = df.merge(\n",
        "    surf_temp_monthly,\n",
        "    on=['reservoir','year','month'],\n",
        "    how='left'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 11 \u2014 Merge Nutrients\n",
        "\n",
        "nut_df.columns = nut_df.columns.str.strip()\n",
        "nut_df['reservoir'] = nut_df['reservoir'].str.strip().str.upper()\n",
        "\n",
        "for col in nut_df.columns:\n",
        "    if col != 'reservoir':\n",
        "        nut_df[col] = pd.to_numeric(nut_df[col], errors='coerce')\n",
        "\n",
        "df = df.merge(nut_df, on='reservoir', how='left')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 12 \u2014 Merge Land Cover\n",
        "\n",
        "land_df.columns = land_df.columns.str.strip()\n",
        "land_df = land_df.rename(columns={'Reservoir':'reservoir'})\n",
        "land_df['reservoir'] = land_df['reservoir'].str.strip().str.upper()\n",
        "\n",
        "df = df.merge(land_df, on='reservoir', how='left')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 13 \u2014 Final Cleaning\n",
        "\n",
        "if 'date' in df.columns:\n",
        "    df = df.drop(columns=['date'])\n",
        "\n",
        "num_cols = df.select_dtypes(include=np.number).columns\n",
        "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 14 \u2014 Final Sanity Check\n",
        "\n",
        "df.info()\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 15 \u2014 Rename columns with spaces & slashes (ML-friendly)\n",
        "\n",
        "df = df.rename(columns={\n",
        "    'density_cells/ml': 'density_cells_ml',\n",
        "    'Barren Land': 'lc_barren',\n",
        "    'Cultivated Crops': 'lc_crops',\n",
        "    'Deciduous Forest': 'lc_deciduous_forest',\n",
        "    'Developed High Intensity': 'lc_dev_high',\n",
        "    'Developed Medium Intensity': 'lc_dev_medium',\n",
        "    'Developed Low Intensity': 'lc_dev_low',\n",
        "    'Developed Open Space': 'lc_dev_open',\n",
        "    'Emergent Herbaceous Wetlands': 'lc_wetland_herb',\n",
        "    'Evergreen Forest': 'lc_evergreen_forest',\n",
        "    'Grassland/Herbaceous': 'lc_grassland',\n",
        "    'Mixed Forest': 'lc_mixed_forest',\n",
        "    'Open Water': 'lc_open_water',\n",
        "    'Pasture/Hay': 'lc_pasture',\n",
        "    'Shrub/Scrub': 'lc_shrub',\n",
        "    'Woody Wetlands': 'lc_wetland_woody'\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Save the final locked dataset\n",
        "\n",
        "df.to_csv(\"cyanobacteria_preprocessed_master_FINAL.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd0d FEATURE IMPACT ANALYSIS\n",
        "##### Cyanobacteria Bloom Drivers in U.S. Reservoirs (1987\u20132018)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##  CELL 1 \u2014 Setup & Target / Feature Split\n",
        "# Copy dataset to avoid accidental changes\n",
        "data = df.copy()\n",
        "\n",
        "# Rename target for convenience (if not already done)\n",
        "data = data.rename(columns={'density_cells/ml': 'density_cells_ml'})\n",
        "\n",
        "# Target variable\n",
        "y = data['density_cells_ml']\n",
        "\n",
        "# Drop non-feature columns\n",
        "X = data.drop(columns=[\n",
        "    'density_cells_ml',\n",
        "    'reservoir',   # categorical, handled later\n",
        "    'year'         # temporal info, optional\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 2 \u2014 Basic Feature Statistics (Sanity Check)\n",
        "\n",
        "X.describe().T[['mean', 'std', 'min', 'max']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PART 1: CORRELATION ANALYSIS (GLOBAL) \u2014 Linear Relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 3 \u2014 Correlation with Target Variable\n",
        "\n",
        "corr_with_target = (\n",
        "    data.corr(numeric_only=True)['density_cells_ml']\n",
        "    .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "corr_with_target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 4 \u2014 Top Positive & Negative Correlations\n",
        "# Top drivers\n",
        "top_positive = corr_with_target[1:11]\n",
        "top_negative = corr_with_target[-10:]\n",
        "\n",
        "top_positive, top_negative\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PART 2: RANDOM FOREST FEATURE IMPORTANCE \u2014 Nonlinear & Interaction Effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 5 \u2014 Train Random Forest Regressor\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=500,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf.fit(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 6 \u2014 Random Forest Feature Importance\n",
        "\n",
        "rf_importance = (\n",
        "    pd.Series(rf.feature_importances_, index=X.columns)\n",
        "    .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "rf_importance.head(15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 7 \u2014 Plot Random Forest Importance\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rf_importance.head(15).plot(kind='barh', figsize=(8,6))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"Random Forest Feature Importance (Global)\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PART 3: XGBOOST FEATURE IMPORTANCE - High-Resolution Nutrient Sensitivity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL 8 \u2014 Train XGBoost Regressor\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "xgb = XGBRegressor(\n",
        "    n_estimators=500,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb.fit(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 9 \u2014 XGBoost Feature Importance\n",
        "\n",
        "xgb_importance = (\n",
        "    pd.Series(xgb.feature_importances_, index=X.columns)\n",
        "    .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "xgb_importance.head(15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 10 \u2014 Plot XGBoost Importance\n",
        "\n",
        "xgb_importance.head(15).plot(kind='barh', figsize=(8,6))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"XGBoost Feature Importance (Global)\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PART 4: CONSOLIDATED FEATURE RANKING \u2014 Robust Consensus Drivers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 11 \u2014 Combined Importance Table\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'RF_importance': rf_importance,\n",
        "    'XGB_importance': xgb_importance\n",
        "}).fillna(0)\n",
        "\n",
        "importance_df['mean_importance'] = importance_df.mean(axis=1)\n",
        "importance_df.sort_values('mean_importance', ascending=False).head(15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udded Reservoir-Wise Feature Impact Analysis\n",
        "Understanding Bloom Drivers Across 20 U.S. Reservoirs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 1 \u2014 Imports & Feature Setup\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 2 \u2014 Define Target & Feature Columns\n",
        "\n",
        "TARGET = 'density_cells_ml'\n",
        "\n",
        "EXCLUDE_COLS = [\n",
        "    TARGET,\n",
        "    'reservoir',\n",
        "    'year'\n",
        "]\n",
        "\n",
        "FEATURE_COLS = [c for c in df.columns if c not in EXCLUDE_COLS]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PART 1: RANDOM FOREST FEATURE IMPORTANCE (PER RESERVOIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 3 \u2014 Compute Feature Importance per Reservoir\n",
        "\n",
        "reservoir_importance = {}\n",
        "\n",
        "for reservoir in df['reservoir'].unique():\n",
        "    sub = df[df['reservoir'] == reservoir]\n",
        "\n",
        "    # Skip reservoirs with too few samples\n",
        "    if len(sub) < 20:\n",
        "        continue\n",
        "\n",
        "    X = sub[FEATURE_COLS]\n",
        "    y = sub[TARGET]\n",
        "\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=300,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    rf.fit(X, y)\n",
        "\n",
        "    importance = pd.Series(\n",
        "        rf.feature_importances_,\n",
        "        index=FEATURE_COLS\n",
        "    ).sort_values(ascending=False)\n",
        "\n",
        "    reservoir_importance[reservoir] = importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 4 \u2014 Top Drivers per Reservoir (Table)\n",
        "\n",
        "top_features_per_reservoir = pd.DataFrame({\n",
        "    r: imp.head(5).index.tolist()\n",
        "    for r, imp in reservoir_importance.items()\n",
        "}).T\n",
        "\n",
        "top_features_per_reservoir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd39 PART 2: VISUALIZE FEATURE IMPORTANCE (RESERVOIR-WISE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 5 \u2014 Plot Top Features for One Reservoir\n",
        "\n",
        "def plot_reservoir_importance(reservoir, top_n=10):\n",
        "    imp = reservoir_importance[reservoir].head(top_n)\n",
        "\n",
        "    plt.figure(figsize=(7,5))\n",
        "    imp.sort_values().plot(kind='barh')\n",
        "    plt.title(f\"Top {top_n} Drivers \u2014 Reservoir {reservoir}\")\n",
        "    plt.xlabel(\"Feature Importance\")\n",
        "    plt.show()\n",
        "\n",
        "# Example\n",
        "plot_reservoir_importance('BVR')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PART 3: RESERVOIR CLASSIFICATION BY DOMINANT DRIVER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 7 \u2014 Categorize Drivers\n",
        "\n",
        "driver_groups = {\n",
        "    'Nutrient': [\n",
        "        'TIN_inflow_ppm','NOx_inflow_ppm','TP_ppb','TKN_ppm',\n",
        "        'NH3_ppm','TON_ppm'\n",
        "    ],\n",
        "    'Temperature': [\n",
        "        'surface_Temp','deep_Temp','surface_Predicted','deep_Predicted',\n",
        "        'deep_slope'\n",
        "    ],\n",
        "    'Oxygen': ['deep_DO'],\n",
        "    'LandCover': [\n",
        "        'Agricultural','Forest','Developed',\n",
        "        'lc_pasture','lc_crops'\n",
        "    ],\n",
        "    'Seasonality': ['month']\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 8 \u2014 Dominant Driver per Reservoir\n",
        "\n",
        "reservoir_driver_summary = []\n",
        "\n",
        "for reservoir, imp in reservoir_importance.items():\n",
        "    scores = {}\n",
        "\n",
        "    for group, features in driver_groups.items():\n",
        "        scores[group] = imp[imp.index.isin(features)].sum()\n",
        "\n",
        "    dominant_driver = max(scores, key=scores.get)\n",
        "\n",
        "    reservoir_driver_summary.append({\n",
        "        'reservoir': reservoir,\n",
        "        'dominant_driver': dominant_driver,\n",
        "        'scores': scores\n",
        "    })\n",
        "\n",
        "driver_df = pd.DataFrame(reservoir_driver_summary)\n",
        "driver_df[['reservoir','dominant_driver']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 9 \u2014 Plot Driver Distribution\n",
        "driver_df['dominant_driver'].value_counts().plot(\n",
        "    kind='bar',\n",
        "    figsize=(6,4),\n",
        "    title='Dominant Bloom Drivers Across Reservoirs'\n",
        ")\n",
        "plt.ylabel('Number of Reservoirs')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\uddec Species Dominance Analysis\n",
        "Identifying Cyanobacteria Species Driving Blooms in U.S. Reservoirs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PART 1: GLOBAL SPECIES DOMINANCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 1 \u2014 Load & Prepare Species Data\n",
        "\n",
        "# Load raw cyanobacteria data (species-level)\n",
        "cyano_species = pd.read_excel(\n",
        "    CYANO_PATH,\n",
        "    sheet_name=\"All_data\"\n",
        ")\n",
        "\n",
        "# Standardize columns\n",
        "cyano_species.columns = cyano_species.columns.str.strip()\n",
        "\n",
        "# Rename for clarity\n",
        "cyano_species = cyano_species.rename(columns={\n",
        "    'density_cells/ml': 'density_cells_ml'\n",
        "})\n",
        "\n",
        "cyano_species.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 2 \u2014 Global Species Abundance\n",
        "\n",
        "species_global = (\n",
        "    cyano_species\n",
        "    .groupby('sci_name')['density_cells_ml']\n",
        "    .mean()\n",
        "    .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "species_global.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 3 \u2014 Plot Top Dominant Species (Global)\n",
        "\n",
        "species_global.head(10).plot(\n",
        "    kind='bar',\n",
        "    figsize=(8,4),\n",
        "    title='Top 10 Dominant Cyanobacteria Species (Global)'\n",
        ")\n",
        "plt.ylabel('Mean Cell Density (cells/ml)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PART 2: RESERVOIR-WISE SPECIES DOMINANCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL 4 \u2014 Species Dominance per Reservoir\n",
        "\n",
        "species_reservoir = (\n",
        "    cyano_species\n",
        "    .groupby(['reservoir','sci_name'])['density_cells_ml']\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "species_reservoir.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 5 \u2014 Top Species per Reservoir\n",
        "\n",
        "top_species_reservoir = (\n",
        "    species_reservoir\n",
        "    .sort_values(['reservoir','density_cells_ml'], ascending=False)\n",
        "    .groupby('reservoir')\n",
        "    .head(3)\n",
        ")\n",
        "\n",
        "top_species_reservoir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 6 \u2014 Heatmap of Species \u00d7 Reservoir\n",
        "\n",
        "pivot_species = species_reservoir.pivot_table(\n",
        "    index='sci_name',\n",
        "    columns='reservoir',\n",
        "    values='density_cells_ml',\n",
        "    aggfunc='mean'\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(35,60))\n",
        "plt.imshow(\n",
        "    np.log1p(pivot_species.fillna(0)),\n",
        "    aspect='auto'\n",
        ")\n",
        "plt.colorbar(label='log(1 + Density)')\n",
        "plt.yticks(range(len(pivot_species.index)), pivot_species.index)\n",
        "plt.xticks(range(len(pivot_species.columns)), pivot_species.columns, rotation=90)\n",
        "plt.title('Species Dominance Across Reservoirs')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PART 3: BLOOM CONTRIBUTION BY TAXONOMIC GROUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 7 \u2014 Taxonomic Group Contribution\n",
        "\n",
        "taxa_contribution = (\n",
        "    cyano_species\n",
        "    .groupby('FinalTaxaType')['density_cells_ml']\n",
        "    .mean()\n",
        "    .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "taxa_contribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 8 \u2014 Plot Taxonomic Contribution\n",
        "\n",
        "taxa_contribution.plot(\n",
        "    kind='bar',\n",
        "    figsize=(6,4),\n",
        "    title='Bloom Contribution by Cyanobacteria Taxonomic Group'\n",
        ")\n",
        "plt.ylabel('Mean Cell Density')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PART 4: SPECIES CONTRIBUTION TO BLOOM EVENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 9 \u2014 High-Bloom Condition Analysis\n",
        "\n",
        "# Define bloom threshold (top 25%)\n",
        "threshold = cyano_species['density_cells_ml'].quantile(0.75)\n",
        "\n",
        "bloom_events = cyano_species[\n",
        "    cyano_species['density_cells_ml'] >= threshold\n",
        "]\n",
        "\n",
        "bloom_species = (\n",
        "    bloom_events\n",
        "    .groupby('sci_name')['density_cells_ml']\n",
        "    .count()\n",
        "    .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "bloom_species.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udcca Baseline ML Models\n",
        "Random Forest & SVR for Cyanobacteria Bloom Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 1 \u2014 Imports & Setup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 2 \u2014 Define Target & Features\n",
        "TARGET = 'density_cells_ml'\n",
        "\n",
        "EXCLUDE_COLS = [\n",
        "    TARGET,\n",
        "    'reservoir'\n",
        "]\n",
        "\n",
        "FEATURES = [c for c in df.columns if c not in EXCLUDE_COLS]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 3 \u2014 Metric Function\n",
        "\n",
        "def regression_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        'R2': r2_score(y_true, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "        'MAE': mean_absolute_error(y_true, y_pred)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 4 \u2014 Reservoir-wise Train/Test Split (Time-Based)\n",
        "\n",
        "def time_based_split(sub_df, test_ratio=0.2):\n",
        "    sub_df = sub_df.sort_values(['year','month'])\n",
        "    split_idx = int(len(sub_df) * (1 - test_ratio))\n",
        "    train = sub_df.iloc[:split_idx]\n",
        "    test = sub_df.iloc[split_idx:]\n",
        "    return train, test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd39 MODEL 1: RANDOM FOREST REGRESSOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 5 \u2014 Train & Evaluate Random Forest (Per Reservoir)\n",
        "\n",
        "rf_results = []\n",
        "\n",
        "for reservoir in df['reservoir'].unique():\n",
        "    sub = df[df['reservoir'] == reservoir]\n",
        "\n",
        "    if len(sub) < 30:\n",
        "        continue\n",
        "\n",
        "    train, test = time_based_split(sub)\n",
        "\n",
        "    X_train = train[FEATURES]\n",
        "    y_train = train[TARGET]\n",
        "    X_test  = test[FEATURES]\n",
        "    y_test  = test[TARGET]\n",
        "\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=300,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    rf.fit(X_train, y_train)\n",
        "    preds = rf.predict(X_test)\n",
        "\n",
        "    metrics = regression_metrics(y_test, preds)\n",
        "    metrics['reservoir'] = reservoir\n",
        "    metrics['model'] = 'RandomForest'\n",
        "\n",
        "    rf_results.append(metrics)\n",
        "\n",
        "rf_results_df = pd.DataFrame(rf_results)\n",
        "rf_results_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd39 MODEL 2: SUPPORT VECTOR REGRESSION (SVR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 6 \u2014 Train & Evaluate SVR (Per Reservoir)\n",
        "\n",
        "svr_results = []\n",
        "\n",
        "for reservoir in df['reservoir'].unique():\n",
        "    sub = df[df['reservoir'] == reservoir]\n",
        "\n",
        "    if len(sub) < 30:\n",
        "        continue\n",
        "\n",
        "    train, test = time_based_split(sub)\n",
        "\n",
        "    X_train = train[FEATURES]\n",
        "    y_train = train[TARGET]\n",
        "    X_test  = test[FEATURES]\n",
        "    y_test  = test[TARGET]\n",
        "\n",
        "    # Scaling is REQUIRED for SVR\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "    svr = SVR(\n",
        "        kernel='rbf',\n",
        "        C=10,\n",
        "        epsilon=0.1\n",
        "    )\n",
        "\n",
        "    svr.fit(X_train_scaled, y_train)\n",
        "    preds = svr.predict(X_test_scaled)\n",
        "\n",
        "    metrics = regression_metrics(y_test, preds)\n",
        "    metrics['reservoir'] = reservoir\n",
        "    metrics['model'] = 'SVR'\n",
        "\n",
        "    svr_results.append(metrics)\n",
        "\n",
        "svr_results_df = pd.DataFrame(svr_results)\n",
        "svr_results_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 7 \u2014 Combine Results \n",
        "\n",
        "baseline_results = pd.concat([rf_results_df, svr_results_df])\n",
        "baseline_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 8 \u2014 Average Performance Across Reservoirs\n",
        "\n",
        "baseline_results.groupby('model')[['R2','RMSE','MAE']].mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CELL 9 \u2014 Reservoir-wise Performance Table\n",
        "\n",
        "baseline_results.pivot_table(\n",
        "    index='reservoir',\n",
        "    columns='model',\n",
        "    values=['R2','RMSE','MAE']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\ude80 XGBoost Reservoir-Wise Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 1 - Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 2 \u2014 Define Target & Features\n",
        "TARGET = 'density_cells_ml'\n",
        "\n",
        "EXCLUDE_COLS = [\n",
        "    TARGET,\n",
        "    'reservoir'\n",
        "]\n",
        "\n",
        "FEATURES = [c for c in df.columns if c not in EXCLUDE_COLS]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3 \u2014 Metric Function\n",
        "\n",
        "def regression_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        'R2': r2_score(y_true, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "        'MAE': mean_absolute_error(y_true, y_pred)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4 \u2014 Time-Based Train/Test Split\n",
        "def time_based_split(sub_df, test_ratio=0.2):\n",
        "    sub_df = sub_df.sort_values(['year','month'])\n",
        "    split_idx = int(len(sub_df) * (1 - test_ratio))\n",
        "    train = sub_df.iloc[:split_idx]\n",
        "    test = sub_df.iloc[split_idx:]\n",
        "    return train, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5 \u2014 XGBoost Reservoir-Wise Training\n",
        "\n",
        "xgb_results = []\n",
        "\n",
        "for reservoir in df['reservoir'].unique():\n",
        "    sub = df[df['reservoir'] == reservoir]\n",
        "\n",
        "    if len(sub) < 30:\n",
        "        continue\n",
        "\n",
        "    train, test = time_based_split(sub)\n",
        "\n",
        "    X_train = train[FEATURES]\n",
        "    y_train = train[TARGET]\n",
        "    X_test  = test[FEATURES]\n",
        "    y_test  = test[TARGET]\n",
        "\n",
        "    xgb = XGBRegressor(\n",
        "        n_estimators=500,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective='reg:squarederror',\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    xgb.fit(X_train, y_train)\n",
        "    preds = xgb.predict(X_test)\n",
        "\n",
        "    metrics = regression_metrics(y_test, preds)\n",
        "    metrics['reservoir'] = reservoir\n",
        "    metrics['model'] = 'XGBoost'\n",
        "\n",
        "    xgb_results.append(metrics)\n",
        "\n",
        "xgb_results_df = pd.DataFrame(xgb_results)\n",
        "xgb_results_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6 \u2014 Reservoir-Wise Performance Table\n",
        "\n",
        "xgb_results_df.sort_values('R2', ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 7 \u2014 Compare with Baseline Models\n",
        "\n",
        "all_results = pd.concat([baseline_results, xgb_results_df])\n",
        "\n",
        "comparison = all_results.pivot_table(\n",
        "    index='reservoir',\n",
        "    columns='model',\n",
        "    values=['R2','RMSE','MAE']\n",
        ")\n",
        "\n",
        "comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 8 \u2014 Average Performance Across Reservoirs\n",
        "all_results.groupby('model')[['R2','RMSE','MAE']].mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udde0 Hybrid CNN\u2013LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 1 \u2014 Imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 2 \u2014 Define Target & Features\n",
        "\n",
        "TARGET = 'density_cells_ml'\n",
        "\n",
        "EXCLUDE_COLS = [\n",
        "    TARGET,\n",
        "    'reservoir'\n",
        "]\n",
        "\n",
        "FEATURES = [c for c in df.columns if c not in EXCLUDE_COLS]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3 \u2014 Sequence Builder Function\n",
        "\n",
        "def create_sequences(X, y, window_size=6):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X) - window_size):\n",
        "        X_seq.append(X[i:i+window_size])\n",
        "        y_seq.append(y[i+window_size])\n",
        "    return np.array(X_seq), np.array(y_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4 \u2014 Train/Test Split (Time-Based)\n",
        "\n",
        "def time_series_split(X, y, test_ratio=0.2):\n",
        "    split = int(len(X) * (1 - test_ratio))\n",
        "    return X[:split], X[split:], y[:split], y[split:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd39 CNN\u2013LSTM MODEL DEFINITION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5 \u2014 CNN\u2013LSTM Architecture\n",
        "\n",
        "def build_cnn_lstm(input_shape):\n",
        "    model = Sequential([\n",
        "        Conv1D(\n",
        "            filters=64,\n",
        "            kernel_size=3,\n",
        "            activation='relu',\n",
        "            input_shape=input_shape\n",
        "        ),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        LSTM(64, return_sequences=False),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='mse'\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6 \u2014 Reservoir-wise Training & Evaluation\n",
        "\n",
        "results = []\n",
        "\n",
        "WINDOW_SIZE = 6\n",
        "\n",
        "for reservoir in df['reservoir'].unique():\n",
        "    sub = df[df['reservoir'] == reservoir].sort_values(['year','month'])\n",
        "\n",
        "    if len(sub) < 40:\n",
        "        continue\n",
        "\n",
        "    X = sub[FEATURES].values\n",
        "    y = sub[TARGET].values\n",
        "\n",
        "    # Log-transform target\n",
        "    y_log = np.log1p(y)\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Create sequences\n",
        "    X_seq, y_seq = create_sequences(X_scaled, y_log, WINDOW_SIZE)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = time_series_split(X_seq, y_seq)\n",
        "\n",
        "    model = build_cnn_lstm(\n",
        "        input_shape=(X_train.shape[1], X_train.shape[2])\n",
        "    )\n",
        "\n",
        "    es = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=100,\n",
        "        batch_size=16,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[es],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Predict\n",
        "    preds_log = model.predict(X_test).flatten()\n",
        "\n",
        "    # Inverse log transform\n",
        "    y_test_orig = np.expm1(y_test)\n",
        "    preds_orig = np.expm1(preds_log)\n",
        "\n",
        "    metrics = {\n",
        "        'reservoir': reservoir,\n",
        "        'model': 'CNN-LSTM',\n",
        "        'R2': r2_score(y_test_orig, preds_orig),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test_orig, preds_orig)),\n",
        "        'MAE': mean_absolute_error(y_test_orig, preds_orig)\n",
        "    }\n",
        "\n",
        "    results.append(metrics)\n",
        "\n",
        "cnn_lstm_results = pd.DataFrame(results)\n",
        "cnn_lstm_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 7 \u2014 Compare with Previous Models\n",
        "\n",
        "final_results = pd.concat([all_results, cnn_lstm_results])\n",
        "\n",
        "final_results.groupby('model')[['R2','RMSE','MAE']].mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd37 Transformer Time-Series Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, LayerNormalization,\n",
        "    MultiHeadAttention, Add, Embedding,\n",
        "    GlobalAveragePooling1D, Flatten, Concatenate\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.losses import Huber\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 2 \u2014 Target & Feature Selection\n",
        "TARGET = 'density_cells_ml'\n",
        "\n",
        "# Seasonal encoding\n",
        "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "\n",
        "EXCLUDE_COLS = [\n",
        "    TARGET,\n",
        "    'reservoir',\n",
        "    'month'\n",
        "]\n",
        "\n",
        "FEATURES = [c for c in df.columns if c not in EXCLUDE_COLS]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3 \u2014 Reservoir Encoding\n",
        "\n",
        "df['reservoir_id'] = df['reservoir'].astype('category').cat.codes\n",
        "FEATURES.append('reservoir_id')\n",
        "\n",
        "NUM_RESERVOIRS = df['reservoir_id'].nunique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4 \u2014 Multi-Horizon Sequence Generator (CRITICAL)\n",
        "\n",
        "def create_multi_horizon_sequences(df, features, target, window=12, horizon=6):\n",
        "    X, y, res_ids = [], [], []\n",
        "\n",
        "    for res in df['reservoir'].unique():\n",
        "        sub = df[df['reservoir'] == res].sort_values(['year','month'])\n",
        "\n",
        "        if len(sub) <= window + horizon:\n",
        "            continue\n",
        "\n",
        "        X_res = sub[features].values\n",
        "        y_res = np.log1p(sub[target].values)\n",
        "        rid   = sub['reservoir_id'].values\n",
        "\n",
        "        for i in range(len(sub) - window - horizon + 1):\n",
        "            X.append(X_res[i:i+window])\n",
        "            y.append(y_res[i+window:i+window+horizon])\n",
        "            res_ids.append(rid[i+window])\n",
        "\n",
        "    return np.array(X), np.array(y), np.array(res_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5 \u2014 Transformer Encoder (Attention Extractable)\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
        "\n",
        "    attn_layer = MultiHeadAttention(\n",
        "        key_dim=head_size,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout\n",
        "    )\n",
        "\n",
        "    attn_output = attn_layer(inputs, inputs)\n",
        "\n",
        "    x = Dropout(dropout)(attn_output)\n",
        "    x = Add()([x, inputs])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    ff = Dense(ff_dim, activation='relu')(x)\n",
        "    ff = Dense(inputs.shape[-1])(ff)\n",
        "    x = Add()([ff, x])\n",
        "\n",
        "    return LayerNormalization(epsilon=1e-6)(x), attn_layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6 \u2014 FULL TRANSFORMER MODEL (Multi-Horizon + Embedding)\n",
        "\n",
        "def build_transformer(input_shape, num_reservoirs, horizon=6, embed_dim=8):\n",
        "\n",
        "    ts_input = Input(shape=input_shape, name='ts_input')\n",
        "    res_input = Input(shape=(1,), name='res_input')\n",
        "\n",
        "    # Reservoir embedding\n",
        "    res_embed = Embedding(\n",
        "        input_dim=num_reservoirs,\n",
        "        output_dim=embed_dim\n",
        "    )(res_input)\n",
        "    res_embed = Flatten()(res_embed)\n",
        "\n",
        "    # Transformer block\n",
        "    x, attn_layer = transformer_encoder(\n",
        "        ts_input,\n",
        "        head_size=64,\n",
        "        num_heads=4,\n",
        "        ff_dim=128,\n",
        "        dropout=0.2\n",
        "    )\n",
        "\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "    x = Concatenate()([x, res_embed])\n",
        "\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    outputs = Dense(horizon)(x)\n",
        "\n",
        "    model = Model(\n",
        "        inputs=[ts_input, res_input],\n",
        "        outputs=outputs\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=Huber(delta=1.0)\n",
        "    )\n",
        "\n",
        "    # Store attention layer for later extraction\n",
        "    model.attn_layer = attn_layer\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 7 \u2014 Create Sequences & Scale\n",
        "\n",
        "WINDOW = 12\n",
        "HORIZON = 6\n",
        "\n",
        "X_seq, y_seq, res_id_seq = create_multi_horizon_sequences(\n",
        "    df,\n",
        "    FEATURES,\n",
        "    TARGET,\n",
        "    window=WINDOW,\n",
        "    horizon=HORIZON\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "n, t, f = X_seq.shape\n",
        "\n",
        "X_scaled = scaler.fit_transform(\n",
        "    X_seq.reshape(-1, f)\n",
        ").reshape(n, t, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 8 \u2014 Time-Based Split\n",
        "\n",
        "split = int(len(X_scaled) * 0.8)\n",
        "\n",
        "X_train, X_test = X_scaled[:split], X_scaled[split:]\n",
        "y_train, y_test = y_seq[:split], y_seq[split:]\n",
        "res_id_train, res_id_test = res_id_seq[:split], res_id_seq[split:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 9 \u2014 Train Model\n",
        "\n",
        "model = build_transformer(\n",
        "    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
        "    num_reservoirs=NUM_RESERVOIRS,\n",
        "    horizon=HORIZON\n",
        ")\n",
        "\n",
        "es = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    [X_train, res_id_train],\n",
        "    y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[es],\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 10 \u2014 Multi-Horizon Evaluation\n",
        "\n",
        "preds_log = model.predict([X_test, res_id_test])\n",
        "\n",
        "preds_orig = np.expm1(preds_log)\n",
        "y_test_orig = np.expm1(y_test)\n",
        "\n",
        "for h in range(HORIZON):\n",
        "    r2 = r2_score(y_test_orig[:,h], preds_orig[:,h])\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_orig[:,h], preds_orig[:,h]))\n",
        "    mae = mean_absolute_error(y_test_orig[:,h], preds_orig[:,h])\n",
        "\n",
        "    print(f\"Month +{h+1}: R2={r2:.3f}, RMSE={rmse:.1f}, MAE={mae:.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 11 \u2014 Attention Weight Visualization (CORRECT)\n",
        "\n",
        "# Build attention probe model\n",
        "attn_probe = Model(\n",
        "    inputs=model.inputs,\n",
        "    outputs=model.attn_layer(\n",
        "        model.inputs[0],\n",
        "        model.inputs[0],\n",
        "        return_attention_scores=True\n",
        "    )[1]\n",
        ")\n",
        "\n",
        "# Extract attention for one sample\n",
        "attn_scores = attn_probe.predict([X_test[:1], res_id_test[:1]])\n",
        "\n",
        "# Average across heads and feature dimensions\n",
        "avg_attn = attn_scores.mean(axis=(1, 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 12 \u2014 Plot Temporal Attention\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(avg_attn[0])\n",
        "plt.xlabel(\"Past Months (Time Steps)\")\n",
        "plt.ylabel(\"Attention Weight\")\n",
        "plt.title(\"Transformer Temporal Attention Pattern\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udde0 Graph Neural Network (GNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 1 - Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 2 \u2014 Prepare Node Features (Reservoir-Level) \n",
        "\n",
        "feature_cols = [\n",
        "    c for c in df.columns\n",
        "    if c not in ['density_cells_ml', 'reservoir']\n",
        "]\n",
        "\n",
        "node_features = (\n",
        "    df\n",
        "    .groupby('reservoir')[feature_cols]\n",
        "    .mean()\n",
        ")\n",
        "\n",
        "node_features_scaled = (\n",
        "    node_features - node_features.mean()\n",
        ") / node_features.std()\n",
        "\n",
        "X_nodes = torch.tensor(\n",
        "    node_features_scaled.values,\n",
        "    dtype=torch.float\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3 \u2014 Target Variable (Node-Level)\n",
        "# We predict mean bloom intensity per reservoir.\n",
        "y_nodes = (\n",
        "    df\n",
        "    .groupby('reservoir')['density_cells_ml']\n",
        "    .mean()\n",
        ")\n",
        "\n",
        "y_nodes = torch.tensor(\n",
        "    y_nodes.values,\n",
        "    dtype=torch.float\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4 \u2014 Build Graph Edges via Similarity\n",
        "\n",
        "similarity = cosine_similarity(node_features_scaled.values)\n",
        "\n",
        "edge_index = []\n",
        "\n",
        "K = 3  # each reservoir connects to its 3 most similar reservoirs\n",
        "\n",
        "for i in range(similarity.shape[0]):\n",
        "    # Get indices of top-K similar reservoirs (excluding itself)\n",
        "    neighbors = np.argsort(similarity[i])[-(K+1):-1]\n",
        "\n",
        "    for j in neighbors:\n",
        "        edge_index.append([i, j])\n",
        "        edge_index.append([j, i])  # make graph undirected\n",
        "\n",
        "edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
        "\n",
        "print(\"Edge index shape:\", edge_index.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5 \u2014 Create PyG Graph Object \n",
        "data = Data(\n",
        "    x=X_nodes,\n",
        "    edge_index=edge_index,\n",
        "    y=y_nodes\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6 \u2014 Define GNN Model (GCN) \n",
        "\n",
        "class ReservoirGNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 64)\n",
        "        self.conv2 = GCNConv(64, 32)\n",
        "        self.lin = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        return self.lin(x).squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 7 \u2014 Train/Test Split (Node-Level)\n",
        "\n",
        "num_nodes = data.num_nodes\n",
        "indices = np.arange(num_nodes)\n",
        "\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "split = int(0.8 * num_nodes)\n",
        "train_idx = indices[:split]\n",
        "test_idx = indices[split:]\n",
        "\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "train_mask[train_idx] = True\n",
        "test_mask[test_idx] = True\n",
        "\n",
        "data.train_mask = train_mask\n",
        "data.test_mask = test_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 8 \u2014 Train GNN\n",
        "\n",
        "model = ReservoirGNN(in_channels=X_nodes.shape[1])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(300):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(data)\n",
        "    loss = F.mse_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 9 \u2014 Evaluate GNN\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds = model(data)\n",
        "\n",
        "y_true = data.y[data.test_mask].numpy()\n",
        "y_pred = preds[data.test_mask].numpy()\n",
        "\n",
        "gnn_metrics = {\n",
        "    'model': 'GNN',\n",
        "    'R2': r2_score(y_true, y_pred),\n",
        "    'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "    'MAE': mean_absolute_error(y_true, y_pred)\n",
        "}\n",
        "\n",
        "gnn_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 10 \u2014 Compare with All Models\n",
        "final_results = pd.concat([\n",
        "    final_results,\n",
        "    pd.DataFrame([gnn_metrics])\n",
        "])\n",
        "\n",
        "final_results.groupby('model')[['R2','RMSE','MAE']].mean()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}