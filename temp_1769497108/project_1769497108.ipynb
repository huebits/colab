{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcd8 1 \u2014 Project Introduction\n",
        "\n",
        "# \ud83d\udd25 AI-Based Wildfire Detection & Forecasting System\n",
        "\n",
        "This project provides:\n",
        "\n",
        "* \ud83d\udd25 Fire & Non-Fire Image Classification\n",
        "* \ud83e\udde0 Deep Learning using CBAM-ResNet18\n",
        "* \ud83d\udcca Confusion Matrix & Classification Report\n",
        "* \ud83e\udde9 Grad-CAM Visual Attention Mapping\n",
        "* \ud83c\udf21\ufe0f DH3-Based Fire Spread Forecast (Simulated)\n",
        "* \ud83c\udf0d Public Deployment using Flask + ngrok\n",
        "\n",
        "### \u2705 Supported Classes:\n",
        "\n",
        "* Fire\n",
        "* Non-Fire\n",
        "\n",
        "This notebook performs:\n",
        "\n",
        "1. Dataset Loading\n",
        "2. Dependency Installation\n",
        "3. Model Training with CBAM Attention\n",
        "4. Model Evaluation & Metrics\n",
        "5. Grad-CAM & Fire Spread Forecasting\n",
        "6. Flask Web App Creation\n",
        "7. Public Deployment via ngrok\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcd8 2 \u2014 Dataset Path Setup & Verification\n",
        "\n",
        "This step:\n",
        "\n",
        "* Mounts Google Drive\n",
        "* Loads Fire & Non-Fire image folders\n",
        "* Verifies dataset existence\n",
        "* Displays sample images\n",
        "\n",
        "# ===============================\n",
        "\n",
        "# \u2705 CELL 1: Dataset Path Setup\n",
        "\n",
        "# ===============================\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MQrnGwNnJ36O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please use the following link in your browser to download the dataset:\n",
        "\n",
        "https://www.kaggle.com/datasets/huebitsvizg/wildfire-detection-dataset"
      ],
      "metadata": {
        "id": "ja7uLuCQSNqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcd8 3 \u2014 Download Kaggle Wildfire Dataset & Load Dataset\n",
        "\n",
        "This step:\n",
        "\n",
        "* Downloads the Wildfire dataset directly from Kaggle\n",
        "* Extracts Fire & Non-Fire image folders\n",
        "* Verifies dataset existence\n",
        "* Displays sample images\n",
        "* \u2705 **Completely removes Google Drive dependency**\n",
        "\n",
        "### \u2705 Dataset Link Used\n",
        "\n",
        "\ud83d\udc49 [https://www.kaggle.com/datasets/huebitsvizg/wildfire-detection-dataset](https://www.kaggle.com/datasets/huebitsvizg/wildfire-detection-dataset)\n",
        "\n",
        "\u2705 **Yes, you can absolutely use this Kaggle dataset instead of Google Drive** \u2014 and it\u2019s actually a **better, cleaner, and more professional approach** for your project \ud83d\udc4f\n",
        "\n",
        "This means you will **REMOVE Google Drive mounting completely** and **LOAD the dataset directly from Kaggle into `/content/`**.\n",
        "\n",
        "---\n",
        "\n",
        "## \u2705 WHAT YOU SHOULD REPLACE (Your Old Code \u274c)\n",
        "\n",
        "You will **REMOVE this entire block**:\n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# \ud83d\udd01 CHANGE THESE TWO PATHS IF YOUR FOLDER NAME IS DIFFERENT\n",
        "FIRE_DIR = \"/content/drive/MyDrive/Sasi Projects/WildFire Dataset/fire_dataset/fire_images\"\n",
        "NONFIRE_DIR = \"/content/drive/MyDrive/Sasi Projects/WildFire Dataset/fire_dataset/non_fire_images\"\n",
        "\n",
        "import os, glob\n",
        "\n",
        "print(\"Fire Exists:\", os.path.exists(FIRE_DIR))\n",
        "print(\"Non-Fire Exists:\", os.path.exists(NONFIRE_DIR))\n",
        "\n",
        "print(\"\\nSample Fire Images:\", glob.glob(FIRE_DIR + \"/*\")[:5])\n",
        "print(\"Sample Non-Fire Images:\", glob.glob(NONFIRE_DIR + \"/*\")[:5])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## \u2705 NEW PROFESSIONAL KAGGLE DATASET SETUP (FINAL \u2705)\n",
        "\n",
        "### \ud83d\udcd8 New Notebook Cell \u2014 *Download Wildfire Dataset from Kaggle*\n",
        "\n",
        "# ===============================\n",
        "\n",
        "# \u2705 CELL 2: Kaggle Dataset Download\n",
        "\n",
        "# ===============================\n",
        "\n",
        "```python\n",
        "!pip install -q kaggle\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udcd8 Upload Kaggle API Key (ONE TIME STEP)\n",
        "\n",
        "1. Go to \ud83d\udc49 **[https://www.kaggle.com/settings](https://www.kaggle.com/settings)**\n",
        "2. Scroll to **API**\n",
        "3. Click **Create New Token**\n",
        "4. A file named `kaggle.json` will download\n",
        "5. Upload it to Colab using this:\n",
        "\n",
        "```python\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udcd8 Configure Kaggle & Download Dataset\n",
        "\n",
        "```python\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "```\n",
        "\n",
        "```python\n",
        "# \u2705 Download Wildfire Detection Dataset\n",
        "!kaggle datasets download -d huebitsvizg/wildfire-detection-dataset\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udcd8 Extract Dataset\n",
        "\n",
        "```python\n",
        "!unzip -q wildfire-detection-dataset.zip\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udcd8 Set FINAL Dataset Paths \u2705 (THIS replaces your Drive paths)\n",
        "\n",
        "```python\n",
        "import os, glob\n",
        "\n",
        "FIRE_DIR = \"/content/wildfire-detection-dataset/fire\"\n",
        "NONFIRE_DIR = \"/content/wildfire-detection-dataset/non_fire\"\n",
        "\n",
        "print(\"\u2705 Fire Exists:\", os.path.exists(FIRE_DIR))\n",
        "print(\"\u2705 Non-Fire Exists:\", os.path.exists(NONFIRE_DIR))\n",
        "\n",
        "print(\"\\n\u2705 Sample Fire Images:\", glob.glob(FIRE_DIR + \"/*\")[:5])\n",
        "print(\"\u2705 Sample Non-Fire Images:\", glob.glob(NONFIRE_DIR + \"/*\")[:5])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## \u2705 FINAL ANSWER TO YOUR QUESTION (FOR WILDFIRE PROJECT)\n",
        "\n",
        "| Old Method                 | New Method                 |\n",
        "| -------------------------- | -------------------------- |\n",
        "| Google Drive manual upload | \u2705 Direct Kaggle Download   |\n",
        "| Risk of missing files      | \u2705 Clean structured dataset |\n",
        "| Slow access                | \u2705 Faster training          |\n",
        "| Manual dataset handling    | \u2705 Fully automated          |\n",
        "\n",
        "\u2705 **You should now use these paths in ALL your wildfire training code:**\n",
        "\n",
        "```python\n",
        "FIRE_DIR = \"/content/wildfire-detection-dataset/fire\"\n",
        "NONFIRE_DIR = \"/content/wildfire-detection-dataset/non_fire\"\n",
        "```"
      ],
      "metadata": {
        "id": "9wU1BKSgRMp0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9zHUpo8FCLj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# \ud83d\udd01 CHANGE THESE TWO PATHS IF YOUR FOLDER NAME IS DIFFERENT\n",
        "FIRE_DIR = \"/content/drive/MyDrive/Sasi Projects/WildFire Dataset/fire_dataset/fire_images\"\n",
        "NONFIRE_DIR = \"/content/drive/MyDrive/Sasi Projects/WildFire Dataset/fire_dataset/non_fire_images\"\n",
        "\n",
        "import os, glob\n",
        "\n",
        "print(\"Fire Exists:\", os.path.exists(FIRE_DIR))\n",
        "print(\"Non-Fire Exists:\", os.path.exists(NONFIRE_DIR))\n",
        "\n",
        "print(\"\\nSample Fire Images:\", glob.glob(FIRE_DIR + \"/*\")[:5])\n",
        "print(\"Sample Non-Fire Images:\", glob.glob(NONFIRE_DIR + \"/*\")[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcd8 3 \u2014 Install Dependencies & Create Project Folders\n",
        "\n",
        "This step installs all required libraries and creates the project directory structure:\n",
        "\n",
        "* Flask & ngrok\n",
        "* PyTorch & Torchvision\n",
        "* OpenCV & Image Processing\n",
        "* Metrics & Visualization Tools\n",
        "\n",
        "It also creates folders for:\n",
        "\n",
        "* Models\n",
        "* Templates\n",
        "* Static Files\n",
        "* Uploads\n",
        "* Results\n",
        "\n",
        "# ===============================\n",
        "\n",
        "# \u2705 CELL 2: Install Dependencies & Create Folders\n",
        "\n",
        "# ===============================\n",
        "\n"
      ],
      "metadata": {
        "id": "hlw3A_IYKzCJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hAnjWsqFJyI"
      },
      "outputs": [],
      "source": [
        "!pip install -q flask pyngrok torch torchvision pillow opencv-python matplotlib seaborn scikit-learn tqdm\n",
        "\n",
        "!mkdir -p wildfire_project/models\n",
        "!mkdir -p wildfire_project/templates\n",
        "!mkdir -p wildfire_project/static\n",
        "!mkdir -p wildfire_project/uploads\n",
        "!mkdir -p wildfire_project/results\n",
        "\n",
        "print(\"\ud83d\udd25 Packages installed and project folders created.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcd8 4 \u2014 Training Script (CBAM-ResNet18 + Metrics)\n",
        "\n",
        "This step defines the full deep learning training pipeline including:\n",
        "\n",
        "* Dataset Copying to Temp Directory\n",
        "* Data Augmentation & Normalization\n",
        "* CBAM Attention Mechanism\n",
        "* ResNet18 Backbone\n",
        "* Training & Validation Loop\n",
        "* Model Checkpoint Saving\n",
        "* Confusion Matrix & Classification Report\n",
        "\n",
        "# ===============================\n",
        "\n",
        "# \u2705 CELL 3: Training Script\n",
        "\n",
        "# ===============================\n",
        "\n"
      ],
      "metadata": {
        "id": "QbMNzJbRLGWT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGEZllypFVG0"
      },
      "outputs": [],
      "source": [
        "%%writefile wildfire_project/train_and_eval.py\n",
        "import os, random, glob, shutil\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# === Paths from environment (set in notebook) ===\n",
        "FIRE_DIR = os.environ[\"FIRE_DIR\"]\n",
        "NONFIRE_DIR = os.environ[\"NONFIRE_DIR\"]\n",
        "\n",
        "BASE = \"wildfire_project\"\n",
        "MODEL_DIR = os.path.join(BASE, \"models\")\n",
        "RESULTS_DIR = os.path.join(BASE, \"results\")\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "TMP = \"/tmp/wildfire_dataset\"\n",
        "if os.path.exists(TMP):\n",
        "    shutil.rmtree(TMP)\n",
        "\n",
        "os.makedirs(os.path.join(TMP, \"fire\"))\n",
        "os.makedirs(os.path.join(TMP, \"non_fire\"))\n",
        "\n",
        "def copy_all(src_pattern, dst_folder):\n",
        "    files = glob.glob(src_pattern)\n",
        "    for f in files:\n",
        "        shutil.copy(f, dst_folder)\n",
        "\n",
        "copy_all(os.path.join(FIRE_DIR, \"*\"), os.path.join(TMP, \"fire\"))\n",
        "copy_all(os.path.join(NONFIRE_DIR, \"*\"), os.path.join(TMP, \"non_fire\"))\n",
        "\n",
        "print(\"\ud83d\udd25 Fire Count:\", len(os.listdir(os.path.join(TMP, \"fire\"))))\n",
        "print(\"\ud83c\udf3f Non-Fire Count:\", len(os.listdir(os.path.join(TMP, \"non_fire\"))))\n",
        "\n",
        "# === Transforms ===\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "full = datasets.ImageFolder(TMP, transform=train_tf)\n",
        "print(\"Classes:\", full.classes)  # Expect ['fire', 'non_fire']\n",
        "\n",
        "indices = list(range(len(full)))\n",
        "random.shuffle(indices)\n",
        "split = int(0.8 * len(indices))\n",
        "\n",
        "train_ds = Subset(full, indices[:split])\n",
        "val_ds = Subset(datasets.ImageFolder(TMP, transform=val_tf), indices[split:])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
        "\n",
        "# === CBAM Attention Blocks ===\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, c):\n",
        "        super().__init__()\n",
        "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(c, c//8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(c//8, c),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        b,c,_,_ = x.size()\n",
        "        y = self.avg(x).view(b,c)\n",
        "        y = self.fc(y).view(b,c,1,1)\n",
        "        return x * y\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n",
        "    def forward(self, x):\n",
        "        avg = torch.mean(x, dim=1, keepdim=True)\n",
        "        mx, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        y = torch.cat([avg, mx], dim=1)\n",
        "        return x * torch.sigmoid(self.conv(y))\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, c):\n",
        "        super().__init__()\n",
        "        self.ca = ChannelAttention(c)\n",
        "        self.sa = SpatialAttention()\n",
        "    def forward(self, x):\n",
        "        return self.sa(self.ca(x))\n",
        "\n",
        "# === Model ===\n",
        "model = models.resnet18(weights=None)\n",
        "model.layer4.add_module(\"cbam\", CBAM(512))\n",
        "model.fc = nn.Linear(512, 2)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "best_acc = 0.0\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "for epoch in range(8):\n",
        "    # ---- Train ----\n",
        "    model.train()\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # ---- Validate ----\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    epoch_preds = []\n",
        "    epoch_labels = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            epoch_preds.extend(preds.cpu().tolist())\n",
        "            epoch_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f\"Epoch {epoch+1}/8 - Val Acc: {acc:.4f}\")\n",
        "\n",
        "    if acc > best_acc:\n",
        "        torch.save(model.state_dict(), os.path.join(MODEL_DIR, \"wildfire_classifier.pth\"))\n",
        "        best_acc = acc\n",
        "        print(\"\u2714 Saved new best model\")\n",
        "\n",
        "    all_preds = epoch_preds\n",
        "    all_labels = epoch_labels\n",
        "\n",
        "# === Metrics ===\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "report = classification_report(all_labels, all_preds, target_names=full.classes)\n",
        "print(\"\\nClassification Report:\\n\", report)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "with open(os.path.join(RESULTS_DIR, \"classification_report.txt\"), \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=full.classes, yticklabels=full.classes, cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULTS_DIR, \"confusion_matrix.png\"))\n",
        "\n",
        "print(\"\\n\ud83c\udf89 Training Completed. Metrics saved in wildfire_project/results\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcd8 5 \u2014 Run Model Training\n",
        "\n",
        "This step:\n",
        "\n",
        "* Injects dataset paths as environment variables\n",
        "* Executes the full model training pipeline\n",
        "* Saves the best performing model\n",
        "* Stores evaluation metrics\n",
        "\n",
        "# ===============================\n",
        "\n",
        "# \u2705 CELL 4: Run Training\n",
        "\n",
        "# ==============================="
      ],
      "metadata": {
        "id": "NkEnsJUXLRoH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r1tTPoCbFYFo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"FIRE_DIR\"] = FIRE_DIR\n",
        "os.environ[\"NONFIRE_DIR\"] = NONFIRE_DIR\n",
        "\n",
        "!python wildfire_project/train_and_eval.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcd8 6 \u2014 Utilities: Grad-CAM + DH3 + Model Loader\n",
        "\n",
        "This step prepares all post-processing utilities including:\n",
        "\n",
        "* Model Loader for CBAM-ResNet18\n",
        "* Grad-CAM Attention Heatmap Generator\n",
        "* DH3-Based Fire Spread Forecast Simulator\n",
        "\n",
        "# ===============================\n",
        "\n",
        "# \u2705 CELL 5: Utilities & Model Loader\n",
        "\n",
        "# ==============================="
      ],
      "metadata": {
        "id": "DSsCsQQaLgGb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-zHh3Om8Cdf"
      },
      "outputs": [],
      "source": [
        "%%writefile wildfire_project/utils_and_models.py\n",
        "import torch, cv2, numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "import torch.nn as nn\n",
        "\n",
        "# === Load CBAM-ResNet18 classifier ===\n",
        "def load_classifier(model_path, device=None):\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    model = models.resnet18(weights=None)\n",
        "\n",
        "    class ChannelAttention(nn.Module):\n",
        "        def __init__(self, c):\n",
        "            super().__init__()\n",
        "            self.avg = nn.AdaptiveAvgPool2d(1)\n",
        "            self.fc = nn.Sequential(\n",
        "                nn.Linear(c, c//8),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(c//8, c),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "        def forward(self, x):\n",
        "            b,c,_,_ = x.size()\n",
        "            y = self.avg(x).view(b,c)\n",
        "            y = self.fc(y).view(b,c,1,1)\n",
        "            return x * y\n",
        "\n",
        "    class SpatialAttention(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.conv = nn.Conv2d(2,1,7,padding=3)\n",
        "        def forward(self, x):\n",
        "            avg = torch.mean(x,1,keepdim=True)\n",
        "            mx, _ = torch.max(x,1,keepdim=True)\n",
        "            y = torch.cat([avg,mx],1)\n",
        "            return x * torch.sigmoid(self.conv(y))\n",
        "\n",
        "    class CBAM(nn.Module):\n",
        "        def __init__(self, c):\n",
        "            super().__init__()\n",
        "            self.ca = ChannelAttention(c)\n",
        "            self.sa = SpatialAttention()\n",
        "        def forward(self, x):\n",
        "            return self.sa(self.ca(x))\n",
        "\n",
        "    model.layer4.add_module(\"cbam\", CBAM(512))\n",
        "    model.fc = nn.Linear(512, 2)\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device).eval()\n",
        "    return model\n",
        "\n",
        "# === Grad-CAM ===\n",
        "def grad_cam(model, img_path, target_class, size=(224,224)):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "    ])\n",
        "\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    inp = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    activations = None\n",
        "    gradients = None\n",
        "\n",
        "    def forward_hook(module, input, output):\n",
        "        nonlocal activations\n",
        "        activations = output\n",
        "\n",
        "    def backward_hook(module, grad_in, grad_out):\n",
        "        nonlocal gradients\n",
        "        gradients = grad_out[0]\n",
        "\n",
        "    h1 = model.layer4.register_forward_hook(forward_hook)\n",
        "    h2 = model.layer4.register_full_backward_hook(backward_hook)\n",
        "\n",
        "    out = model(inp)\n",
        "    score = out[0, target_class]\n",
        "    model.zero_grad()\n",
        "    score.backward()\n",
        "\n",
        "    h1.remove()\n",
        "    h2.remove()\n",
        "\n",
        "    pooled = torch.mean(gradients, dim=(0,2,3))\n",
        "    activ = activations[0]\n",
        "\n",
        "    for i in range(activ.shape[0]):\n",
        "        activ[i] *= pooled[i]\n",
        "\n",
        "    heatmap = torch.sum(activ, dim=0).detach().cpu().numpy()\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "    heatmap = heatmap / (heatmap.max() + 1e-8)\n",
        "    heatmap = cv2.resize(heatmap, img.size)\n",
        "    heatmap_uint8 = (heatmap * 255).astype(np.uint8)\n",
        "\n",
        "    overlay = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
        "    original = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "    final = cv2.addWeighted(original, 0.6, overlay, 0.4, 0)\n",
        "\n",
        "    return final, heatmap_uint8\n",
        "\n",
        "# === DH3-like Forecast (simulated spread) ===\n",
        "def dh3_forecast_from_heatmap(heatmap_uint8, steps=3):\n",
        "    kernel = np.ones((5,5), np.uint8)\n",
        "    forecasts = []\n",
        "    cur = heatmap_uint8.copy()\n",
        "    for i in range(steps):\n",
        "        cur = cv2.dilate(cur, kernel, iterations=1)\n",
        "        cur = np.clip(cur - i*15, 0, 255)\n",
        "        forecasts.append(cur.copy())\n",
        "    return forecasts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcd8 7 \u2014 Create Frontend Web Interface\n",
        "\n",
        "This step builds the complete user interface which supports:\n",
        "\n",
        "* Image Upload\n",
        "* Fire Detection Result Display\n",
        "* Grad-CAM Visualization\n",
        "* DH3 Forecast Visualization\n",
        "\n",
        "# ===============================\n",
        "\n",
        "# \u2705 CELL 6: Create index.html\n",
        "\n",
        "# ===============================\n",
        "\n"
      ],
      "metadata": {
        "id": "66Z6pWK3L3ud"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9iJ9Ft08CaQ"
      },
      "outputs": [],
      "source": [
        "%%writefile wildfire_project/templates/index.html\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>\ud83d\udd25 Wildfire Detection & Forecasting</title>\n",
        "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\">\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container\">\n",
        "    <h1>\ud83d\udd25 Wildfire Detection & DH3 Forecast</h1>\n",
        "    <p class=\"subtitle\">Upload an image to detect fire, view attention map (Grad-CAM), and forecast spread.</p>\n",
        "\n",
        "    {% if error %}\n",
        "      <div class=\"error\">{{ error }}</div>\n",
        "    {% endif %}\n",
        "\n",
        "    <div class=\"card\">\n",
        "      <form method=\"POST\" enctype=\"multipart/form-data\">\n",
        "        <div class=\"upload-area\">\n",
        "          <input type=\"file\" name=\"image\" accept=\"image/*\" required>\n",
        "          <p>Click to upload Fire / Non-Fire image</p>\n",
        "        </div>\n",
        "        <button type=\"submit\">Detect</button>\n",
        "      </form>\n",
        "    </div>\n",
        "\n",
        "    {% if result_text %}\n",
        "    <div class=\"results\">\n",
        "      <h2>{{ result_text }}</h2>\n",
        "\n",
        "      <div class=\"images-row\">\n",
        "        <div class=\"image-box\">\n",
        "          <h3>Uploaded Image</h3>\n",
        "          <img src=\"{{ url_for('upl', fn=uploaded_file) }}\">\n",
        "        </div>\n",
        "\n",
        "        <div class=\"image-box\">\n",
        "          <h3>Grad-CAM Attention</h3>\n",
        "          <img src=\"{{ url_for('res', fn=attention_image) }}\">\n",
        "        </div>\n",
        "      </div>\n",
        "\n",
        "      <h3>DH3 Forecast (Simulated Spread)</h3>\n",
        "      <div class=\"forecast-row\">\n",
        "        {% for f in forecast_images %}\n",
        "          <img src=\"{{ url_for('res', fn=f) }}\" width=\"200\">\n",
        "        {% endfor %}\n",
        "      </div>\n",
        "    </div>\n",
        "    {% endif %}\n",
        "</div>\n",
        "</body>\n",
        "</html>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcd8 8 \u2014 UI Styling using CSS\n",
        "\n",
        "This step styles the complete web application including:\n",
        "\n",
        "* Dark Theme UI\n",
        "* Image Display Boxes\n",
        "* Buttons & Upload Areas\n",
        "* Forecast Image Grid\n",
        "\n",
        "# ===============================\n",
        "\n",
        "# \u2705 CELL 6 (Continued): Create style.css\n",
        "\n",
        "# ==============================="
      ],
      "metadata": {
        "id": "-8ZJoaqjMdDF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHQ73Aj98CYW"
      },
      "outputs": [],
      "source": [
        "%%writefile wildfire_project/static/style.css\n",
        "body {\n",
        "    background: #222;\n",
        "    color: #f8f8f8;\n",
        "    font-family: Arial, sans-serif;\n",
        "    text-align: center;\n",
        "    padding: 20px;\n",
        "}\n",
        ".container {\n",
        "    max-width: 1000px;\n",
        "    margin: 0 auto;\n",
        "}\n",
        ".subtitle {\n",
        "    opacity: 0.9;\n",
        "    margin-bottom: 20px;\n",
        "}\n",
        ".card {\n",
        "    background: #333;\n",
        "    padding: 20px;\n",
        "    border-radius: 10px;\n",
        "    margin-bottom: 20px;\n",
        "}\n",
        ".upload-area {\n",
        "    border: 2px dashed #888;\n",
        "    padding: 20px;\n",
        "    border-radius: 10px;\n",
        "    margin-bottom: 10px;\n",
        "}\n",
        "button {\n",
        "    padding: 10px 20px;\n",
        "    background: #4ade80;\n",
        "    border: none;\n",
        "    border-radius: 8px;\n",
        "    font-size: 16px;\n",
        "    cursor: pointer;\n",
        "    font-weight: bold;\n",
        "}\n",
        "button:hover {\n",
        "    background: #22c55e;\n",
        "}\n",
        ".images-row {\n",
        "    display: flex;\n",
        "    justify-content: center;\n",
        "    gap: 20px;\n",
        "    margin-top: 20px;\n",
        "    flex-wrap: wrap;\n",
        "}\n",
        ".image-box img {\n",
        "    max-width: 300px;\n",
        "    border-radius: 10px;\n",
        "}\n",
        ".forecast-row img {\n",
        "    margin: 8px;\n",
        "    border-radius: 10px;\n",
        "}\n",
        ".error {\n",
        "    background: #b91c1c;\n",
        "    padding: 10px;\n",
        "    border-radius: 8px;\n",
        "    margin-bottom: 10px;\n",
        "}\n",
        ".results {\n",
        "    margin-top: 20px;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcd8 9 \u2014 Flask Backend Application\n",
        "\n",
        "This step builds the Flask web server that supports:\n",
        "\n",
        "* Image Upload\n",
        "* Fire Prediction\n",
        "* Grad-CAM Overlay Generation\n",
        "* DH3 Forecast Image Generation\n",
        "* Result Visualization\n",
        "\n",
        "# ===============================\n",
        "\n",
        "# \u2705 CELL 7: Flask App\n",
        "\n",
        "# ==============================="
      ],
      "metadata": {
        "id": "NX59XJEnNIb6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzMi6XwL8CWd"
      },
      "outputs": [],
      "source": [
        "%%writefile wildfire_project/app.py\n",
        "from flask import Flask, render_template, request, send_from_directory\n",
        "import os, uuid, cv2\n",
        "from utils_and_models import load_classifier, grad_cam, dh3_forecast_from_heatmap\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "BASE = \"/content/wildfire_project\"\n",
        "UPLOAD_FOLDER = os.path.join(BASE, \"uploads\")\n",
        "RESULT_FOLDER = os.path.join(BASE, \"results\")\n",
        "\n",
        "app = Flask(__name__,\n",
        "            template_folder=os.path.join(BASE, \"templates\"),\n",
        "            static_folder=os.path.join(BASE, \"static\"))\n",
        "\n",
        "model = load_classifier(os.path.join(BASE, \"models\", \"wildfire_classifier.pth\"))\n",
        "\n",
        "def allowed_file(filename):\n",
        "    return \".\" in filename and filename.rsplit(\".\",1)[1].lower() in {\"jpg\",\"jpeg\",\"png\"}\n",
        "\n",
        "@app.route(\"/\", methods=[\"GET\",\"POST\"])\n",
        "def home():\n",
        "    if request.method == \"GET\":\n",
        "        return render_template(\"index.html\")\n",
        "\n",
        "    file = request.files.get(\"image\")\n",
        "    if file is None or file.filename == \"\":\n",
        "        return render_template(\"index.html\", error=\"No file uploaded\")\n",
        "    if not allowed_file(file.filename):\n",
        "        return render_template(\"index.html\", error=\"Invalid file type\")\n",
        "\n",
        "    filename = f\"{uuid.uuid4().hex}.jpg\"\n",
        "    filepath = os.path.join(UPLOAD_FOLDER, filename)\n",
        "    file.save(filepath)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "    ])\n",
        "\n",
        "    img = Image.open(filepath).convert(\"RGB\")\n",
        "    x = transform(img).unsqueeze(0).to(next(model.parameters()).device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(x)\n",
        "        pred = torch.argmax(out, 1).item()   # 0=fire,1=non_fire\n",
        "\n",
        "    result_text = \"\ud83d\udd25 Fire Detected\" if pred == 0 else \"\u2714 No Fire\"\n",
        "\n",
        "    overlay, heatmap = grad_cam(model, filepath, target_class=pred)\n",
        "    att_name = f\"att_{filename}\"\n",
        "    cv2.imwrite(os.path.join(RESULT_FOLDER, att_name), overlay)\n",
        "\n",
        "    forecast_files = []\n",
        "    for i, hm in enumerate(dh3_forecast_from_heatmap(heatmap, steps=3)):\n",
        "        fname = f\"forecast_{i}_{filename}\"\n",
        "        cv2.imwrite(os.path.join(RESULT_FOLDER, fname), cv2.applyColorMap(hm, cv2.COLORMAP_JET))\n",
        "        forecast_files.append(fname)\n",
        "\n",
        "    return render_template(\n",
        "        \"index.html\",\n",
        "        uploaded_file=filename,\n",
        "        attention_image=att_name,\n",
        "        forecast_images=forecast_files,\n",
        "        result_text=result_text\n",
        "    )\n",
        "\n",
        "@app.route(\"/uploads/<fn>\")\n",
        "def upl(fn):\n",
        "    return send_from_directory(UPLOAD_FOLDER, fn)\n",
        "\n",
        "@app.route(\"/results/<fn>\")\n",
        "def res(fn):\n",
        "    return send_from_directory(RESULT_FOLDER, fn)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host=\"0.0.0.0\", port=8000, debug=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcd8 7 \u2014 Authenticate ngrok\n",
        "\n",
        "This step:\n",
        "\n",
        "* Authenticates ngrok with your account\n",
        "* Enables secure public HTTPS access\n",
        "* Prepares the system for live deployment\n",
        "\n",
        "# ===============================\n",
        "\n",
        "## \ud83c\udf10 Ngrok Setup (Public Deployment)\n",
        "\n",
        "Ngrok provides a **secure public HTTPS link** to your locally running Flask application.\n",
        "\n",
        "\ud83d\udd10 **For security reasons, your ngrok token should NOT be shared publicly.**\n",
        "\n",
        "### \u2705 To Use Ngrok, Follow These Steps:\n",
        "\n",
        "### \ud83d\udccc Step 1 \u2014 Get Your Auth Token\n",
        "\n",
        "Go to this link and copy your personal token:\n",
        "\ud83d\udc49 **[https://dashboard.ngrok.com/get-started/your-authtoken](https://dashboard.ngrok.com/get-started/your-authtoken)**\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udccc Step 2 \u2014 Add Token Inside Notebook\n",
        "\n",
        "Paste your token in the following line:\n",
        "\n",
        "```python\n",
        "#from pyngrok import ngrok, conf\n",
        "\n",
        "#conf.get_default().auth_token = \"YOUR_NGROK_TOKEN_HERE\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udccc Step 3 \u2014 Start Ngrok Tunnel\n",
        "\n",
        "```python\n",
        "#public_url = ngrok.connect(8000)\n",
        "#print(\"\ud83c\udf0d Public URL:\", public_url)\n",
        "```\n",
        "\n",
        "\u2705 After running this, a **shareable public link** will appear here.\n",
        "You can open it in your browser and access your Flask app from **anywhere in the world** \ud83c\udf0e\n",
        "\n",
        "---\n",
        "\n",
        "### \u2705 Summary\n",
        "\n",
        "\u2714 Secure HTTPS URL\n",
        "\n",
        "\u2714 No port forwarding required\n",
        "\n",
        "\u2714 Works on Google Colab\n",
        "\n",
        "\u2714 Perfect for project demos, reviews, and viva\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcd8 10 \u2014 Run Flask Server & ngrok Deployment\n",
        "\n",
        "This step:\n",
        "\n",
        "* Kills any previously running servers\n",
        "* Starts Flask Server\n",
        "* Creates a secure public URL using ngrok\n",
        "\n",
        "# \ud83d\udd10 ngrok Token Removed for Security\n",
        "\n",
        "User should insert their own token.\n",
        "\n",
        "# ===============================\n",
        "\n",
        "# \u2705 CELL 8: Run Server & ngrok\n",
        "\n",
        "# ===============================\n",
        "\n"
      ],
      "metadata": {
        "id": "rnnJBhuCNWNC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "151kv4Q48CUe"
      },
      "outputs": [],
      "source": [
        "# Kill anything old (safe)\n",
        "!pkill -f flask || true\n",
        "!pkill -f ngrok || true\n",
        "!fuser -k 8000/tcp || true\n",
        "\n",
        "# Start Flask in background\n",
        "import time\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "get_ipython().system_raw(\"python wildfire_project/app.py &> flask.log &\")\n",
        "\n",
        "# Ngrok auth token\n",
        "conf.get_default().auth_token = \"PASTE_YOUR_NGROK_TOKEN_HERE\"  # \u2b05\ufe0f put your token\n",
        "\n",
        "# Close old tunnels & open new\n",
        "ngrok.kill()\n",
        "time.sleep(2)\n",
        "public_url = ngrok.connect(8000, \"http\")\n",
        "print(\"\ud83c\udf0d Web App URL:\", public_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcd8 11 \u2014 View Logs & Evaluation Metrics\n",
        "\n",
        "This step displays:\n",
        "\n",
        "* Flask Logs\n",
        "* Classification Report\n",
        "* Confusion Matrix Visualization\n",
        "\n",
        "# ===============================\n",
        "\n",
        "# \u2705 CELL 9: Logs & Metrics\n",
        "\n",
        "# ===============================\n",
        "\n"
      ],
      "metadata": {
        "id": "8GykzIyUNoKF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H4491Gd8CSd"
      },
      "outputs": [],
      "source": [
        "print(\"---- Flask Log ----\")\n",
        "!tail -n 40 flask.log || echo \"No log yet.\"\n",
        "\n",
        "print(\"\\n---- Classification Report ----\")\n",
        "report_path = \"wildfire_project/results/classification_report.txt\"\n",
        "if os.path.exists(report_path):\n",
        "    print(open(report_path).read())\n",
        "else:\n",
        "    print(\"No classification report found.\")\n",
        "\n",
        "from IPython.display import Image as IPyImage, display\n",
        "cm_path = \"wildfire_project/results/confusion_matrix.png\"\n",
        "if os.path.exists(cm_path):\n",
        "    display(IPyImage(cm_path))\n",
        "else:\n",
        "    print(\"No confusion matrix image found.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}