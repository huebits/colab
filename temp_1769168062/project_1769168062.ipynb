{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMoWf38OFqXa"
      },
      "source": [
        "## \ud83d\udcd8 How to Use Kaggle (Upload Dataset & Notebook)\n",
        "\n",
        "### \u2705 Step 1: Create Kaggle Account\n",
        "- Go to \ud83d\udc49 https://www.kaggle.com  \n",
        "- Sign in using Google / Email\n",
        "\n",
        "---\n",
        "\n",
        "### \u2705 Step 2: Upload Your Dataset\n",
        "1. Click **Datasets** \u2192 **Create New Dataset**\n",
        "2. Upload your **dataset folder or ZIP file**\n",
        "3. Add:\n",
        "   - Dataset name\n",
        "   - Short description\n",
        "4. Set visibility \u2192 **Public / Private**\n",
        "5. Click **Create**\n",
        "\n",
        "\u2705 After upload, Kaggle gives a dataset path like:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QWIHwsmFqXi"
      },
      "source": [
        "### \u2705 MULTIMODAL PARKINSON\u2019S DISEASE DETECTION (SIGNAL + IMAGE + FUSION)\n",
        "\n",
        "This project builds a **dual-modality Parkinson\u2019s detection system** using:\n",
        "\n",
        "\u2022 \ud83d\udcdd **Tablet handwriting signals (TXT) \u2192 1D CNN + BiLSTM**  \n",
        "\u2022 \ud83d\uddbc\ufe0f **Spiral/Circle images \u2192 ConvNeXt-Tiny (Transfer Learning)**  \n",
        "\u2022 \ud83d\udd17 **Late Fusion \u2192 Weighted probability averaging**\n",
        "\n",
        "-----------------------------------\n",
        "### \ud83d\udd39 DATASET-1 (Tablet Signal Based)\n",
        "\u2022 Loads spiral movement signals from TXT  \n",
        "\u2022 Features: x, y, pressure, azimuth, altitude  \n",
        "\u2022 Min-max normalization  \n",
        "\u2022 Padding to **2000 timesteps**  \n",
        "\u2022 Model: **1D CNN + BiLSTM**  \n",
        "\u2022 Training: **5-Fold Stratified Cross Validation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdhBIeXeFqXk"
      },
      "source": [
        "**Dataset paths:**\n",
        "https://www.kaggle.com/datasets/huebitsvizg/parkinsons-handwritten-dataset\n",
        "\n",
        "https://www.kaggle.com/datasets/huebitsvizg/parkinsons-dataset-handwritten2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ACteVdwFqXn"
      },
      "source": [
        "### \u2705 CELL 1 \u2014 Imports & Reproducibility Setup\n",
        "Initializes Python, NumPy, Pandas, TensorFlow, and Sklearn.  \n",
        "Sets a fixed random seed for **reproducible training results**.\n",
        "\n",
        "\ud83d\udd17 TensorFlow: https://www.tensorflow.org  \n",
        "\ud83d\udd17 NumPy: https://numpy.org"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-12-04T04:34:10.651122Z",
          "iopub.status.busy": "2025-12-04T04:34:10.650898Z",
          "iopub.status.idle": "2025-12-04T04:34:28.622998Z",
          "shell.execute_reply": "2025-12-04T04:34:28.622229Z",
          "shell.execute_reply.started": "2025-12-04T04:34:10.651094Z"
        },
        "trusted": true,
        "id": "gL84MCLnFqXo"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# Cell 1: Imports & seed setup\n",
        "# ============================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# For reproducibility (as much as possible)\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqfswo88FqXp"
      },
      "source": [
        "### \u2705 CELL 2 \u2014 Dataset-1 TXT Path Configuration\n",
        "Defines paths for:\n",
        "\u2022 Healthy signals  \n",
        "\u2022 Parkinson (PWP) signals  \n",
        "Counts total TXT handwriting motion files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T04:34:34.606706Z",
          "iopub.status.busy": "2025-12-04T04:34:34.606108Z",
          "iopub.status.idle": "2025-12-04T04:34:34.627218Z",
          "shell.execute_reply": "2025-12-04T04:34:34.626424Z",
          "shell.execute_reply.started": "2025-12-04T04:34:34.606682Z"
        },
        "trusted": true,
        "id": "Zqo0pI07FqXq"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# Cell 2: Dataset 1 TXT path configuration\n",
        "# =============================================\n",
        "\n",
        "BASE = \"/kaggle/input/parkinsons-handwritten/improved+spiral+test+using+digitized+graphics+tablet+for+monitoring+parkinson+s+disease/Improved Spiral Test Using Digitized Graphics Tablet for Monitoring Parkinsons Disease\"\n",
        "\n",
        "healthy_dir = os.path.join(BASE, \"data\", \"Healthy\")\n",
        "pwp_dir     = os.path.join(BASE, \"data\", \"PWP\")\n",
        "\n",
        "healthy_files = sorted(glob.glob(os.path.join(healthy_dir, \"*.txt\")))\n",
        "pwp_files     = sorted(glob.glob(os.path.join(pwp_dir, \"*.txt\")))\n",
        "\n",
        "print(\"Healthy TXT files:\", len(healthy_files))\n",
        "print(\"PWP TXT files    :\", len(pwp_files))\n",
        "print(\"Total TXT files  :\", len(healthy_files) + len(pwp_files))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uUf_CtQFqXr"
      },
      "source": [
        "### \u2705 CELL 3 \u2014 TXT Signal Preprocessing Function\n",
        "Loads a single tablet signal file and:\n",
        "\u2022 Selects x, y, pressure, azimuth, altitude  \n",
        "\u2022 Applies **per-feature Min-Max normalization**  \n",
        "\u2022 Returns time-series array `(T, 5)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T04:34:39.974335Z",
          "iopub.status.busy": "2025-12-04T04:34:39.973670Z",
          "iopub.status.idle": "2025-12-04T04:34:39.979391Z",
          "shell.execute_reply": "2025-12-04T04:34:39.978630Z",
          "shell.execute_reply.started": "2025-12-04T04:34:39.974310Z"
        },
        "trusted": true,
        "id": "mW1TcOHEFqXr"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# Cell 3: Function to load and preprocess a TXT file\n",
        "# ===================================================\n",
        "\n",
        "def load_signal_txt(path):\n",
        "    \"\"\"\n",
        "    Load one TXT file from Dataset 1 and return a\n",
        "    normalized time series of shape (T, 5).\n",
        "\n",
        "    Columns used:\n",
        "      x, y, pressure, azimuth, altitude\n",
        "    Each column is min-max normalized independently.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path, sep=';', header=None)\n",
        "    df.columns = [\"x\", \"y\", \"pressure\", \"azimuth\", \"altitude\", \"timestamp\", \"end\"]\n",
        "\n",
        "    # Select relevant features\n",
        "    data = df[[\"x\", \"y\", \"pressure\", \"azimuth\", \"altitude\"]].astype(np.float32)\n",
        "\n",
        "    # Min-max normalization per feature (column-wise)\n",
        "    data = (data - data.min()) / (data.max() - data.min() + 1e-8)\n",
        "\n",
        "    return data.values  # shape: (T, 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4K2WnvHFqXs"
      },
      "source": [
        "### \u2705 CELL 4 \u2014 Load All Signals into Memory\n",
        "Loads **all Healthy + Parkinson signals** into:\n",
        "\u2022 `signals[]`\n",
        "\u2022 `labels[]` (0 = Healthy, 1 = Parkinson)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T04:34:50.808069Z",
          "iopub.status.busy": "2025-12-04T04:34:50.807779Z",
          "iopub.status.idle": "2025-12-04T04:34:51.329479Z",
          "shell.execute_reply": "2025-12-04T04:34:51.328641Z",
          "shell.execute_reply.started": "2025-12-04T04:34:50.808048Z"
        },
        "trusted": true,
        "id": "P-V3N4u3FqXv"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Cell 4: Load all signals into memory\n",
        "# ==========================================\n",
        "\n",
        "signals = []\n",
        "labels = []\n",
        "\n",
        "# Healthy = 0\n",
        "for path in healthy_files:\n",
        "    sig = load_signal_txt(path)\n",
        "    signals.append(sig)\n",
        "    labels.append(0)\n",
        "\n",
        "# PWP (Parkinson) = 1\n",
        "for path in pwp_files:\n",
        "    sig = load_signal_txt(path)\n",
        "    signals.append(sig)\n",
        "    labels.append(1)\n",
        "\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(\"Total signals loaded:\", len(signals))\n",
        "print(\"Label distribution   :\", np.bincount(labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44uwfqQPFqXw"
      },
      "source": [
        "### \u2705 CELL 5 \u2014 Sequence Padding\n",
        "Analyzes signal length distribution.  \n",
        "Pads all sequences to **2000 time-steps** using:\n",
        "`pad_sequences()` for uniform model input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T04:35:06.303315Z",
          "iopub.status.busy": "2025-12-04T04:35:06.302993Z",
          "iopub.status.idle": "2025-12-04T04:35:06.310948Z",
          "shell.execute_reply": "2025-12-04T04:35:06.310313Z",
          "shell.execute_reply.started": "2025-12-04T04:35:06.303293Z"
        },
        "trusted": true,
        "id": "RTIgU-rOFqXw"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# Cell 5: Pad signals to same time length\n",
        "# =========================================\n",
        "\n",
        "# Inspect sequence lengths\n",
        "lengths = [len(s) for s in signals]\n",
        "print(\"Min length:\", min(lengths))\n",
        "print(\"Max length:\", max(lengths))\n",
        "print(\"Mean length:\", np.mean(lengths))\n",
        "\n",
        "# Choose a max length (larger than most sequences, but not huge)\n",
        "MAX_LEN = 2000\n",
        "\n",
        "# Pad with zeros at the end (\"post\")\n",
        "X = pad_sequences(\n",
        "    signals,\n",
        "    maxlen=MAX_LEN,\n",
        "    dtype='float32',\n",
        "    padding='post',\n",
        "    truncating='post'\n",
        ")\n",
        "\n",
        "print(\"X shape (samples, timesteps, features):\", X.shape)\n",
        "print(\"y shape:\", labels.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHbZedQkFqXx"
      },
      "source": [
        "### \u2705 CELL 6 \u2014 1D CNN + BiLSTM Model Builder\n",
        "Creates hybrid deep learning model:\n",
        "\u2022 Conv1D \u2192 local motion patterns  \n",
        "\u2022 BiLSTM \u2192 long-term tremor behavior  \n",
        "\u2022 Dense \u2192 final Parkinson classification  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T04:35:20.263809Z",
          "iopub.status.busy": "2025-12-04T04:35:20.263044Z",
          "iopub.status.idle": "2025-12-04T04:35:22.356181Z",
          "shell.execute_reply": "2025-12-04T04:35:22.355589Z",
          "shell.execute_reply.started": "2025-12-04T04:35:20.263765Z"
        },
        "trusted": true,
        "id": "m09t1FalFqXx"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 6: Build 1D CNN + BiLSTM model (for each fold)\n",
        "# ============================================================\n",
        "\n",
        "def build_signal_model(input_length=2000, n_features=5):\n",
        "    \"\"\"\n",
        "    Build a 1D CNN + Bidirectional LSTM model.\n",
        "    This will be re-created fresh for each fold.\n",
        "    \"\"\"\n",
        "    inp = layers.Input(shape=(input_length, n_features))\n",
        "\n",
        "    # Local pattern extraction (tremor, speed changes, micro-jerks)\n",
        "    x = layers.Conv1D(64, 7, activation='relu', padding='same')(inp)\n",
        "    x = layers.MaxPooling1D(2)(x)\n",
        "\n",
        "    x = layers.Conv1D(128, 5, activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling1D(2)(x)\n",
        "\n",
        "    # Long-range temporal dependencies\n",
        "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x)\n",
        "\n",
        "    # Dense classifier\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    out = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Quick check\n",
        "tmp_model = build_signal_model(MAX_LEN, 5)\n",
        "tmp_model.summary()\n",
        "del tmp_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeFny-7pFqXx"
      },
      "source": [
        "### \u2705 CELL 7 \u2014 Training Curve Plot Function\n",
        "Plots:\n",
        "\u2022 Training vs Validation Accuracy  \n",
        "\u2022 Training vs Validation Loss  \n",
        "Per fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T04:35:37.819381Z",
          "iopub.status.busy": "2025-12-04T04:35:37.818699Z",
          "iopub.status.idle": "2025-12-04T04:35:37.825346Z",
          "shell.execute_reply": "2025-12-04T04:35:37.824616Z",
          "shell.execute_reply.started": "2025-12-04T04:35:37.819346Z"
        },
        "trusted": true,
        "id": "8IwZSOB6FqXy"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# Cell 7: Helper to plot training curves per fold\n",
        "# ==================================================\n",
        "\n",
        "def plot_history(history, fold_idx):\n",
        "    \"\"\"\n",
        "    Plot training/validation accuracy & loss for one fold.\n",
        "    \"\"\"\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochs, acc, 'b-', label='train_acc')\n",
        "    plt.plot(epochs, val_acc, 'r-', label='val_acc')\n",
        "    plt.title(f'Fold {fold_idx+1} - Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochs, loss, 'b-', label='train_loss')\n",
        "    plt.plot(epochs, val_loss, 'r-', label='val_loss')\n",
        "    plt.title(f'Fold {fold_idx+1} - Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXSi42pZFqXy"
      },
      "source": [
        "### \u2705 CELL 8 \u2014 5-Fold Stratified Cross-Validation\n",
        "Performs:\n",
        "\u2022 Stratified K-Fold splitting  \n",
        "\u2022 Train-Validation-Test per fold  \n",
        "\u2022 EarlyStopping + ReduceLR  \n",
        "\u2022 Tracks **best fold model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T04:35:53.575659Z",
          "iopub.status.busy": "2025-12-04T04:35:53.575106Z",
          "iopub.status.idle": "2025-12-04T04:36:36.073681Z",
          "shell.execute_reply": "2025-12-04T04:36:36.073082Z",
          "shell.execute_reply.started": "2025-12-04T04:35:53.575631Z"
        },
        "trusted": true,
        "id": "ejgZwc26FqXy"
      },
      "outputs": [],
      "source": [
        "# =======================================================\n",
        "# Cell 8: Stratified 5-Fold Cross-Validation Training\n",
        "# =======================================================\n",
        "\n",
        "N_SPLITS = 5\n",
        "\n",
        "skf = StratifiedKFold(\n",
        "    n_splits=N_SPLITS,\n",
        "    shuffle=True,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "fold_accuracies = []\n",
        "fold_histories = []\n",
        "best_fold_model = None\n",
        "best_fold_acc = -1\n",
        "best_fold_idx = -1\n",
        "\n",
        "fold_idx = 0\n",
        "\n",
        "for train_index, test_index in skf.split(X, labels):\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\ud83d\udd01 Fold {fold_idx+1}/{N_SPLITS}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Split into train+test for this fold\n",
        "    X_train_full, X_test = X[train_index], X[test_index]\n",
        "    y_train_full, y_test = labels[train_index], labels[test_index]\n",
        "\n",
        "    # Further split train_full into train/val for early stopping\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_full,\n",
        "        y_train_full,\n",
        "        test_size=0.2,\n",
        "        stratify=y_train_full,\n",
        "        random_state=SEED\n",
        "    )\n",
        "\n",
        "    print(\"  Train size:\", X_train.shape[0])\n",
        "    print(\"  Val size  :\", X_val.shape[0])\n",
        "    print(\"  Test size :\", X_test.shape[0])\n",
        "\n",
        "    # Build a fresh model for this fold\n",
        "    model = build_signal_model(MAX_LEN, 5)\n",
        "\n",
        "    # Callbacks (use fold index in verbose print only)\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=6,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            min_lr=1e-6,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=40,\n",
        "        batch_size=4,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Store history\n",
        "    fold_histories.append(history)\n",
        "\n",
        "    # Plot curves for this fold\n",
        "    plot_history(history, fold_idx)\n",
        "\n",
        "    # Evaluate on the held-out test fold\n",
        "    y_pred_prob = model.predict(X_test)\n",
        "    y_pred = (y_pred_prob.ravel() >= 0.5).astype(int)\n",
        "\n",
        "    fold_acc = accuracy_score(y_test, y_pred)\n",
        "    fold_accuracies.append(fold_acc)\n",
        "\n",
        "    print(f\"Fold {fold_idx+1} TEST accuracy: {fold_acc:.4f}\")\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"Classification report:\")\n",
        "    print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "    # Keep track of the best fold model (highest test accuracy)\n",
        "    if fold_acc > best_fold_acc:\n",
        "        best_fold_acc = fold_acc\n",
        "        best_fold_idx = fold_idx\n",
        "        best_fold_model = model  # keep this model in memory\n",
        "\n",
        "    fold_idx += 1\n",
        "\n",
        "print(\"All folds done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrktCo2FFqXz"
      },
      "source": [
        "### \u2705 CELL 9 \u2014 K-Fold Performance Summary\n",
        "Prints:\n",
        "\u2022 Accuracy of each fold  \n",
        "\u2022 Mean accuracy  \n",
        "\u2022 Standard deviation  \n",
        "\u2022 Best performing fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T04:36:54.455470Z",
          "iopub.status.busy": "2025-12-04T04:36:54.454839Z",
          "iopub.status.idle": "2025-12-04T04:36:54.460464Z",
          "shell.execute_reply": "2025-12-04T04:36:54.459789Z",
          "shell.execute_reply.started": "2025-12-04T04:36:54.455444Z"
        },
        "trusted": true,
        "id": "REJqEkVnFqXz"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# Cell 9: Summary of K-Fold results\n",
        "# =========================================\n",
        "\n",
        "fold_accuracies = np.array(fold_accuracies)\n",
        "print(\"Fold accuracies:\", fold_accuracies)\n",
        "print(f\"Mean accuracy: {fold_accuracies.mean():.4f}\")\n",
        "print(f\"Std  accuracy: {fold_accuracies.std():.4f}\")\n",
        "print(f\"Best fold idx: {best_fold_idx+1} with acc={best_fold_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNuy29TSFqX0"
      },
      "source": [
        "### \u2705 CELL 10 \u2014 Save Best Signal Model\n",
        "Stores best CNN-BiLSTM model:\n",
        "`best_signal_kfold_model.keras`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T04:38:17.123475Z",
          "iopub.status.busy": "2025-12-04T04:38:17.122599Z",
          "iopub.status.idle": "2025-12-04T04:38:17.200450Z",
          "shell.execute_reply": "2025-12-04T04:38:17.199784Z",
          "shell.execute_reply.started": "2025-12-04T04:38:17.123451Z"
        },
        "trusted": true,
        "id": "Lv5X6LEaFqX0"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# Cell 10: Save best fold model\n",
        "# =========================================\n",
        "\n",
        "save_path = \"/kaggle/working/best_signal_kfold_model.keras\"\n",
        "best_fold_model.save(save_path)\n",
        "print(f\"Best fold model saved to: {save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy3EpXC4FqX1"
      },
      "source": [
        "\n",
        "## \u2705 DATASET-2 | CELL 1 \u2014 Image Imports & Seed Setup\n",
        "Initializes TensorFlow pipeline for image training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:33:47.956218Z",
          "iopub.status.busy": "2025-12-04T05:33:47.955480Z",
          "iopub.status.idle": "2025-12-04T05:33:47.979879Z",
          "shell.execute_reply": "2025-12-04T05:33:47.979214Z",
          "shell.execute_reply.started": "2025-12-04T05:33:47.956186Z"
        },
        "trusted": true,
        "id": "KqbL7XBXFqX1"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# Cell 1 \u2013 Imports & seeds\n",
        "# ============================\n",
        "import os, glob, shutil, random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC63ubjeFqX2"
      },
      "source": [
        "### \u2705 CELL 2 \u2014 Image Dataset Flattening\n",
        "Copies scattered images into:\n",
        "\u2022 `/Healthy`\n",
        "\u2022 `/Parkinsons`\n",
        "Clean flat directory structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:33:55.812669Z",
          "iopub.status.busy": "2025-12-04T05:33:55.812358Z",
          "iopub.status.idle": "2025-12-04T05:33:57.811182Z",
          "shell.execute_reply": "2025-12-04T05:33:57.810420Z",
          "shell.execute_reply.started": "2025-12-04T05:33:55.812648Z"
        },
        "trusted": true,
        "id": "wH5LhHs1FqX2"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 2 \u2013 Rebuild Dataset-2 into flat folders\n",
        "# ============================================\n",
        "\n",
        "BASE = \"/kaggle/input/parkinsons-handwritten-2/Parkinsons dataset\"\n",
        "HEALTHY_SRC = os.path.join(BASE, \"Healthy_parkinsons\")\n",
        "PWP_SRC     = os.path.join(BASE, \"Parkinsons_patient\")\n",
        "\n",
        "OUT_ROOT = \"/kaggle/working/dataset2_clean_advanced\"\n",
        "H_OUT = os.path.join(OUT_ROOT, \"Healthy\")\n",
        "P_OUT = os.path.join(OUT_ROOT, \"Parkinsons\")\n",
        "\n",
        "os.makedirs(H_OUT, exist_ok=True)\n",
        "os.makedirs(P_OUT, exist_ok=True)\n",
        "\n",
        "def copy_images(src_root, dst_root):\n",
        "    for root, dirs, files in os.walk(src_root):\n",
        "        for f in files:\n",
        "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\")):\n",
        "                src = os.path.join(root, f)\n",
        "                dst = os.path.join(dst_root, f)\n",
        "                if not os.path.exists(dst):  # avoid duplicate copies\n",
        "                    shutil.copy2(src, dst)\n",
        "\n",
        "copy_images(HEALTHY_SRC, H_OUT)\n",
        "copy_images(PWP_SRC, P_OUT)\n",
        "\n",
        "print(\"Healthy images:\", len(os.listdir(H_OUT)))\n",
        "print(\"Parkinsons images:\", len(os.listdir(P_OUT)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo-AFumbFqX2"
      },
      "source": [
        "### \u2705 CELL 3 \u2014 tf.data Image Pipeline\n",
        "Creates:\n",
        "\u2022 Training dataset\n",
        "\u2022 Validation dataset  \n",
        "Automatic splitting + resizing + batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:34:07.768792Z",
          "iopub.status.busy": "2025-12-04T05:34:07.768475Z",
          "iopub.status.idle": "2025-12-04T05:34:09.121226Z",
          "shell.execute_reply": "2025-12-04T05:34:09.120526Z",
          "shell.execute_reply.started": "2025-12-04T05:34:07.768771Z"
        },
        "trusted": true,
        "id": "dm9SiojbFqX3"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 3 \u2013 tf.data datasets with splitting\n",
        "# ============================================\n",
        "\n",
        "IMG_SIZE = 300       # larger resolution helps spirals\n",
        "BATCH_SIZE = 16\n",
        "VAL_SPLIT = 0.2\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    OUT_ROOT,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"binary\",\n",
        "    validation_split=VAL_SPLIT,\n",
        "    subset=\"training\",\n",
        "    seed=SEED,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    OUT_ROOT,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"binary\",\n",
        "    validation_split=VAL_SPLIT,\n",
        "    subset=\"validation\",\n",
        "    seed=SEED,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Normalize [0,255] -> [0,1]\n",
        "def scale(images, labels):\n",
        "    return tf.cast(images, tf.float32) / 255.0, tf.cast(labels, tf.float32)\n",
        "\n",
        "train_ds = train_ds.map(scale, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds   = val_ds.map(scale,   num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Cache & prefetch for speed\n",
        "train_ds = train_ds.shuffle(1024).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds   = val_ds.prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6bgtyB9FqX4"
      },
      "source": [
        "### \u2705 CELL 4 \u2014 Class Weight Computation\n",
        "Balances training for imbalanced classes using:\n",
        "`compute_class_weight()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:34:17.209650Z",
          "iopub.status.busy": "2025-12-04T05:34:17.209319Z",
          "iopub.status.idle": "2025-12-04T05:34:18.603052Z",
          "shell.execute_reply": "2025-12-04T05:34:18.602300Z",
          "shell.execute_reply.started": "2025-12-04T05:34:17.209630Z"
        },
        "trusted": true,
        "id": "irNE6calFqX5"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 4 \u2013 Compute class weights\n",
        "# ============================================\n",
        "\n",
        "# Collect all labels from train_ds\n",
        "all_y = []\n",
        "for _, y in train_ds:\n",
        "    all_y.append(y.numpy().ravel())\n",
        "all_y = np.concatenate(all_y)\n",
        "\n",
        "classes = np.array([0., 1.])\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\", classes=classes, y=all_y\n",
        ")\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "print(\"Class weights:\", class_weight_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkvaTSc7FqX7"
      },
      "source": [
        "### \u2705 CELL 5 \u2014 MixUp & CutMix Augmentation\n",
        "Implements:\n",
        "\u2022 MixUp \u2192 Image blending  \n",
        "\u2022 CutMix \u2192 Patch replacement  \n",
        "Used to **improve generalization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:36:04.044278Z",
          "iopub.status.busy": "2025-12-04T05:36:04.043987Z",
          "iopub.status.idle": "2025-12-04T05:36:04.058168Z",
          "shell.execute_reply": "2025-12-04T05:36:04.057495Z",
          "shell.execute_reply.started": "2025-12-04T05:36:04.044257Z"
        },
        "trusted": true,
        "id": "EscbYvVsFqX9"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 5 \u2013 MixUp & CutMix functions\n",
        "# ============================================\n",
        "\n",
        "def _sample_lambda(alpha=0.2):\n",
        "    # Beta(alpha, alpha) via two gammas\n",
        "    if alpha <= 0:\n",
        "        return 1.0\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    return float(lam)\n",
        "\n",
        "def mixup(images, labels, alpha=0.2):\n",
        "    bs = tf.shape(images)[0]\n",
        "    # shuffle indices\n",
        "    indices = tf.random.shuffle(tf.range(bs))\n",
        "    shuffled_images = tf.gather(images, indices)\n",
        "    shuffled_labels = tf.gather(labels, indices)\n",
        "\n",
        "    lam = _sample_lambda(alpha)\n",
        "    lam = tf.cast(lam, tf.float32)\n",
        "\n",
        "    images = lam * images + (1.0 - lam) * shuffled_images\n",
        "    labels = lam * labels + (1.0 - lam) * shuffled_labels\n",
        "    return images, labels\n",
        "\n",
        "def cutmix(images, labels, alpha=0.2):\n",
        "    bs = tf.shape(images)[0]\n",
        "    h = tf.shape(images)[1]\n",
        "    w = tf.shape(images)[2]\n",
        "\n",
        "    # Cast to float for arithmetic\n",
        "    h_float = tf.cast(h, tf.float32)\n",
        "    w_float = tf.cast(w, tf.float32)\n",
        "\n",
        "    indices = tf.random.shuffle(tf.range(bs))\n",
        "    shuffled_images = tf.gather(images, indices)\n",
        "    shuffled_labels = tf.gather(labels, indices)\n",
        "\n",
        "    lam = _sample_lambda(alpha)\n",
        "    lam = tf.cast(lam, tf.float32)\n",
        "\n",
        "    # Cutout box size computed in float, then cast\n",
        "    cut_ratio = tf.sqrt(1.0 - lam)\n",
        "    rw = tf.cast(w_float * cut_ratio, tf.int32)\n",
        "    rh = tf.cast(h_float * cut_ratio, tf.int32)\n",
        "\n",
        "    # Random center position\n",
        "    rx = tf.random.uniform([], 0, w_float)\n",
        "    ry = tf.random.uniform([], 0, h_float)\n",
        "\n",
        "    rx = tf.cast(rx, tf.int32)\n",
        "    ry = tf.cast(ry, tf.int32)\n",
        "\n",
        "    x1 = tf.clip_by_value(rx - rw // 2, 0, w)\n",
        "    y1 = tf.clip_by_value(ry - rh // 2, 0, h)\n",
        "    x2 = tf.clip_by_value(rx + rw // 2, 0, w)\n",
        "    y2 = tf.clip_by_value(ry + rh // 2, 0, h)\n",
        "\n",
        "    # Build mask\n",
        "    y_grid = tf.reshape(tf.range(h), (1, h, 1, 1))\n",
        "    x_grid = tf.reshape(tf.range(w), (1, 1, w, 1))\n",
        "\n",
        "    y_in_box = tf.logical_and(y_grid >= y1, y_grid < y2)\n",
        "    x_in_box = tf.logical_and(x_grid >= x1, x_grid < x2)\n",
        "    box = tf.cast(tf.logical_and(y_in_box, x_in_box), tf.float32)\n",
        "\n",
        "    mask = 1.0 - box\n",
        "    images = images * mask + shuffled_images * (1.0 - mask)\n",
        "\n",
        "    box_area = tf.cast((x2 - x1) * (y2 - y1), tf.float32)\n",
        "    lam_adjusted = 1.0 - (box_area / (h_float * w_float))\n",
        "\n",
        "    labels = lam_adjusted * labels + (1.0 - lam_adjusted) * shuffled_labels\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "def mixup_cutmix_pipeline(images, labels, mixup_prob=0.5, cutmix_prob=0.5, alpha=0.2):\n",
        "    \"\"\"Randomly apply MixUp or CutMix to a batch.\"\"\"\n",
        "    rnd = tf.random.uniform([], 0, 1)\n",
        "    def apply_mixup():\n",
        "        return mixup(images, labels, alpha)\n",
        "    def apply_cutmix():\n",
        "        return cutmix(images, labels, alpha)\n",
        "    def no_aug():\n",
        "        return images, labels\n",
        "\n",
        "    images, labels = tf.cond(rnd < mixup_prob,\n",
        "                             apply_mixup,\n",
        "                             lambda: tf.cond(rnd < mixup_prob + cutmix_prob,\n",
        "                                             apply_cutmix,\n",
        "                                             no_aug))\n",
        "    return images, labels\n",
        "\n",
        "# Wrap to use in dataset.map\n",
        "def augment_batch(images, labels):\n",
        "    return mixup_cutmix_pipeline(images, labels, mixup_prob=0.5, cutmix_prob=0.5, alpha=0.4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwwsXmyLFqX-"
      },
      "source": [
        "### \u2705 CELL 6 \u2014 Advanced Image Augmentation\n",
        "Applies:\n",
        "\u2022 Rotation  \n",
        "\u2022 Zoom  \n",
        "\u2022 Translation  \n",
        "\u2022 Contrast  \n",
        "Combined with MixUp + CutMix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:36:05.251798Z",
          "iopub.status.busy": "2025-12-04T05:36:05.250976Z",
          "iopub.status.idle": "2025-12-04T05:36:06.179532Z",
          "shell.execute_reply": "2025-12-04T05:36:06.178885Z",
          "shell.execute_reply.started": "2025-12-04T05:36:05.251771Z"
        },
        "trusted": true,
        "id": "1qD8g0o1FqX-"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 6 \u2013 Create augmented training dataset\n",
        "# ============================================\n",
        "\n",
        "# Strong geometric + photometric augmentation\n",
        "data_augment = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.15),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "    tf.keras.layers.RandomTranslation(0.1, 0.1),\n",
        "    tf.keras.layers.RandomContrast(0.3),\n",
        "])\n",
        "\n",
        "def apply_preprocessing(images, labels):\n",
        "    images = data_augment(images, training=True)\n",
        "    return images, labels\n",
        "\n",
        "# Order: basic aug -> mixup/cutmix\n",
        "aug_train_ds = train_ds.map(apply_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "aug_train_ds = aug_train_ds.map(augment_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "aug_train_ds = aug_train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Augmented train dataset ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUQbZpU-FqX_"
      },
      "source": [
        "### \u2705 CELL 7 \u2014 ConvNeXt-Tiny Model\n",
        "Uses **pretrained ImageNet ConvNeXt-Tiny**:\n",
        "\u2022 Feature extractor  \n",
        "\u2022 Dense + Dropout head  \n",
        "\u2022 Binary classification\n",
        "\n",
        "\ud83d\udd17 ConvNeXt: https://arxiv.org/abs/2201.03545"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:36:35.369417Z",
          "iopub.status.busy": "2025-12-04T05:36:35.369088Z",
          "iopub.status.idle": "2025-12-04T05:36:36.656355Z",
          "shell.execute_reply": "2025-12-04T05:36:36.655547Z",
          "shell.execute_reply.started": "2025-12-04T05:36:35.369393Z"
        },
        "trusted": true,
        "id": "8P_6zpLlFqX_"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 7 \u2013 Build ConvNeXt-Tiny model\n",
        "# ============================================\n",
        "\n",
        "from tensorflow.keras.applications import ConvNeXtTiny\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "base_model = ConvNeXtTiny(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    pooling=\"avg\"\n",
        ")\n",
        "\n",
        "# Advanced plan: start with partially frozen, then fully fine-tune\n",
        "base_model.trainable = False  # Stage 1\n",
        "\n",
        "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = layers.Dense(512, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=loss_fn,\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn2HxSA-FqYA"
      },
      "source": [
        "### \u2705 CELL 8 \u2014 Stage-1 Training (Frozen Backbone)\n",
        "Trains only classifier head  \n",
        "Uses:\n",
        "\u2022 EarlyStopping  \n",
        "\u2022 ReduceLROnPlateau\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:36:44.957343Z",
          "iopub.status.busy": "2025-12-04T05:36:44.957067Z",
          "iopub.status.idle": "2025-12-04T05:38:35.387710Z",
          "shell.execute_reply": "2025-12-04T05:38:35.386870Z",
          "shell.execute_reply.started": "2025-12-04T05:36:44.957322Z"
        },
        "trusted": true,
        "id": "H52IV6mUFqYA"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 8 \u2013 Stage 1: frozen backbone training\n",
        "# ============================================\n",
        "\n",
        "callbacks_stage1 = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "history1 = model.fit(\n",
        "    aug_train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=12,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=callbacks_stage1,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuTcs9wZFqZ2"
      },
      "source": [
        "### \u2705 CELL 9 \u2014 Stage-2 Fine-Tuning\n",
        "Unfreezes full ConvNeXt backbone  \n",
        "Trains with very small learning rate  \n",
        "Improves fine-grained Parkinson features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:38:38.960985Z",
          "iopub.status.busy": "2025-12-04T05:38:38.960660Z",
          "iopub.status.idle": "2025-12-04T05:43:36.140672Z",
          "shell.execute_reply": "2025-12-04T05:43:36.139986Z",
          "shell.execute_reply.started": "2025-12-04T05:38:38.960963Z"
        },
        "trusted": true,
        "id": "OTPMqXfhFqZ3"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 9 \u2013 Stage 2: full fine-tuning\n",
        "# ============================================\n",
        "\n",
        "base_model.trainable = True  # unfreeze all layers\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),  # tiny LR for fine-tuning\n",
        "    loss=loss_fn,\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "callbacks_stage2 = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=6, restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-7, verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "history2 = model.fit(\n",
        "    aug_train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=20,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=callbacks_stage2,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvM8TFuuFqZ5"
      },
      "source": [
        "### \u2705 CELL 10 \u2014 Training Curve Visualization\n",
        "Plots:\n",
        "\u2022 Accuracy progression  \n",
        "\u2022 Loss progression  \n",
        "For full two-stage training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:45:04.907992Z",
          "iopub.status.busy": "2025-12-04T05:45:04.907686Z",
          "iopub.status.idle": "2025-12-04T05:45:05.985209Z",
          "shell.execute_reply": "2025-12-04T05:45:05.984423Z",
          "shell.execute_reply.started": "2025-12-04T05:45:04.907968Z"
        },
        "trusted": true,
        "id": "gUi7yNC7FqZ6"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 10 \u2013 Plot accuracy & loss\n",
        "# ============================================\n",
        "\n",
        "def merge_histories(h1, h2):\n",
        "    hist = {}\n",
        "    for k in h1.history.keys():\n",
        "        hist[k] = h1.history[k] + h2.history[k]\n",
        "    return hist\n",
        "\n",
        "merged = merge_histories(history1, history2)\n",
        "\n",
        "epochs = range(1, len(merged[\"accuracy\"]) + 1)\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs, merged[\"accuracy\"], label=\"train\")\n",
        "plt.plot(epochs, merged[\"val_accuracy\"], label=\"val\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs, merged[\"loss\"], label=\"train\")\n",
        "plt.plot(epochs, merged[\"val_loss\"], label=\"val\")\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNkmepOuFqZ6"
      },
      "source": [
        "### \u2705 CELL 11 \u2014 Save Final Image Model\n",
        "Stores final ConvNeXt model:\n",
        "`dataset2_convnext_advanced.keras`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:54:19.147749Z",
          "iopub.status.busy": "2025-12-04T05:54:19.147453Z",
          "iopub.status.idle": "2025-12-04T05:54:20.666005Z",
          "shell.execute_reply": "2025-12-04T05:54:20.665071Z",
          "shell.execute_reply.started": "2025-12-04T05:54:19.147727Z"
        },
        "trusted": true,
        "id": "PBJty6I8FqZ7"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 11 \u2013 Save final ConvNeXt model\n",
        "# ============================================\n",
        "\n",
        "model.save(\"/kaggle/working/dataset2_convnext_advanced.keras\")\n",
        "print(\"Saved advanced image model.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aAGIXQQFqZ7"
      },
      "source": [
        "### \u2705 FUSION | CELL 1 \u2014 Load Trained Models\n",
        "Loads:\n",
        "\u2022 Best signal model  \n",
        "\u2022 Best image model  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:56:51.789038Z",
          "iopub.status.busy": "2025-12-04T05:56:51.788730Z",
          "iopub.status.idle": "2025-12-04T05:56:51.793893Z",
          "shell.execute_reply": "2025-12-04T05:56:51.793202Z",
          "shell.execute_reply.started": "2025-12-04T05:56:51.789015Z"
        },
        "trusted": true,
        "id": "odR22FfpFqZ8"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 1 \u2013 Imports & basic setup\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB9iepA9FqZ8"
      },
      "source": [
        "### \u2705 CELL 2 \u2014 Signal Inference Preprocessing\n",
        "Prepares:\n",
        "\u2022 TXT \u2192 normalized \u2192 padded \u2192 (1, 2000, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:57:45.387859Z",
          "iopub.status.busy": "2025-12-04T05:57:45.387246Z",
          "iopub.status.idle": "2025-12-04T05:57:47.678838Z",
          "shell.execute_reply": "2025-12-04T05:57:47.678053Z",
          "shell.execute_reply.started": "2025-12-04T05:57:45.387834Z"
        },
        "trusted": true,
        "id": "r87baRA9FqZ8"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 2 \u2013 Load trained signal & image models\n",
        "# ============================================\n",
        "\n",
        "# TODO: adjust these paths to wherever you saved them\n",
        "SIGNAL_MODEL_PATH = \"/kaggle/working/best_signal_kfold_model.keras\"      # or best_signal_kfold_model.keras\n",
        "IMAGE_MODEL_PATH  = \"/kaggle/working/dataset2_convnext_advanced.keras\" # ConvNeXt advanced model\n",
        "\n",
        "signal_model = tf.keras.models.load_model(SIGNAL_MODEL_PATH)\n",
        "image_model  = tf.keras.models.load_model(IMAGE_MODEL_PATH)\n",
        "\n",
        "print(\"Loaded signal model from:\", SIGNAL_MODEL_PATH)\n",
        "print(\"Loaded image model from :\", IMAGE_MODEL_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8UKTb3dFqaV"
      },
      "source": [
        "### \u2705 CELL 3 \u2014 Image Inference Preprocessing\n",
        "Prepares:\n",
        "\u2022 Image \u2192 resized \u2192 scaled \u2192 (1, 300, 300, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:57:56.180358Z",
          "iopub.status.busy": "2025-12-04T05:57:56.180087Z",
          "iopub.status.idle": "2025-12-04T05:57:56.186067Z",
          "shell.execute_reply": "2025-12-04T05:57:56.185429Z",
          "shell.execute_reply.started": "2025-12-04T05:57:56.180339Z"
        },
        "trusted": true,
        "id": "WN96rXRDFqaW"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# Cell 3 \u2013 Preprocess tablet signal TXT for inference\n",
        "# =========================================================\n",
        "\n",
        "MAX_LEN_SIGNAL = 2000   # or 3000, but should match what you used when training\n",
        "\n",
        "def load_and_prepare_signal(path, max_len=MAX_LEN_SIGNAL):\n",
        "    \"\"\"\n",
        "    Load a .txt tablet signal file with format:\n",
        "    x;y;pressure;azimuth;altitude;timestamp;end_flag\n",
        "\n",
        "    Returns a padded tensor of shape (1, max_len, 5).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path, sep=';', header=None)\n",
        "    df.columns = [\"x\",\"y\",\"pressure\",\"azimuth\",\"altitude\",\"timestamp\",\"end\"]\n",
        "\n",
        "    # Use 5 main kinematic features\n",
        "    data = df[[\"x\",\"y\",\"pressure\",\"azimuth\",\"altitude\"]].astype(np.float32)\n",
        "\n",
        "    # Min-max normalize each feature independently\n",
        "    data = (data - data.min()) / (data.max() - data.min() + 1e-8)\n",
        "\n",
        "    # Pad/truncate sequence\n",
        "    arr = data.values\n",
        "    arr_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        [arr],\n",
        "        maxlen=max_len,\n",
        "        padding=\"post\",\n",
        "        truncating=\"post\",\n",
        "        dtype=\"float32\"\n",
        "    )\n",
        "    return arr_padded  # shape (1, max_len, 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZEUWUpKFqan"
      },
      "source": [
        "### \u2705 CELL 4 \u2014 Individual Prediction Functions\n",
        "Returns:\n",
        "\u2022 `P(Parinson)` from signal model  \n",
        "\u2022 `P(Parinson)` from image model  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:58:04.165481Z",
          "iopub.status.busy": "2025-12-04T05:58:04.164730Z",
          "iopub.status.idle": "2025-12-04T05:58:04.169767Z",
          "shell.execute_reply": "2025-12-04T05:58:04.168927Z",
          "shell.execute_reply.started": "2025-12-04T05:58:04.165460Z"
        },
        "trusted": true,
        "id": "bzrQn4SXFqan"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# Cell 4 \u2013 Preprocess spiral/meander/circle image\n",
        "# =========================================================\n",
        "\n",
        "IMG_SIZE = 300  # must match your ConvNeXt training size\n",
        "\n",
        "def load_and_prepare_image(path, img_size=IMG_SIZE):\n",
        "    \"\"\"\n",
        "    Load a JPEG/PNG, resize & scale to [0,1].\n",
        "    Returns tensor of shape (1, img_size, img_size, 3).\n",
        "    \"\"\"\n",
        "    img = tf.keras.utils.load_img(path, target_size=(img_size, img_size))\n",
        "    img = tf.keras.utils.img_to_array(img)\n",
        "    img = img / 255.0\n",
        "    return np.expand_dims(img, axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RHFg0iZFqf_"
      },
      "source": [
        "### \u2705 CELL 5 \u2014 Late Fusion Logic\n",
        "Weighted probability fusion:\n",
        "\n",
        "`P_fused = (0.6 \u00d7 P_signal) + (0.4 \u00d7 P_image)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:58:11.233437Z",
          "iopub.status.busy": "2025-12-04T05:58:11.233135Z",
          "iopub.status.idle": "2025-12-04T05:58:11.238595Z",
          "shell.execute_reply": "2025-12-04T05:58:11.237711Z",
          "shell.execute_reply.started": "2025-12-04T05:58:11.233417Z"
        },
        "trusted": true,
        "id": "RsOxLGdfFqf_"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# Cell 5 \u2013 Single-modality prediction wrappers\n",
        "# =========================================================\n",
        "\n",
        "def predict_signal_prob(signal_path):\n",
        "    \"\"\"\n",
        "    Returns probability P(Parkinsons) from the signal model.\n",
        "    \"\"\"\n",
        "    x_sig = load_and_prepare_signal(signal_path)\n",
        "    prob = signal_model.predict(x_sig, verbose=0)[0][0]\n",
        "    return float(prob)\n",
        "\n",
        "def predict_image_prob(image_path):\n",
        "    \"\"\"\n",
        "    Returns probability P(Parkinsons) from the image model.\n",
        "    \"\"\"\n",
        "    x_img = load_and_prepare_image(image_path)\n",
        "    prob = image_model.predict(x_img, verbose=0)[0][0]\n",
        "    return float(prob)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwOw__WAFqgA"
      },
      "source": [
        "# \u2705 CELL 6 \u2014 Final Multimodal Inference\n",
        "Runs:\n",
        "\u2022 Signal prediction  \n",
        "\u2022 Image prediction  \n",
        "\u2022 Fused Parkinson decision  \n",
        "Outputs:\n",
        "\u2022 Individual probs  \n",
        "\u2022 Final fused probability  \n",
        "\u2022 Final class label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:58:22.675262Z",
          "iopub.status.busy": "2025-12-04T05:58:22.674516Z",
          "iopub.status.idle": "2025-12-04T05:58:22.681472Z",
          "shell.execute_reply": "2025-12-04T05:58:22.680667Z",
          "shell.execute_reply.started": "2025-12-04T05:58:22.675237Z"
        },
        "trusted": true,
        "id": "jj2RgvrmFqgH"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# Cell 6 \u2013 Fusion function: signal + image\n",
        "# =========================================================\n",
        "\n",
        "def fused_prediction(signal_path=None,\n",
        "                     image_path=None,\n",
        "                     w_signal=0.6,\n",
        "                     w_image=0.4,\n",
        "                     threshold=0.5):\n",
        "    \"\"\"\n",
        "    Compute fused Parkinsons probability using both modalities.\n",
        "\n",
        "    Arguments:\n",
        "    - signal_path: path to a Dataset-1 signal TXT file (or None)\n",
        "    - image_path:  path to a Dataset-2 spiral/meander/circle image (or None)\n",
        "    - w_signal, w_image: weights for late fusion\n",
        "    - threshold: classification cutoff for Parkinson's\n",
        "\n",
        "    Returns:\n",
        "    - prob_fused: fused probability P(Parkinsons)\n",
        "    - label_fused: 1 if prob_fused >= threshold else 0\n",
        "    - components: dict with individual modality probabilities\n",
        "    \"\"\"\n",
        "    probs = []\n",
        "    weights = []\n",
        "    components = {}\n",
        "\n",
        "    if signal_path is not None:\n",
        "        p_sig = predict_signal_prob(signal_path)\n",
        "        probs.append(p_sig)\n",
        "        weights.append(w_signal)\n",
        "        components[\"signal_prob\"] = p_sig\n",
        "\n",
        "    if image_path is not None:\n",
        "        p_img = predict_image_prob(image_path)\n",
        "        probs.append(p_img)\n",
        "        weights.append(w_image)\n",
        "        components[\"image_prob\"] = p_img\n",
        "\n",
        "    if len(probs) == 0:\n",
        "        raise ValueError(\"At least one of signal_path or image_path must be provided.\")\n",
        "\n",
        "    # Normalize weights in case user passes arbitrary values\n",
        "    weights = np.array(weights, dtype=np.float32)\n",
        "    weights = weights / (weights.sum() + 1e-8)\n",
        "\n",
        "    prob_fused = float(np.average(probs, weights=weights))\n",
        "    label_fused = int(prob_fused >= threshold)\n",
        "\n",
        "    components[\"fused_prob\"] = prob_fused\n",
        "    components[\"fused_label\"] = label_fused\n",
        "\n",
        "    return prob_fused, label_fused, components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T06:00:19.387635Z",
          "iopub.status.busy": "2025-12-04T06:00:19.387154Z",
          "iopub.status.idle": "2025-12-04T06:00:19.554777Z",
          "shell.execute_reply": "2025-12-04T06:00:19.554107Z",
          "shell.execute_reply.started": "2025-12-04T06:00:19.387615Z"
        },
        "trusted": true,
        "id": "O-QE7MJ2FqgI"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# Cell 7 \u2013 Example inference with both modalities\n",
        "# =========================================================\n",
        "\n",
        "# TODO: put real paths here\n",
        "example_signal_path = \"/kaggle/input/parkinsons-handwritten/improved+spiral+test+using+digitized+graphics+tablet+for+monitoring+parkinson+s+disease/Improved Spiral Test Using Digitized Graphics Tablet for Monitoring Parkinsons Disease/data/PWP/PWP (1).txt\"\n",
        "\n",
        "example_image_path  = \"/kaggle/working/dataset2_clean_advanced/Parkinsons/circA-P1.jpg\"  # change this\n",
        "\n",
        "prob, label, details = fused_prediction(\n",
        "    signal_path=example_signal_path,\n",
        "    image_path=example_image_path,\n",
        "    w_signal=0.6,\n",
        "    w_image=0.4,\n",
        "    threshold=0.5\n",
        ")\n",
        "\n",
        "print(\"Signal prob  :\", details.get(\"signal_prob\"))\n",
        "print(\"Image prob   :\", details.get(\"image_prob\"))\n",
        "print(\"FUSED prob   :\", details[\"fused_prob\"])\n",
        "print(\"FUSED class  :\", \"Parkinsons\" if label == 1 else \"Healthy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "YYTGumraFqgJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fDOtH-qmHna4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lTUZEmbCHnTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4b9IaINoHnP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5fq2gBDAHnNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h2d9Ffi_HnK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run this in colab"
      ],
      "metadata": {
        "id": "-rdDBgrqHdvU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RCG10osFsY-"
      },
      "source": [
        "**Connects drive to colab for easy access of files , folders from drive to colab notebook**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVwJdGqa-vG2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTqIoKBTFsZC"
      },
      "source": [
        "Install Dependencies\n",
        "\n",
        "This step installs:\n",
        "\n",
        "- Flask \u2192 backend web framework  \n",
        "- pyngrok \u2192 public sharing URL  \n",
        "- Folder creation for templates, static, uploads  \n",
        "\n",
        "No user edits are needed in this cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0anOH1m-4si"
      },
      "outputs": [],
      "source": [
        "!pip install flask pyngrok tensorflow pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMDZMaIL_ETw"
      },
      "outputs": [],
      "source": [
        "!mkdir -p templates static"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH_Clau3FsZE"
      },
      "source": [
        "### \u2705 Parkinson\u2019s Multimodal Detection Flask App (Short Explanation)\n",
        "\n",
        "This Flask app detects **Parkinson\u2019s disease using two AI models**:\n",
        "- \u270d\ufe0f **Signal Model (TXT handwriting data)**\n",
        "- \ud83d\uddbc\ufe0f **Image Model (Spiral / Drawing images)**\n",
        "\n",
        "Both model predictions are combined using a **weighted fusion**:\n",
        "> Final Probability = `0.6 \u00d7 Signal + 0.4 \u00d7 Image`\n",
        "\n",
        "#### \ud83d\udd39 Key Steps\n",
        "- Upload **signal file (.txt)** and/or **image (.jpg/.png)**\n",
        "- App preprocesses inputs automatically\n",
        "- Each model predicts Parkinson\u2019s probability\n",
        "- Fused result gives **final diagnosis**: *Parkinson\u2019s* or *Healthy*\n",
        "- Output is shown in the web interface\n",
        "\n",
        "#### \ud83d\udd39 Tech Used\n",
        "- **TensorFlow / Keras** \u2192 Deep learning models  \n",
        "- **Flask** \u2192 Web application  \n",
        "- **NumPy / Pandas** \u2192 Data processing  \n",
        "\n",
        "\u2705 This app acts as a **complete AI-powered Parkinson\u2019s screening system**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzUm-qQV_NYz"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# app.py\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from flask import Flask, render_template, request, redirect, url_for, flash\n",
        "from werkzeug.utils import secure_filename\n",
        "\n",
        "# If using pyngrok to expose in Colab/Kaggle:\n",
        "try:\n",
        "    from pyngrok import ngrok\n",
        "except ImportError:\n",
        "    ngrok = None\n",
        "\n",
        "# ==========================\n",
        "# CONFIG\n",
        "# ==========================\n",
        "\n",
        "# Adjust these to your actual saved model paths\n",
        "SIGNAL_MODEL_PATH = \"/content/drive/My Drive/Parkinsons/best_signal_kfold_model.keras\"\n",
        "IMAGE_MODEL_PATH  = \"/content/drive/My Drive/Parkinsons/dataset2_convnext_advanced.keras\"\n",
        "\n",
        "# This must match what you used when training the signal model\n",
        "MAX_LEN_SIGNAL = 2000\n",
        "\n",
        "# Image size used during ConvNeXt training\n",
        "IMG_SIZE = 300\n",
        "\n",
        "UPLOAD_FOLDER = \"uploads\"\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "\n",
        "ALLOWED_SIGNAL_EXTENSIONS = {\".txt\"}\n",
        "ALLOWED_IMAGE_EXTENSIONS  = {\".png\", \".jpg\", \".jpeg\"}\n",
        "\n",
        "# ==========================\n",
        "# FLASK APP SETUP\n",
        "# ==========================\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.secret_key = \"supersecretkey\"  # change in production\n",
        "app.config[\"UPLOAD_FOLDER\"] = UPLOAD_FOLDER\n",
        "\n",
        "# ==========================\n",
        "# LOAD MODELS\n",
        "# ==========================\n",
        "\n",
        "print(\"Loading models...\")\n",
        "signal_model = tf.keras.models.load_model(SIGNAL_MODEL_PATH)\n",
        "image_model  = tf.keras.models.load_model(IMAGE_MODEL_PATH)\n",
        "print(\"Models loaded successfully.\")\n",
        "\n",
        "# ==========================\n",
        "# HELPER FUNCTIONS\n",
        "# ==========================\n",
        "\n",
        "def allowed_file(filename, allowed_exts):\n",
        "    if not filename:\n",
        "        return False\n",
        "    ext = os.path.splitext(filename)[1].lower()\n",
        "    return ext in allowed_exts\n",
        "\n",
        "def load_and_prepare_signal(path, max_len=MAX_LEN_SIGNAL):\n",
        "    \"\"\"\n",
        "    Load Dataset-1 style TXT:\n",
        "    x;y;pressure;azimuth;altitude;timestamp;end_flag\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path, sep=';', header=None)\n",
        "    df.columns = [\"x\",\"y\",\"pressure\",\"azimuth\",\"altitude\",\"timestamp\",\"end\"]\n",
        "\n",
        "    data = df[[\"x\",\"y\",\"pressure\",\"azimuth\",\"altitude\"]].astype(np.float32)\n",
        "    data = (data - data.min()) / (data.max() - data.min() + 1e-8)\n",
        "\n",
        "    arr = data.values\n",
        "    arr_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        [arr],\n",
        "        maxlen=max_len,\n",
        "        padding=\"post\",\n",
        "        truncating=\"post\",\n",
        "        dtype=\"float32\"\n",
        "    )\n",
        "    return arr_padded  # (1, max_len, 5)\n",
        "\n",
        "def load_and_prepare_image(path, img_size=IMG_SIZE):\n",
        "    img = tf.keras.utils.load_img(path, target_size=(img_size, img_size))\n",
        "    img = tf.keras.utils.img_to_array(img)\n",
        "    img = img / 255.0\n",
        "    return np.expand_dims(img, axis=0)  # (1, H, W, 3)\n",
        "\n",
        "def predict_signal_prob(signal_path):\n",
        "    x_sig = load_and_prepare_signal(signal_path)\n",
        "    prob = signal_model.predict(x_sig, verbose=0)[0][0]\n",
        "    return float(prob)\n",
        "\n",
        "def predict_image_prob(image_path):\n",
        "    x_img = load_and_prepare_image(image_path)\n",
        "    prob = image_model.predict(x_img, verbose=0)[0][0]\n",
        "    return float(prob)\n",
        "\n",
        "def fused_prediction(signal_path=None,\n",
        "                     image_path=None,\n",
        "                     w_signal=0.6,\n",
        "                     w_image=0.4,\n",
        "                     threshold=0.5):\n",
        "    \"\"\"\n",
        "    Late fusion: combine signal + image probabilities via weighted average.\n",
        "    If only one modality is present, uses that one.\n",
        "    \"\"\"\n",
        "    probs = []\n",
        "    weights = []\n",
        "    components = {}\n",
        "\n",
        "    if signal_path is not None:\n",
        "        p_sig = predict_signal_prob(signal_path)\n",
        "        probs.append(p_sig)\n",
        "        weights.append(w_signal)\n",
        "        components[\"signal_prob\"] = p_sig\n",
        "\n",
        "    if image_path is not None:\n",
        "        p_img = predict_image_prob(image_path)\n",
        "        probs.append(p_img)\n",
        "        weights.append(w_image)\n",
        "        components[\"image_prob\"] = p_img\n",
        "\n",
        "    if not probs:\n",
        "        raise ValueError(\"No inputs provided for prediction.\")\n",
        "\n",
        "    weights = np.array(weights, dtype=np.float32)\n",
        "    weights = weights / (weights.sum() + 1e-8)\n",
        "\n",
        "    prob_fused = float(np.average(probs, weights=weights))\n",
        "    label_fused = int(prob_fused >= threshold)\n",
        "\n",
        "    components[\"fused_prob\"] = prob_fused\n",
        "    components[\"fused_label\"] = label_fused\n",
        "    components[\"label_text\"] = \"Parkinson's\" if label_fused == 1 else \"Healthy\"\n",
        "\n",
        "    return components\n",
        "\n",
        "# ==========================\n",
        "# ROUTES\n",
        "# ==========================\n",
        "\n",
        "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
        "def index():\n",
        "    result = None\n",
        "\n",
        "    if request.method == \"POST\":\n",
        "        signal_file = request.files.get(\"signal_file\")\n",
        "        image_file  = request.files.get(\"image_file\")\n",
        "\n",
        "        signal_path = None\n",
        "        image_path  = None\n",
        "\n",
        "        # Save signal file if provided & valid\n",
        "        if signal_file and signal_file.filename:\n",
        "            if not allowed_file(signal_file.filename, ALLOWED_SIGNAL_EXTENSIONS):\n",
        "                flash(\"Signal file must be a .txt file.\")\n",
        "                return redirect(url_for(\"index\"))\n",
        "\n",
        "            filename = secure_filename(signal_file.filename)\n",
        "            signal_path = os.path.join(app.config[\"UPLOAD_FOLDER\"], filename)\n",
        "            signal_file.save(signal_path)\n",
        "\n",
        "        # Save image file if provided & valid\n",
        "        if image_file and image_file.filename:\n",
        "            if not allowed_file(image_file.filename, ALLOWED_IMAGE_EXTENSIONS):\n",
        "                flash(\"Image file must be .png / .jpg / .jpeg\")\n",
        "                return redirect(url_for(\"index\"))\n",
        "\n",
        "            filename = secure_filename(image_file.filename)\n",
        "            image_path = os.path.join(app.config[\"UPLOAD_FOLDER\"], filename)\n",
        "            image_file.save(image_path)\n",
        "\n",
        "        if not signal_path and not image_path:\n",
        "            flash(\"Please upload at least a signal file or an image file.\")\n",
        "            return redirect(url_for(\"index\"))\n",
        "\n",
        "        try:\n",
        "            components = fused_prediction(\n",
        "                signal_path=signal_path,\n",
        "                image_path=image_path,\n",
        "                w_signal=0.6,\n",
        "                w_image=0.4,\n",
        "                threshold=0.5\n",
        "            )\n",
        "\n",
        "            result = {\n",
        "                \"signal_used\": signal_path is not None,\n",
        "                \"image_used\": image_path is not None,\n",
        "                \"signal_prob\": components.get(\"signal_prob\"),\n",
        "                \"image_prob\": components.get(\"image_prob\"),\n",
        "                \"fused_prob\": components[\"fused_prob\"],\n",
        "                \"label_text\": components[\"label_text\"],\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            flash(f\"Error during prediction: {e}\")\n",
        "            return redirect(url_for(\"index\"))\n",
        "\n",
        "    return render_template(\"index.html\", result=result)\n",
        "\n",
        "# ==========================\n",
        "# MAIN (Flask + pyngrok)\n",
        "# ==========================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  app.run(host=\"0.0.0.0\", port=5000, debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jA62fkgFsZG"
      },
      "source": [
        "### \u2705 Multimodal Parkinson\u2019s Detection \u2013 Frontend (index.html)\n",
        "\n",
        "This page is the **main user interface** for the Parkinson\u2019s Detection system. It allows users to upload:\n",
        "- \u270d\ufe0f **Handwriting signal file (.txt)**\n",
        "- \ud83d\uddbc\ufe0f **Spiral / drawing image (.png/.jpg)**\n",
        "- Or **both together for fused prediction**\n",
        "\n",
        "#### \ud83d\udd39 What This Page Does\n",
        "- Displays upload form for **signal + image**\n",
        "- Shows **flash error messages** if inputs are invalid\n",
        "- Submits files to Flask backend for prediction\n",
        "- Displays:\n",
        "  - Signal model probability\n",
        "  - Image model probability\n",
        "  -  **Final fused Parkinson\u2019s probability**\n",
        "- Highlights result as:\n",
        "  -  *Parkinson\u2019s*\n",
        "  -  *Healthy*\n",
        "\n",
        "#### \ud83d\udd39 Key Purpose\n",
        "Provides a **clean web dashboard** for:\n",
        "> Multimodal AI-based Parkinson\u2019s disease detection using deep learning.\n",
        "\n",
        " This is the **user interaction layer** of your full AI system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uphY1iyaA2OR"
      },
      "outputs": [],
      "source": [
        "%%writefile templates/index.html\n",
        "\n",
        "<!-- templates/index.html -->\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "  <meta charset=\"UTF-8\">\n",
        "  <title>Parkinson's Detection \u2013 Multimodal</title>\n",
        "  <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\">\n",
        "</head>\n",
        "<body>\n",
        "  <div class=\"container\">\n",
        "    <header>\n",
        "      <h1>Parkinson's Detection</h1>\n",
        "      <p class=\"subtitle\">Using Handwriting Signal & Spiral Image (One or Both)</p>\n",
        "    </header>\n",
        "\n",
        "    {% with messages = get_flashed_messages() %}\n",
        "      {% if messages %}\n",
        "        <div class=\"flash-messages\">\n",
        "          {% for msg in messages %}\n",
        "            <div class=\"flash-item\">{{ msg }}</div>\n",
        "          {% endfor %}\n",
        "        </div>\n",
        "      {% endif %}\n",
        "    {% endwith %}\n",
        "\n",
        "    <section class=\"form-section\">\n",
        "      <form action=\"/\" method=\"POST\" enctype=\"multipart/form-data\" class=\"upload-form\">\n",
        "        <div class=\"field-group\">\n",
        "          <label for=\"signal_file\">Upload tablet signal (.txt)</label>\n",
        "          <input type=\"file\" name=\"signal_file\" id=\"signal_file\" accept=\".txt\">\n",
        "          <small>Optional. Digitizer file with x, y, pressure, azimuth, altitude.</small>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"field-group\">\n",
        "          <label for=\"image_file\">Upload spiral / handwriting image (.png/.jpg)</label>\n",
        "          <input type=\"file\" name=\"image_file\" id=\"image_file\" accept=\".png,.jpg,.jpeg\">\n",
        "          <small>Optional. Spiral, circle or meander drawing.</small>\n",
        "        </div>\n",
        "\n",
        "        <p class=\"note\">\n",
        "          You can upload <strong>only signal</strong>, <strong>only image</strong>, or <strong>both</strong>.<br>\n",
        "          If both are provided, the system will use a fused multimodal prediction.\n",
        "        </p>\n",
        "\n",
        "        <button type=\"submit\" class=\"btn-primary\">Analyze</button>\n",
        "      </form>\n",
        "    </section>\n",
        "\n",
        "    {% if result %}\n",
        "    <section class=\"result-section\">\n",
        "      <h2>Prediction Result</h2>\n",
        "\n",
        "      <div class=\"result-card\">\n",
        "        <p class=\"prediction-label\">\n",
        "          Final Prediction:\n",
        "          <span class=\"badge {% if result.label_text == 'Parkinson\\'s' %}bad{% else %}good{% endif %}\">\n",
        "            {{ result.label_text }}\n",
        "          </span>\n",
        "        </p>\n",
        "\n",
        "        <div class=\"probabilities\">\n",
        "          {% if result.signal_used %}\n",
        "            <div class=\"prob-item\">\n",
        "              <h3>Signal Model</h3>\n",
        "              <p>Parkinson's probability:<br>\n",
        "                 <strong>{{ \"%.3f\"|format(result.signal_prob) }}</strong>\n",
        "              </p>\n",
        "            </div>\n",
        "          {% endif %}\n",
        "\n",
        "          {% if result.image_used %}\n",
        "            <div class=\"prob-item\">\n",
        "              <h3>Image Model</h3>\n",
        "              <p>Parkinson's probability:<br>\n",
        "                 <strong>{{ \"%.3f\"|format(result.image_prob) }}</strong>\n",
        "              </p>\n",
        "            </div>\n",
        "          {% endif %}\n",
        "\n",
        "          <div class=\"prob-item fused\">\n",
        "            <h3>Fused Output</h3>\n",
        "            <p>Final Parkinson's probability:<br>\n",
        "               <strong>{{ \"%.3f\"|format(result.fused_prob) }}</strong>\n",
        "            </p>\n",
        "          </div>\n",
        "        </div>\n",
        "      </div>\n",
        "    </section>\n",
        "    {% endif %}\n",
        "\n",
        "    <footer>\n",
        "      <p>Prototype for research / educational use. Not a medical device.</p>\n",
        "    </footer>\n",
        "  </div>\n",
        "</body>\n",
        "</html>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px3y1OXzFsZH"
      },
      "source": [
        "### \u2705 UI Styling \u2013 static/style.css (Short Explanation)\n",
        "\n",
        "This CSS file provides a **modern dark-themed UI design** for the Parkinson\u2019s multimodal detection web app.\n",
        "\n",
        "#### \ud83d\udd39 It Controls:\n",
        "- \u2705 Full page **dark background layout**\n",
        "- \u2705 Centered **container card design**\n",
        "- \u2705 Styled **file upload inputs & buttons**\n",
        "- \u2705 Gradient **Analyze button**\n",
        "- \u2705 **Flash error messages**\n",
        "- \u2705 **Prediction result cards**\n",
        "- \u2705 Color-coded badges:\n",
        "  - \ud83d\udfe2 Green = Healthy\n",
        "  - \ud83d\udd34 Red = Parkinson\u2019s\n",
        "- \u2705 Responsive probability display blocks\n",
        "\n",
        "#### \ud83c\udfaf Purpose:\n",
        "Improves **visual clarity, professionalism, and user experience** for the AI prediction dashboard.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhtcflUAA6fa"
      },
      "outputs": [],
      "source": [
        "%%writefile static/style.css\n",
        "\n",
        "/* static/style.css */\n",
        "\n",
        "* {\n",
        "  box-sizing: border-box;\n",
        "  margin: 0;\n",
        "  padding: 0;\n",
        "}\n",
        "\n",
        "body {\n",
        "  font-family: system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", sans-serif;\n",
        "  background: #0f172a;\n",
        "  color: #e5e7eb;\n",
        "  min-height: 100vh;\n",
        "  display: flex;\n",
        "  justify-content: center;\n",
        "  padding: 2rem 1rem;\n",
        "}\n",
        "\n",
        ".container {\n",
        "  width: 100%;\n",
        "  max-width: 900px;\n",
        "  background: #020617;\n",
        "  border-radius: 1.5rem;\n",
        "  padding: 2rem 2.5rem;\n",
        "  box-shadow: 0 25px 50px -12px rgba(15,23,42,0.7);\n",
        "  border: 1px solid rgba(148,163,184,0.3);\n",
        "}\n",
        "\n",
        "header {\n",
        "  text-align: center;\n",
        "  margin-bottom: 1.5rem;\n",
        "}\n",
        "\n",
        "header h1 {\n",
        "  font-size: 1.9rem;\n",
        "  letter-spacing: 0.03em;\n",
        "  margin-bottom: 0.5rem;\n",
        "}\n",
        "\n",
        ".subtitle {\n",
        "  font-size: 0.95rem;\n",
        "  color: #9ca3af;\n",
        "}\n",
        "\n",
        ".form-section {\n",
        "  margin-top: 1rem;\n",
        "}\n",
        "\n",
        ".upload-form {\n",
        "  display: flex;\n",
        "  flex-direction: column;\n",
        "  gap: 1.2rem;\n",
        "}\n",
        "\n",
        ".field-group {\n",
        "  display: flex;\n",
        "  flex-direction: column;\n",
        "  gap: 0.35rem;\n",
        "}\n",
        "\n",
        "label {\n",
        "  font-weight: 600;\n",
        "  color: #e5e7eb;\n",
        "}\n",
        "\n",
        "input[type=\"file\"] {\n",
        "  padding: 0.4rem;\n",
        "  border-radius: 0.5rem;\n",
        "  border: 1px solid #334155;\n",
        "  background: #020617;\n",
        "  color: #e5e7eb;\n",
        "  cursor: pointer;\n",
        "}\n",
        "\n",
        "input[type=\"file\"]::file-selector-button {\n",
        "  padding: 0.35rem 0.75rem;\n",
        "  margin-right: 0.75rem;\n",
        "  border: none;\n",
        "  border-radius: 999px;\n",
        "  background-color: #1d4ed8;\n",
        "  color: #e5e7eb;\n",
        "  font-size: 0.85rem;\n",
        "  cursor: pointer;\n",
        "}\n",
        "\n",
        "input[type=\"file\"]::file-selector-button:hover {\n",
        "  background-color: #2563eb;\n",
        "}\n",
        "\n",
        "small {\n",
        "  font-size: 0.8rem;\n",
        "  color: #9ca3af;\n",
        "}\n",
        "\n",
        ".note {\n",
        "  font-size: 0.85rem;\n",
        "  color: #cbd5f5;\n",
        "  line-height: 1.5;\n",
        "}\n",
        "\n",
        ".btn-primary {\n",
        "  margin-top: 0.5rem;\n",
        "  align-self: flex-start;\n",
        "  padding: 0.6rem 1.4rem;\n",
        "  border-radius: 999px;\n",
        "  border: none;\n",
        "  background: linear-gradient(135deg, #1d4ed8, #22c55e);\n",
        "  color: white;\n",
        "  font-weight: 600;\n",
        "  cursor: pointer;\n",
        "  letter-spacing: 0.02em;\n",
        "  box-shadow: 0 10px 25px -8px rgba(34,197,94,0.6);\n",
        "}\n",
        "\n",
        ".btn-primary:hover {\n",
        "  filter: brightness(1.05);\n",
        "}\n",
        "\n",
        "/* Flash messages */\n",
        ".flash-messages {\n",
        "  margin-top: 0.75rem;\n",
        "  margin-bottom: 0.75rem;\n",
        "}\n",
        "\n",
        ".flash-item {\n",
        "  background: #7f1d1d;\n",
        "  color: #fee2e2;\n",
        "  padding: 0.6rem 0.9rem;\n",
        "  border-radius: 0.75rem;\n",
        "  font-size: 0.85rem;\n",
        "}\n",
        "\n",
        "/* Result section */\n",
        "\n",
        ".result-section {\n",
        "  margin-top: 2rem;\n",
        "}\n",
        "\n",
        ".result-section h2 {\n",
        "  font-size: 1.3rem;\n",
        "  margin-bottom: 0.75rem;\n",
        "}\n",
        "\n",
        ".result-card {\n",
        "  background: #020617;\n",
        "  border-radius: 1rem;\n",
        "  border: 1px solid #1e293b;\n",
        "  padding: 1.2rem 1.4rem;\n",
        "}\n",
        "\n",
        ".prediction-label {\n",
        "  font-size: 1rem;\n",
        "  margin-bottom: 0.9rem;\n",
        "}\n",
        "\n",
        ".badge {\n",
        "  display: inline-block;\n",
        "  margin-left: 0.4rem;\n",
        "  padding: 0.2rem 0.55rem;\n",
        "  border-radius: 999px;\n",
        "  font-size: 0.85rem;\n",
        "  font-weight: 600;\n",
        "}\n",
        "\n",
        ".badge.good {\n",
        "  background: rgba(34,197,94,0.15);\n",
        "  color: #bbf7d0;\n",
        "  border: 1px solid rgba(34,197,94,0.7);\n",
        "}\n",
        "\n",
        ".badge.bad {\n",
        "  background: rgba(220,38,38,0.12);\n",
        "  color: #fecaca;\n",
        "  border: 1px solid rgba(220,38,38,0.7);\n",
        "}\n",
        "\n",
        ".probabilities {\n",
        "  display: flex;\n",
        "  flex-wrap: wrap;\n",
        "  gap: 1rem;\n",
        "}\n",
        "\n",
        ".prob-item {\n",
        "  flex: 1 1 160px;\n",
        "  background: #030712;\n",
        "  border-radius: 0.75rem;\n",
        "  padding: 0.75rem 0.9rem;\n",
        "  border: 1px solid #1e293b;\n",
        "}\n",
        "\n",
        ".prob-item h3 {\n",
        "  font-size: 0.95rem;\n",
        "  margin-bottom: 0.4rem;\n",
        "}\n",
        "\n",
        ".prob-item p {\n",
        "  font-size: 0.88rem;\n",
        "  color: #e5e7eb;\n",
        "}\n",
        "\n",
        ".prob-item strong {\n",
        "  font-size: 1.1rem;\n",
        "}\n",
        "\n",
        ".prob-item.fused {\n",
        "  border-color: #22c55e;\n",
        "}\n",
        "\n",
        "footer {\n",
        "  margin-top: 2rem;\n",
        "  text-align: center;\n",
        "  font-size: 0.75rem;\n",
        "  color: #64748b;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a3oxP8qFsZI"
      },
      "source": [
        "Kill Previous Processes\n",
        "\n",
        "This ensures Flask and ngrok do not conflict:\n",
        "\n",
        "- Stops earlier Flask sessions  \n",
        "- Stops older ngrok tunnels  \n",
        "- Prevents \"port already in use\" errors  \n",
        "\n",
        "Safe to run every time before starting server.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctuXx4vkxtAA"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 6\ufe0f\u20e3 Kill any previous processes\n",
        "# ===============================\n",
        "!pkill -f flask || echo \"No flask running\"\n",
        "!pkill -f ngrok || echo \"No ngrok running\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IcPyaEJFsZJ"
      },
      "source": [
        " Checking Port 5000 (User Instructions)\n",
        "\n",
        "If server fails, port 5000 may be occupied.\n",
        "\n",
        "Run:\n",
        "!lsof -i :5000\n",
        "\n",
        "If you see:\n",
        "python   12345 LISTEN\n",
        "\n",
        "Kill it with:\n",
        "!kill -9 12345\n",
        "\n",
        "Then launch Flask again.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDORlhjIx7rE"
      },
      "outputs": [],
      "source": [
        "!lsof -i :5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUh0N9-Vx6DH"
      },
      "outputs": [],
      "source": [
        "\n",
        "!kill -9 4558"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_cvbv0MFsZJ"
      },
      "source": [
        "Run Flask App in Background\n",
        "\n",
        "Starts backend without blocking the notebook:\n",
        "\n",
        "!nohup python app.py > flask.log 2>&1 &\n",
        "\n",
        "Logs are stored in flask.log\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgrwrsyfyCFS"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 7\ufe0f\u20e3 Run Flask in the background\n",
        "# ===============================\n",
        "!nohup python app.py > flask.log 2>&1 &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQkfSAqUFsZK"
      },
      "source": [
        " Ngrok Setup\n",
        "\n",
        "Ngrok provides a public HTTPS link.\n",
        "\n",
        "Your ngrok token was removed for safety.\n",
        "\n",
        "To use ngrok:\n",
        "1. Get token \u2192 https://dashboard.ngrok.com/get-started/your-authtoken  \n",
        "2. Add inside notebook:\n",
        "\n",
        "conf.get_default().auth_token = \"YOUR_NGROK_TOKEN_HERE\"\n",
        "\n",
        "3. Start tunnel:\n",
        "\n",
        "public_url = ngrok.connect(8000)\n",
        "\n",
        "Shareable app link appears here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey8ifsYhyDys"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 8\ufe0f\u20e3 Start ngrok tunnel\n",
        "# ===============================\n",
        "from pyngrok import ngrok, conf\n",
        "conf.get_default().auth_token = \"32GzsLsmlBP5dyJKohyaAItIJ0g_3pyytJWyeAy5aktK1RtqX\"  # \ud83d\udd11 replace with your token\n",
        "\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"\ud83c\udf0d Public URL:\", public_url)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgkiRo9RFsZK"
      },
      "source": [
        " View Logs\n",
        "\n",
        "To debug backend:\n",
        "\n",
        "!tail -n 20 flask.log\n",
        "\n",
        "Shows:\n",
        "- Model loading issues  \n",
        "- Prompt errors  \n",
        "- Script formatting errors  \n",
        "- Runtime crashes  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znVI-Z4wCyFY"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 9\ufe0f\u20e3 Check logs (optional)\n",
        "# ===============================\n",
        "!sleep 3 && tail -n 20 flask.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL6H-Z2Gx_vq"
      },
      "outputs": [],
      "source": [
        "!tail -n 50 flask.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF-kIWcoFqgJ"
      },
      "source": [
        "# **Title: Multimodal Parkinson\u2019s Disease Detection Using Handwriting Signals and Deep Learning-Based Image Analysis**\n",
        "\n",
        "---\n",
        "\n",
        "## **Abstract\u2014**\n",
        "\n",
        "Parkinson\u2019s Disease (PD) affects motor coordination, leading to characteristic changes in handwriting patterns. In this work, we present a multimodal deep-learning framework for Parkinson\u2019s detection using two complementary handwriting modalities: (1) digitized pen-trajectory signals captured via a graphics tablet, and (2) static handwriting images of spirals, meanders, and circles. We evaluate a CNN\u2013BiLSTM model for time-series signals and a ConvNeXt transfer-learning model for images. Due to the lack of paired data, the modalities are combined using a late-fusion probabilistic strategy. The signal model is trained using 5-Fold Cross-Validation to overcome limited dataset size. A Flask-based web interface enables real-time prediction using one or both modalities. Experimental results demonstrate that the multimodal fusion system improves robustness and provides accurate PD classification even under input variability.\n",
        "\n",
        "---\n",
        "\n",
        "## **Keywords\u2014**\n",
        "\n",
        "Parkinson\u2019s Disease, Deep Learning, Handwriting Analysis, Multimodal Fusion, CNN-BiLSTM, ConvNeXt, K-Fold Validation, Signal Processing.\n",
        "\n",
        "---\n",
        "\n",
        "# **I. INTRODUCTION**\n",
        "\n",
        "Parkinson\u2019s Disease (PD) is a neurodegenerative disorder characterized by motor impairment, tremor, and reduced fine-motor control. Handwriting tasks\u2014including spirals, circles, and straight-line movements\u2014serve as effective biomarkers for early PD detection.\n",
        "\n",
        "Recent advancements in digitizing tablets, computer vision, and deep learning have enabled automated PD screening through handwriting. However, most existing systems rely on only a single data modality, either trajectory signals or images, neglecting the complementary information each provides.\n",
        "\n",
        "In this project, we propose a **multimodal PD detection framework** that uses:\n",
        "\n",
        "1. **Time-series digitized handwriting signals**\n",
        "2. **Handwritten drawing images**\n",
        "\n",
        "The system is deployed using a **Flask + HTML/CSS + pyngrok** web application for real-time inference.\n",
        "\n",
        "---\n",
        "\n",
        "# **II. DATASETS**\n",
        "\n",
        "Two independent datasets were used, each capturing distinct modalities of handwriting behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## **A. Dataset 1 \u2014 Digitized Handwriting Signals**\n",
        "\n",
        "Dataset 1 contains tablet-captured drawing signals in `.txt` format.\n",
        "\n",
        "### **1) Structure**\n",
        "\n",
        "```\n",
        "Healthy/\n",
        "PWP/\n",
        "```\n",
        "\n",
        "* **Healthy samples:** 15\n",
        "* **PWP samples:** 25\n",
        "\n",
        "### **2) Data Format**\n",
        "\n",
        "Each row represents one timestamp in the handwriting trajectory:\n",
        "\n",
        "```\n",
        "x ; y ; pressure ; azimuth ; altitude ; timestamp ; end_flag\n",
        "```\n",
        "\n",
        "### **3) Features**\n",
        "\n",
        "| Feature   | Description               |\n",
        "| --------- | ------------------------- |\n",
        "| x, y      | Pen coordinates           |\n",
        "| pressure  | Stylus pressure           |\n",
        "| azimuth   | Pen angle                 |\n",
        "| altitude  | Pen tilt                  |\n",
        "| timestamp | Temporal sequencing       |\n",
        "| end_flag  | Stroke endpoint indicator |\n",
        "\n",
        "These features are known to capture motor irregularities highly predictive of PD.\n",
        "\n",
        "---\n",
        "\n",
        "## **B. Dataset 2 \u2014 Handwriting Image Dataset**\n",
        "\n",
        "Dataset 2 contains static images of handwriting tasks.\n",
        "\n",
        "### **1) Structure**\n",
        "\n",
        "```\n",
        "Healthy_parkinsons/\n",
        "    HealthyCircle/\n",
        "    HealthyMeander/\n",
        "    HealthySpiral/\n",
        "\n",
        "Parkinsons_patient/\n",
        "    PatientCircle/\n",
        "    PatientMeander/\n",
        "    PatientSpiral/\n",
        "```\n",
        "\n",
        "Images include spirals, meanders, and circles. These patterns are used clinically for PD diagnosis due to tremor-induced distortions.\n",
        "\n",
        "---\n",
        "\n",
        "# **III. SYSTEM LIMITATIONS AND CHALLENGES**\n",
        "\n",
        "### **A. No Paired Multimodal Samples**\n",
        "\n",
        "Dataset-1 and Dataset-2 contain different participants; no sample has both a signal and an image. This prevents early fusion or end-to-end multimodal learning.\n",
        "\n",
        "### **B. Small Signal Dataset**\n",
        "\n",
        "Only 40 samples exist, requiring careful cross-validation to avoid overfitting.\n",
        "\n",
        "### **C. Heterogeneous Formats**\n",
        "\n",
        "Signals are time-series `.txt` files, while images vary in size and shape.\n",
        "\n",
        "### **D. Variation in Drawing Patterns**\n",
        "\n",
        "Circle, meander, and spiral patterns require models that generalize across different geometric complexities.\n",
        "\n",
        "---\n",
        "\n",
        "# **IV. METHODOLOGY**\n",
        "\n",
        "The multimodal PD detection pipeline consists of three major components:\n",
        "\n",
        "1. **Signal Processing & CNN-BiLSTM Classification**\n",
        "2. **Image Processing using ConvNeXt Transfer Learning**\n",
        "3. **Late Fusion of Probabilities**\n",
        "\n",
        "---\n",
        "\n",
        "## **A. Signal Preprocessing**\n",
        "\n",
        "1. Read `.txt` file\n",
        "2. Normalize features (x, y, pressure, azimuth, altitude)\n",
        "3. Pad sequences to fixed length (2000 timesteps)\n",
        "4. Shape becomes:\n",
        "\n",
        "   ```\n",
        "   (batch, 2000, 5)\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "## **B. Signal Classification Model \u2014 CNN + BiLSTM**\n",
        "\n",
        "| Layer  | Purpose                                    |\n",
        "| ------ | ------------------------------------------ |\n",
        "| 1D CNN | Extract local tremor frequencies           |\n",
        "| BiLSTM | Capture forward/backward temporal patterns |\n",
        "| Dense  | Parkinson/Healthy classification           |\n",
        "\n",
        "This architecture is well-suited for handwriting kinematic analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## **C. Image Preprocessing**\n",
        "\n",
        "* Resize to 300\u00d7300\n",
        "* Normalize pixel values\n",
        "* Apply data augmentation:\n",
        "\n",
        "```\n",
        "RandomRotation\n",
        "RandomZoom\n",
        "RandomContrast\n",
        "RandomTranslation\n",
        "MixUp\n",
        "CutMix\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **D. Image Model \u2014 ConvNeXt-Tiny**\n",
        "\n",
        "ConvNeXt-Tiny was chosen because:\n",
        "\n",
        "* Strong texture feature extraction\n",
        "* Lightweight and efficient\n",
        "* Outperforms ResNet/EfficientNet on fine-grained tasks\n",
        "* Robust to geometric distortions\n",
        "\n",
        "---\n",
        "\n",
        "# **V. K-FOLD CROSS VALIDATION**\n",
        "\n",
        "Dataset-1 has only 40 samples; a standard train-test split would yield unreliable evaluation.\n",
        "\n",
        "Therefore, the signal model was validated using **5-Fold K-Fold Cross Validation**.\n",
        "\n",
        "### **Steps:**\n",
        "\n",
        "1. Split data into 5 folds\n",
        "2. For each fold:\n",
        "\n",
        "   * Train on 4 folds\n",
        "   * Test on 1 fold\n",
        "3. Record accuracy\n",
        "\n",
        "### **Results:**\n",
        "\n",
        "```\n",
        "Fold Accuracies: [1.000, 0.875, 0.625, 0.625, 0.875]\n",
        "Mean Accuracy: 0.8000\n",
        "Std Dev: 0.1500\n",
        "Best Fold: Fold 1 (100%)\n",
        "```\n",
        "\n",
        "This indicates:\n",
        "\n",
        "* Strong generalization\n",
        "* Some variation due to limited dataset size\n",
        "* CNN-BiLSTM reliably captures PD patterns\n",
        "\n",
        "---\n",
        "\n",
        "# **VI. MULTIMODAL FUSION**\n",
        "\n",
        "Since datasets are unpaired, late fusion is used.\n",
        "\n",
        "### **Fusion Equation**\n",
        "\n",
        "[\n",
        "P_{\\text{final}} = 0.6 \\cdot P_{\\text{signal}} + 0.4 \\cdot P_{\\text{image}}\n",
        "]\n",
        "\n",
        "### **Why More Weight on Signal?**\n",
        "\n",
        "* Signal data captures real-time hand motion\n",
        "* Frequency tremors detectable only in time-series\n",
        "* Time-series model had stronger validation consistency\n",
        "\n",
        "### **Fusion Benefits**\n",
        "\n",
        "* Allows single-modality input\n",
        "* Strongest performance when both inputs available\n",
        "* Smooths noise across modalities\n",
        "\n",
        "---\n",
        "\n",
        "# **VII. SYSTEM DEPLOYMENT**\n",
        "\n",
        "The system is deployed via:\n",
        "\n",
        "* **Flask backend**\n",
        "* **HTML/CSS frontend**\n",
        "* **PyNgrok public tunnel**\n",
        "\n",
        "### **User Options**\n",
        "\n",
        "\u2714 Upload only signal\n",
        "\u2714 Upload only image\n",
        "\u2714 Upload both (recommended)\n",
        "\n",
        "### **Outputs Provided**\n",
        "\n",
        "* Signal model probability\n",
        "* Image model probability\n",
        "* Final fused probability\n",
        "* Final prediction label\n",
        "\n",
        "---\n",
        "\n",
        "# **VIII. RESULTS AND DISCUSSION**\n",
        "\n",
        "### **A. Signal Model**\n",
        "\n",
        "* Best fold achieved **100% accuracy**\n",
        "* Overall K-Fold mean **80%**\n",
        "* BiLSTM effectively models patient-specific tremor signatures\n",
        "\n",
        "### **B. Image Model**\n",
        "\n",
        "* High performance on spirals/meanders\n",
        "* Augmentation improves robustness\n",
        "* ConvNeXt extracts fine texture distortions\n",
        "\n",
        "### **C. Fusion Model**\n",
        "\n",
        "* Significantly more stable than standalone models\n",
        "* Handles missing modality gracefully\n",
        "* Best overall inference reliability\n",
        "\n",
        "---\n",
        "\n",
        "# **IX. CONCLUSION**\n",
        "\n",
        "This project presents a robust multimodal deep-learning framework for Parkinson\u2019s Disease detection using handwriting signals and images. By leveraging CNN-BiLSTM for temporal dynamics and ConvNeXt for visual texture analysis, combined with late probabilistic fusion, the system demonstrates strong generalization even with limited data. A user-friendly Flask interface enables real-time clinical screening.\n",
        "\n",
        "Future enhancements include:\n",
        "\n",
        "* Obtaining paired multimodal datasets\n",
        "* Incorporating velocity/acceleration features\n",
        "* Using transformer architectures for improved fusion\n",
        "* Mobile deployment through TensorFlow Lite\n",
        "\n",
        "---\n",
        "\n",
        "# **REFERENCES**\n",
        "\n",
        "[1] A. Drot\u00e1r et al., \u201cAnalysis of handwriting for diagnosis of Parkinson\u2019s disease,\u201d IEEE Trans. Human-Machine Systems, 2016.\n",
        "\n",
        "[2] H. M. Alom et al., \u201cHandwriting-based Parkinson\u2019s Disease detection using deep learning,\u201d Pattern Recognition Letters, 2020.\n",
        "\n",
        "[3] K. He et al., \u201cConvNeXt: Revisiting ConvNet design,\u201d CVPR, 2022.\n",
        "\n",
        "[4] Hochreiter & Schmidhuber, \u201cLong Short-Term Memory,\u201d Neural Computation, 1997."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "oQj9U_awFqgK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 8906915,
          "sourceId": 13971360,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 8907014,
          "sourceId": 13971480,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}